--- PROJECT STRUCTURE SUMMARY ---
Location: D:\Main\3. Work - Teaching\Projects\Question extractor
 üìÑ add_Manually.py
 üìÑ adhoc.py
 üìÑ AIsuggestedTags.py
 üìÑ AIsuggestedTagsGroq - Copy.py
 üìÑ AIsuggestedTagsGroq.py
 üìÑ AIsuggestedTagsLight.py
 üìÑ AIsuggestedTagsWorkload.py
 üìÑ autoQC.py
 üìÑ batchExtractMainsPYQs.py
 üìÑ batchExtract_CollegeDoors.py
 üìÑ batchExtract_CollegeDoorsBulk.py
 üìÑ batchUnzipCD.py
 üìÑ CD_MergeAllAnswerKeys.py
 üìÑ CD_QuestionCount.py
 üìÑ DBHealthCheck.py
 üìÑ deleteFilesUploadedToDrive.py
 üìÑ fetchManualTagsFromMetadata.py
 üìÑ GoogleDriveUploader.py
 üìÑ Home - Copy.py
 üìÑ Home.py
 üìÑ imageCompression.py
 üìÑ MergeAISuggestedTagsToDB.py
 üìÑ migrateToFirebase.py
 üìÑ OCR_SmartBatch.py
 üìÑ pushToGit.py
 üìÑ sanitizeDB.py
 üìÑ validateAITags.py
 üìÑ validateFirebaseData.py

Total Files: 28
--------------------------------------------------

--- FILE CONTENTS ---

================================================================================
FILE: add_Manually.py
================================================================================
import streamlit as st
import pandas as pd
import os
import shutil
from datetime import datetime
from PIL import Image
import io

# --- TRY IMPORTING PASTE BUTTON ---
try:
    from streamlit_paste_button import paste_image_button as pbutton
except ImportError:
    st.error("‚ö†Ô∏è Library missing! Please run: pip install streamlit-paste-button")
    st.stop()

# --- CONFIGURATION ---
BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor/'
DB_PATH = os.path.join(BASE_PATH, 'DB Master.xlsx')
TOPIC_MAP_PATH = os.path.join(BASE_PATH, 'ChapterTopics.csv')
PROCESSED_DIR = os.path.join(BASE_PATH, 'Processed_Database')
MANUAL_FOLDER_NAME = "Manual_Uploads"

st.set_page_config(page_title="Add New Question", layout="wide") # Changed to Wide for better side-by-side view

# --- SESSION STATE INITIALIZATION ---
if 'step' not in st.session_state: st.session_state['step'] = 1
if 'new_q_data' not in st.session_state: st.session_state['new_q_data'] = {}
if 'q_image_data' not in st.session_state: st.session_state['q_image_data'] = None
if 'sol_image_data' not in st.session_state: st.session_state['sol_image_data'] = None
# Counter ensures unique keys for paste buttons per question
if 'session_counter' not in st.session_state: st.session_state['session_counter'] = 1

# --- HELPERS ---
def load_topics():
    if os.path.exists(TOPIC_MAP_PATH):
        return pd.read_csv(TOPIC_MAP_PATH)
    return pd.DataFrame()

def get_manual_question_no():
    if os.path.exists(DB_PATH):
        df = pd.read_excel(DB_PATH)
        if 'Folder' in df.columns:
            df_manual = df[df['Folder'] == MANUAL_FOLDER_NAME].copy()
            if df_manual.empty: return 1
            df_manual['Question No.'] = pd.to_numeric(df_manual['Question No.'], errors='coerce').fillna(0)
            return int(df_manual['Question No.'].max()) + 1
    return 1

def save_final_question():
    data = st.session_state['new_q_data']
    q_img = st.session_state['q_image_data']
    sol_img = st.session_state['sol_image_data']
    
    q_num = get_manual_question_no()
    
    # Prepare Paths
    target_folder = os.path.join(PROCESSED_DIR, MANUAL_FOLDER_NAME)
    os.makedirs(target_folder, exist_ok=True)
    
    # Save Images
    if q_img:
        q_path = os.path.join(target_folder, f"Q_{q_num}.png")
        q_img.save(q_path)
    if sol_img:
        sol_path = os.path.join(target_folder, f"Sol_{q_num}.png")
        sol_img.save(sol_path)
        
    # Save to Excel
    new_row = {
        'Question No.': q_num,
        'Folder': MANUAL_FOLDER_NAME,
        'Subject': data.get('Subject'),
        'Chapter': data.get('Chapter'),
        'Topic': data.get('Topic'),
        'Topic_L2': data.get('Topic_L2'),
        'Exam': data.get('Exam'),
        'Question type': data.get('Question type'),
        'Difficulty_tag': data.get('Difficulty_tag'),
        'Correct Answer': data.get('Correct Answer'),
        'PYQ': data.get('PYQ'),
        'PYQ_Year': data.get('PYQ_Year'),
        'Classroom_Illustration': data.get('Classroom_Illustration'),
        'QC_Status': 'Pass', 
        'QC_Locked': 1,
        'manually updated': 1
    }
    
    if os.path.exists(DB_PATH):
        df_master = pd.read_excel(DB_PATH)
        df_master = pd.concat([df_master, pd.DataFrame([new_row])], ignore_index=True)
    else:
        df_master = pd.DataFrame([new_row])
        
    try:
        df_master.to_excel(DB_PATH, index=False)
        st.toast(f"‚úÖ Question {q_num} saved!", icon="üéâ")
        
        # --- RESET FOR NEXT CYCLE ---
        st.session_state['step'] = 1
        st.session_state['new_q_data'] = {}
        st.session_state['q_image_data'] = None
        st.session_state['sol_image_data'] = None
        st.session_state['session_counter'] += 1
        
        st.rerun()
        
    except PermissionError:
        st.error("‚ùå Could not save: Excel file is open. Close it and try again.")

# --- NAVIGATION ---
def next_step(): st.session_state['step'] += 1
def prev_step(): st.session_state['step'] -= 1

# =======================
#       UI STEPS
# =======================

# --- STEP 1: LANDING ---
if st.session_state['step'] == 1:
    st.title("Question Entry Wizard")
    next_num = get_manual_question_no()
    st.caption(f"Next Manual ID: #{next_num}")
    
    if st.button("‚ûï Start Adding Question", type="primary"):
        next_step()
        st.rerun()

# --- STEP 2: THE WORKSPACE (MERGED) ---
elif st.session_state['step'] == 2:
    st.header("New Question Details")
    df_topics = load_topics()
    
    # --- METADATA SECTION ---
    with st.container():
        c1, c2, c3 = st.columns([1, 1, 1])
        
        # Column 1: Basic Tags
        with c1:
            st.subheader("1. Tags")
            exam = st.selectbox("Exam", ["JEE Main", "JEE Advanced", "NEET", "Board"], index=0)
            q_type = st.selectbox("Question Type", ["Single Correct", "Multiple Correct", "Numerical", "Passage", "Matrix Match"])
            difficulty = st.selectbox("Difficulty", ["Easy", "Medium", "Hard"])
            
            st.markdown("---")
            val_pyq = st.selectbox("PYQ", ["No", "Yes"], index=0) 
            pyq_year = st.text_input("PYQ Year", placeholder="e.g. 2023 Shift 1") if val_pyq == "Yes" else ""
            val_class_illus = st.selectbox("Classroom Illustration", ["No", "Yes"], index=0)

        # Column 2: Topic Hierarchy
        with c2:
            st.subheader("2. Topic")
            subjects = df_topics['Subject'].unique().tolist() if not df_topics.empty else ['Physics', 'Chemistry', 'Maths']
            subject = st.selectbox("Subject", subjects, index=subjects.index('Physics') if 'Physics' in subjects else 0)
            
            chapters = df_topics[df_topics['Subject'] == subject]['Chapter'].unique().tolist() if not df_topics.empty else []
            chapter = st.selectbox("Chapter", chapters)
            
            topics = []
            if not df_topics.empty and chapter:
                topics = df_topics[(df_topics['Subject'] == subject) & (df_topics['Chapter'] == chapter)]['Topic'].unique().tolist()
            topic = st.selectbox("Topic", topics) if topics else st.text_input("Topic (Manual)")

            l2_topics = []
            if not df_topics.empty and topic and 'Topic_L2' in df_topics.columns:
                l2_topics = df_topics[(df_topics['Subject'] == subject) & (df_topics['Chapter'] == chapter) & (df_topics['Topic'] == topic)]['Topic_L2'].unique().tolist()
                l2_topics = [x for x in l2_topics if str(x) != 'nan']
            topic_l2 = st.selectbox("Sub-Topic", l2_topics) if l2_topics else st.text_input("Sub-Topic (Manual)")
            
        # Column 3: Images & Answer
        with c3:
            st.subheader("3. Content")
            unique_id = st.session_state['session_counter']
            
            # Question Image
            st.caption("Question Image (Win+Shift+S -> Paste)")
            paste_q = pbutton("üìã Paste Question", background_color="#FF4B4B", hover_background_color="#FF0000", key=f"btn_q_{unique_id}")
            if paste_q.image_data is not None: st.session_state['q_image_data'] = paste_q.image_data
            
            if st.session_state['q_image_data']:
                st.image(st.session_state['q_image_data'], width=200)
            else:
                st.warning("Required")
                
            st.markdown("---")
            
            # Solution Image
            st.caption("Solution Image (Optional)")
            paste_sol = pbutton("üìã Paste Solution", background_color="#4CAF50", hover_background_color="#45a049", key=f"btn_sol_{unique_id}")
            if paste_sol.image_data is not None: st.session_state['sol_image_data'] = paste_sol.image_data
            
            if st.session_state['sol_image_data']:
                st.image(st.session_state['sol_image_data'], width=200)

            st.markdown("---")
            
            # Answer Key
            curr_ans = st.session_state['new_q_data'].get('Correct Answer', "")
            ans_key = st.text_input("Correct Answer", value=curr_ans, placeholder="e.g. A, B, 4")

    st.divider()

    # --- NAVIGATION ---
    c_back, c_next = st.columns([1, 6])
    if c_back.button("‚¨ÖÔ∏è Cancel"): 
        st.session_state['step'] = 1
        st.rerun()
    
    if st.session_state['q_image_data']:
        if c_next.button("Review & Save ‚û°Ô∏è", type="primary"): 
            # Save current state to dict before moving
            st.session_state['new_q_data'] = {
                'Exam': exam, 'Question type': q_type, 'Difficulty_tag': difficulty,
                'Subject': subject, 'Chapter': chapter, 'Topic': topic, 'Topic_L2': topic_l2,
                'PYQ': val_pyq, 'PYQ_Year': pyq_year, 'Classroom_Illustration': val_class_illus,
                'Correct Answer': ans_key
            }
            next_step()
            st.rerun()
    else:
        c_next.warning("‚ö†Ô∏è Please paste a question image.")

# --- STEP 3: REVIEW & SAVE ---
elif st.session_state['step'] == 3:
    st.header("Review & Save")
    
    # 1. Show Metadata Table
    df_sum = pd.DataFrame([st.session_state['new_q_data']]).T
    df_sum.columns = ["Value"]
    st.table(df_sum)
    
    # 2. Show Images Large
    col_img1, col_img2 = st.columns(2)
    with col_img1:
        st.subheader("Question")
        if st.session_state['q_image_data']: st.image(st.session_state['q_image_data'])
    
    with col_img2:
        st.subheader("Solution")
        if st.session_state['sol_image_data']: st.image(st.session_state['sol_image_data'])
    
    st.divider()
    
    b1, b2 = st.columns([1, 4])
    if b1.button("‚¨ÖÔ∏è Edit"): prev_step(); st.rerun()
    
    if b2.button("üíæ CONFIRM SAVE", type="primary", use_container_width=True):
        save_final_question()

================================================================================
FILE: adhoc.py
================================================================================
import pandas as pd
import os
import sys
from PIL import Image
try:
    from tqdm import tqdm
except ImportError:
    # Fallback if tqdm is missing
    def tqdm(iterable, **kwargs): return iterable

# --- CONFIGURATION ---
BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor/'
DB_PATH = os.path.join(BASE_PATH, 'DB Master.csv')
IMG_DIR = os.path.join(BASE_PATH, 'Processed_Database')

# --- UTILITIES ---
def load_db():
    if not os.path.exists(DB_PATH):
        print(f"‚ùå Database not found at: {DB_PATH}")
        return None
    try:
        # Support both CSV and Excel for legacy reasons, though we moved to CSV
        if DB_PATH.endswith('.csv'):
            return pd.read_csv(DB_PATH)
        else:
            return pd.read_excel(DB_PATH)
    except Exception as e:
        print(f"‚ùå Error loading database: {e}")
        return None

def save_db(df):
    try:
        df.to_csv(DB_PATH, index=False)
        print("‚úÖ Database saved successfully.")
    except PermissionError:
        print("‚ùå ERROR: File is open. Close 'DB Master.csv' and try again.")
    except Exception as e:
        print(f"‚ùå Save failed: {e}")

def get_image_dims(folder, filename):
    """Returns (width, height) or (None, None)"""
    try:
        path = os.path.join(IMG_DIR, str(folder).strip(), filename)
        if os.path.exists(path) and os.path.getsize(path) > 0:
            with Image.open(path) as img:
                return img.width, img.height
    except Exception:
        pass
    return None, None

# --- FUNCTIONS ---

def func_populate_dimensions():
    print("\nüìê POPULATING IMAGE DIMENSIONS (Q & SOL)...")
    df = load_db()
    if df is None: return

    # 1. Initialize Columns if missing
    targets = ['q_width', 'q_height', 'sol_width', 'sol_height']
    for col in targets:
        if col not in df.columns:
            df[col] = None
            print(f"   ‚ÑπÔ∏è Created new column: {col}")

    # 2. Iterate
    updates = 0
    print(f"   üöÄ Scanning {len(df)} questions...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), unit="row"):
        folder = row.get('Folder')
        if pd.isna(folder) or str(folder).strip() == "": continue

        # Smart Q-Number Fetch (Handles 'Q' or 'Question No.')
        q_val = row.get('Question No.')
        if pd.isna(q_val) or str(q_val).strip() == "":
            q_val = row.get('Q')
        
        if pd.isna(q_val): continue
        
        # Normalize Q number to integer if possible
        try:
            q_num = int(float(q_val))
        except:
            q_num = str(q_val).strip()

        # A. Get Question Dims
        qw, qh = get_image_dims(folder, f"Q_{q_num}.png")
        if qw: 
            df.at[idx, 'q_width'] = qw
            df.at[idx, 'q_height'] = qh
            updates += 1

        # B. Get Solution Dims
        sw, sh = get_image_dims(folder, f"Sol_{q_num}.png")
        if sw:
            df.at[idx, 'sol_width'] = sw
            df.at[idx, 'sol_height'] = sh

    # 3. Save
    print(f"\n   ‚ú® Processed dimensions for {updates} questions.")
    save_db(df)

import pandas as pd
import firebase_admin
from firebase_admin import credentials, firestore
from pathlib import Path

# --- CONFIG ---
# Path to your EXISTING local Excel file
EXCEL_PATH = Path(r'D:/Main/3. Work - Teaching/Projects/Question extractor/DB Metadata.xlsx')

# Initialize Firebase (same as before)
cred = credentials.Certificate('studysmart-5da53-firebase-adminsdk-fbsvc-ca5974c5e9.json')
if not firebase_admin._apps:
    firebase_admin.initialize_app(cred)
db = firestore.client()


import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
import pandas as pd
import re

# --- CONFIGURATION ---
EXCEL_FILE = 'DB Metadata.xlsx'
SHEET_NAME = 'Syllabus tree'
KEY_PATH = 'serviceAccountKey.json' # Path to your downloaded Firebase key

# Initialize Firebase (only if not already initialized)
if not firebase_admin._apps:
    cred = credentials.Certificate(KEY_PATH)
    firebase_admin.initialize_app(cred)

db = firestore.Client.from_service_account_json('serviceAccountKey.json')

def generate_slug(text):
    """
    Cleaner Slug Generator:
    'Moment of Inertia' -> 'moment_of_inertia'
    """
    if not isinstance(text, str): return "unknown"
    slug = text.lower().strip()
    slug = re.sub(r'[^a-z0-9\s-]', '', slug) # Allow hyphens too
    slug = re.sub(r'[\s-]+', '_', slug)      # Replace spaces/hyphens with underscore
    return slug

def update_syllabus_tree():
    print(f"üìÇ Reading {EXCEL_FILE}...")
    
    try:
        df = pd.read_excel(EXCEL_FILE, sheet_name=SHEET_NAME)
        
        # We start with a cleaner root structure
        syllabus_tree = {"subjects": {}}

        print("‚öôÔ∏è  Processing rows with Clean IDs...")
        
        for index, row in df.iterrows():
            subject_name = str(row['Subject']).strip()
            chapter_name = str(row['Chapter']).strip()
            topic_name = str(row['Topic']).strip()

            # --- GENERATE SHORT IDs ---
            # We treat the input text as the source of truth for the ID
            
            sub_id = generate_slug(subject_name)   # e.g., "physics"
            chap_id = generate_slug(chapter_name)  # e.g., "rotational_motion"
            topic_id = generate_slug(topic_name)   # e.g., "moment_of_inertia"

            # --- BUILD THE HIERARCHY ---
            
            # 1. Subject Level
            if sub_id not in syllabus_tree["subjects"]:
                syllabus_tree["subjects"][sub_id] = {
                    "name": subject_name,
                    "chapters": {}
                }
            
            # 2. Chapter Level (Scoped inside Subject)
            if chap_id not in syllabus_tree["subjects"][sub_id]["chapters"]:
                syllabus_tree["subjects"][sub_id]["chapters"][chap_id] = {
                    "name": chapter_name,
                    "topics": {}
                }
            
            # 3. Topic Level (Scoped inside Chapter)
            # Just "topic_id": "Topic Name"
            syllabus_tree["subjects"][sub_id]["chapters"][chap_id]["topics"][topic_id] = topic_name

        # --- UPLOAD ---
        print("‚òÅÔ∏è  Uploading cleaned data to Firestore...")
        db.collection('static_data').document('syllabus').set(syllabus_tree)
        
        print("‚úÖ SUCCESS! Database updated with clean, readable IDs.")
        print("Example path: subjects -> physics -> chapters -> rotational_motion -> topics -> moment_of_inertia")

    except Exception as e:
        print(f"‚ùå Error: {e}")

def generate_slug(text):
    """
    Cleaner Slug Generator:
    'Parallel  Axis Theorem ' -> 'parallel_axis_theorem'
    """
    if not isinstance(text, str): return None
    slug = text.lower().strip()
    slug = re.sub(r'[^a-z0-9\s-]', '', slug) # Remove special chars
    slug = re.sub(r'[\s-]+', '_', slug)      # Replace spaces with underscore
    return slug

def add_ids_to_existing_questions():
    print("üöÄ Starting Migration: Adding IDs (Chapter, Topic, Topic_L2)...")

    # --- STEP 1: Build Lookup Maps for verified Syllabus items ---
    print("üìñ Reading Syllabus...")
    syllabus_doc = db.collection('static_data').document('syllabus').get()
    
    chapter_name_to_id = {}
    topic_name_to_id = {}

    if syllabus_doc.exists:
        data = syllabus_doc.to_dict()
        subjects = data.get('subjects', {})
        
        for sub_id, sub_data in subjects.items():
            chapters = sub_data.get('chapters', {})
            for chap_id, chap_data in chapters.items():
                c_name = chap_data.get('name', '').strip()
                if c_name:
                    chapter_name_to_id[c_name] = chap_id
                
                topics = chap_data.get('topics', {})
                for topic_id, topic_name in topics.items():
                    t_name = str(topic_name).strip()
                    if t_name:
                        topic_name_to_id[t_name] = topic_id
    else:
        print("‚ö†Ô∏è Warning: Syllabus not found. Only L2 IDs will be generated.")

    print(f"‚úÖ Loaded lookup tables: {len(chapter_name_to_id)} Chapters, {len(topic_name_to_id)} Topics.")

    # --- STEP 2: Update Questions ---
    print("üì• Fetching all questions...")
    questions_ref = db.collection('questions')
    docs = list(questions_ref.stream()) 
    
    print(f"üîÑ Scanning {len(docs)} questions...")
    
    batch = db.batch()
    batch_count = 0
    updated_total = 0
    
    for doc in tqdm(docs, desc="Processing", unit="q"):
        q_data = doc.to_dict()
        
        # Get existing fields
        current_chap_name = q_data.get('Chapter', '').strip()
        current_topic_name = q_data.get('Topic', '').strip()
        current_l2_name = q_data.get('Topic_L2', '').strip()
        
        updates = {}
        
        # 1. Chapter ID (Lookup)
        if current_chap_name in chapter_name_to_id:
            updates['chapterId'] = chapter_name_to_id[current_chap_name]

        # 2. Topic ID (Lookup)
        if current_topic_name in topic_name_to_id:
            updates['topicId'] = topic_name_to_id[current_topic_name]

        # 3. Topic L2 ID (Generate on the fly)
        # Since this isn't in our master syllabus, we standardize whatever string exists
        if current_l2_name:
            generated_id = generate_slug(current_l2_name)
            if generated_id:
                updates['topicL2Id'] = generated_id

        # Add to Batch
        if updates:
            doc_ref = questions_ref.document(doc.id)
            batch.update(doc_ref, updates)
            batch_count += 1
            updated_total += 1
        
        if batch_count >= 400:
            batch.commit()
            batch = db.batch()
            batch_count = 0

    if batch_count > 0:
        batch.commit()
    
    print("\n" + "="*40)
    print(f"üéâ MIGRATION COMPLETE!")
    print(f"‚úÖ Documents Updated: {updated_total} / {len(docs)}")
    print("="*40)



# --- Create option_set ---

def seed_static_data():
    # The reference to the document we want to create
    doc_ref = db.collection('static_data').document('option_sets')
    
    # Check if it already exists to prevent overwriting live data
    if doc_ref.get().exists:
        print("‚ö†Ô∏è  Aborting: 'static_data/option_sets' already exists.")
        print("   If you really want to reset it, delete the document in the console first.")
        return

    # 2. DATA: The Master Configuration
    # Note: target_years is dynamic in real apps, but static here as requested
    data = {
        "exams_list": [
            "NEET", 
            "JEE Main", 
            "JEE Advanced"
        ],
        "target_years": [
            2026, 
            2027, 
            2028, 
            2029
        ],
        "classes_list": [
            "Class XI", 
            "Class XII", 
            "Class XII (Passed)"
        ],
        "subjects_list": [
            "Physics", 
            "Chemistry", 
            "Maths", 
            "Biology"
        ],
        # CRITICAL: The atomic counter for generating student_ids
        "last_assigned_student_id": 0 
    }

    # 3. WRITE: Commit to database
    try:
        doc_ref.set(data)
        print("‚úÖ Success: 'static_data/option_sets' created.")
        print("   Initial Student ID counter set to 0.")
        print("   Dropdown options populated.")
    except Exception as e:
        print(f"‚ùå Error: {e}")



# --- ClearCollections ---


def clearCollections(collection_names, batch_size=50):
    """
    Deletes all documents in the specified list of collections.
    
    Args:
        collection_names (list): List of strings (collection names).
        batch_size (int): Number of docs to delete in one batch (default 50).
    """
    db = firestore.Client.from_service_account_json('serviceAccountKey.json')

    for coll_name in collection_names:
        print(f"Starting cleanup for collection: {coll_name}")
        coll_ref = db.collection(coll_name)
        
        while True:
            # Get a batch of documents
            docs = list(coll_ref.limit(batch_size).stream())
            deleted = 0

            if not docs:
                break

            batch = db.batch()
            for doc in docs:
                batch.delete(doc.reference)
                deleted += 1

            # Commit the batch
            batch.commit()
            print(f"Deleted {deleted} documents from '{coll_name}'...")

        print(f"Successfully cleared: {coll_name}")


# --- MAIN MENU ---

from firebase_admin import credentials, auth

def delete_all_users():
    """
    Lists all users in Firebase Auth and deletes them one by one.
    This effectively clears the 'Users' table in the Firebase Console.
    """
    try:
        # Fetch users in batches to be memory efficient
        page = auth.list_users()
        while page:
            for user in page.users:
                print(f"Deleting user: {user.uid} ({user.email})")
                auth.delete_user(user.uid)
            
            # Get next batch of users if they exist
            page = page.get_next_page()
            
        print("Successfully deleted all users from Firebase Auth.")
    except Exception as e:
        print(f"Error during bulk deletion: {e}")



# The data mapping based on your requirements
# Structure: "Exam_Subject": Seconds
IDEAL_TIME_MAP = {
    "NEET_Physics": 80,
    "JEE Main_Physics": 150,
    
    # You can easily add more here later:
    # "NEET_Chemistry": 60,
    # "JEE Main_Maths": 120,
}

def addIdealTimeMapToOptionSets():
    try:

        
        # 2. Reference the document
        doc_ref = db.collection('static_data').document('option_sets')
        
        # 3. Update the document
        # We use set(..., merge=True) to ensure we don't wipe out 
        # existing fields like 'exams_list' or 'classes_list'.
        doc_ref.set({
            'idealTimePerQuestion': IDEAL_TIME_MAP
        }, merge=True)
        
        print(f"‚úÖ Successfully updated /static_data/option_sets")
        print(f"   Added {len(IDEAL_TIME_MAP)} benchmarks.")
        
    except Exception as e:
        print(f"‚ùå Error updating Firestore: {e}")



# --- MAIN MENU ---

def main():
    while True:
        print("\n" + "="*40)
        print("      üõ†Ô∏è  ADHOC UTILITY MENU")
        print("="*40)
        print("1. Add Image Dimensions (Q & Sol) to DB (local csv)")
        print("2. Add Sylabbus tree to firebase")
        print("3. Add chapterID, topicID, topic_l2ID to existing questions in firebase")
        print("4. Create option sets document in static_data collection")
        print ("5. clearCollection [names already provided in file]")
        print ("6. Delete all user records from auth system")
        print ("7. Add ideal time per question map")
        print("0. Exit")
        print("-" * 40)
        
        choice = input("Enter choice: ").strip()
        
        if choice == '1':
            func_populate_dimensions()
        
        if choice == '2':
            update_syllabus_tree()

        if choice == '3':
            add_ids_to_existing_questions()

        if choice == '4':
            seed_static_data()

        if choice == '5':
            clearCollections(['users','attempts','attempt_items','questions_curation','student_question_tracker'])

        if choice == '6':
            delete_all_users()

        if choice == '7':
            addIdealTimeMapToOptionSets()

        elif choice == '0':
            print("Bye! üëã")
            sys.exit()
        else:
            print("‚ùå Invalid choice. Try again.")

if __name__ == "__main__":
    main()

================================================================================
FILE: AIsuggestedTags.py
================================================================================
import pandas as pd
import ollama
import os
import json
import time

def run_universal_auditor():
    # --- 1. CONFIGURATION ---
    CONFIG_PATH = 'config.json'
    
    # Defaults
    config = {
        "BASE_PATH": r"D:\Main\3. Work - Teaching\Projects\Question extractor",
        "DB_FILENAME": "DB Master.xlsx",
        "OCR_FILENAME": "DB Master_OCR.csv",
        "METADATA_FILENAME": "DB Metadata.xlsx"
    }

    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r') as f:
            loaded = json.load(f)
            config.update(loaded)
    else:
        print("‚ö†Ô∏è Config not found. Using internal defaults.")

    BASE_PATH = config['BASE_PATH']
    # If paths are just filenames, join them with BASE_PATH
    input_csv = os.path.join(BASE_PATH, config.get("OCR_FILENAME", "DB Master_OCR.csv"))
    metadata_path = os.path.join(BASE_PATH, config.get("METADATA_FILENAME", "DB Metadata.xlsx"))
    suggested_csv = os.path.join(BASE_PATH, "Suggested_Tags.csv")
    
    img_base_path = os.path.join(BASE_PATH, "Processed_Database")
    
    client = ollama.Client(host='http://localhost:11434', timeout=180.0)

    # --- 2. BUILD HIERARCHY ---
    print("\n[SYSTEM] Initializing Syllabus Hierarchy...")
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        meta_df = meta_df.map(lambda x: str(x).strip() if pd.notna(x) else "nan")

        taxonomy = {}
        for ch, group in meta_df.groupby('Chapter'):
            taxonomy[ch] = {}
            for top, sub_group in group.groupby('Topic'):
                if top.lower() != "nan" and top.lower() != ch.lower():
                    l2s = [v for v in sub_group['Topic_L2'].unique() if v.lower() != "nan"]
                    cleaned_l2s = []
                    for item in l2s:
                        cleaned_l2s.extend([i.strip() for i in item.replace('[','').replace(']','').replace("'",'').split(',')])
                    taxonomy[ch][top] = sorted(list(set(cleaned_l2s)))
    except Exception as e:
        print(f"‚ùå Critical Error loading Metadata: {e}")
        return

    # --- 3. LOAD DATA & FIX SCHEMA ---
    if not os.path.exists(input_csv):
        print(f"‚ùå Input CSV not found: {input_csv}")
        return

    df_main = pd.read_csv(input_csv)
    output_cols = ['Q', 'Exam', 'Subject', 'Difficulty_tag', 'Chapter', 'Topic', 'Topic_L2', 
                   'unique_id', 'OCR_Text', 'Sug_Chapter', 'Sug_Topic', 'Sug_Topic_L2', 
                   'Sug_Difficulty', 'AI_Reasoning', 'AI_Confidence', 'AI_Tag_Accepted']

    processed_ids = set()
    
    # Check existing file
    if os.path.exists(suggested_csv):
        try:
            res_df = pd.read_csv(suggested_csv)
            
            # Schema Fix: Add missing column if needed
            if 'AI_Tag_Accepted' not in res_df.columns:
                print("[SYSTEM] Updating existing CSV schema to include 'AI_Tag_Accepted'...")
                res_df['AI_Tag_Accepted'] = 'No' 
                res_df = res_df.reindex(columns=output_cols) 
                res_df.to_csv(suggested_csv, index=False)
                print("[SYSTEM] Schema updated successfully.")
            
            processed_ids = set(res_df['unique_id'].astype(str).unique())
        except Exception as e:
            print(f"[ERROR] Could not read existing results file: {e}")
            print("[SYSTEM] Starting fresh to avoid corruption.")
            pd.DataFrame(columns=output_cols).to_csv(suggested_csv, index=False)
    else:
        pd.DataFrame(columns=output_cols).to_csv(suggested_csv, index=False)

    # Filter Pending Questions
    df_to_do = df_main[~df_main['unique_id'].astype(str).isin(processed_ids)].copy()
    
    print(f"\n[SYSTEM] Starting Audit: {len(df_to_do)} questions remaining.\n")

    # --- 4. PROCESSING LOOP ---
    for index, row in df_to_do.iterrows():
        q_start = time.perf_counter()
        
        # Variables
        q_num = str(row['Q']).strip()
        u_id = str(row['unique_id']).strip()
        folder = str(row['Folder']).strip()
        ocr_text = str(row['OCR_Text'])[:1200]
        
        # --- STAGE 1: CHAPTER ---
        existing_chapter = str(row['Chapter']).strip()
        
        if existing_chapter.lower() not in ["nan", "unknown", "", "none"] and existing_chapter in taxonomy:
            print(f"\n>>> [Q {q_num}] STAGE 1: [SKIPPED] - CHAPTER KNOWN: '{existing_chapter}'")
            chapter = existing_chapter
        else:
            # AI Selects Chapter
            ch_list = sorted(list(taxonomy.keys()))
            ch_prompt = f"""
            ### ROLE: Senior Physics Faculty
            ### QUESTION:
            {ocr_text}

            ### TASK:
            Identify the single Chapter this question belongs to.
            """
            print(f"\n>>> [Q {q_num}] STAGE 1 PROMPT (CHAPTER):\n{ch_prompt}")
            try:
                ch_resp = client.chat(model='llama3.1', messages=[{'role': 'user', 'content': ch_prompt}], 
                                     format={'type':'object', 'properties':{'chapter':{'enum': ch_list + ["Miscellaneous"]}}, 'required':['chapter']})
                chapter = json.loads(ch_resp['message']['content']).get('chapter')
            except:
                chapter = "Miscellaneous"
            print(f"   [AI SELECTION]: {chapter}")

        # --- STAGE 2: TOPIC ---
        topics_available = taxonomy.get(chapter, {})
        topic_list = sorted(list(topics_available.keys())) + ["Miscellaneous"]
        
        existing_topic = str(row['Topic']).strip()
        
        # LOGIC UPGRADE: If Topic is known AND valid for this Chapter, skip prompt
        if existing_topic.lower() not in ["nan", "unknown", "", "none"] and existing_topic in topics_available:
            print(f">>> [Q {q_num}] STAGE 2: [SKIPPED] - TOPIC KNOWN: '{existing_topic}'")
            selected_topic = existing_topic
            t_data = {"reasoning": "Pre-classified in source."}
        else:
            # AI Selects Topic
            t_prompt = f"""
            ### ROLE: Senior Physics Faculty
            ### QUESTION:
            {ocr_text}

            ### CONTEXT:
            Chapter: {chapter}

            ### ALLOWED TOPICS:
            {topic_list}

            ### TASK:
            Analyze the physical setup and select the most specific Topic.

            ### GENERAL GUIDELINES:
            1. **Keyword Matching:** Look for specific physics terms in the question (e.g., 'Flux', 'Inertia', 'Interference') and match them to the Topic list.
            2. **Process vs. Property:** - If the question asks to calculate a static value (like Resistance, Inductance, Moment of Inertia), choose the Topic representing that property.
               - If the question involves a change or interaction (like Conservation, Decay, Collision), choose the Topic representing that process.
            3. **Fallback:** Only use 'Miscellaneous' if the concept is completely absent from the list.
            """
            print(f"\n>>> [Q {q_num}] STAGE 2 PROMPT (TOPIC):\n{t_prompt}")
            
            try:
                t_resp = client.chat(model='llama3.1', messages=[{'role': 'user', 'content': t_prompt}], 
                                     format={'type':'object', 'properties':{'topic':{'enum': topic_list}, 'reasoning':{'type':'string'}}, 'required':['topic', 'reasoning']})
                t_data = json.loads(t_resp['message']['content'])
                selected_topic = t_data.get('topic')
            except:
                selected_topic = "Miscellaneous"
                t_data = {"reasoning": "Error in AI response"}

            print(f"   [AI SELECTION]: {selected_topic}")
            print(f"   [REASONING]:    {t_data.get('reasoning')}")

        # --- STAGE 3: L2 (SUB-TOPIC) ---
        l2_list = topics_available.get(selected_topic, [])
        
        if not l2_list:
            final_l2 = "N/A"
            # Just assess difficulty if no L2s exist
            f_prompt = f"### QUESTION:\n{ocr_text}\n\nTASK: Assess Difficulty."
            try:
                f_resp = client.chat(model='llama3.1', messages=[{'role': 'user', 'content': f_prompt}], 
                                     format={'type':'object', 'properties':{'difficulty':{'enum':['Easy','Medium','Difficult']}, 'confidence':{'type':'integer','minimum':1}}, 'required':['difficulty', 'confidence']})
                f_data = json.loads(f_resp['message']['content'])
            except:
                f_data = {"difficulty": "Unknown", "confidence": 0}
        else:
            l2_options = l2_list + ["Miscellaneous"]
            f_prompt = f"""
            ### ROLE: JEE Physics Subject Matter Expert
            ### QUESTION:
            {ocr_text}

            ### CATEGORY:
            {chapter} > {selected_topic}

            ### ALLOWED SUB-TOPICS (L2):
            {l2_options}

            ### GUIDELINES:
            1. **Specificity:** Choose the sub-topic that describes the specific *case* or *object* in the question (e.g., 'Solenoid' vs 'Toroid', 'Sphere' vs 'Disc').
            2. **Theorem Application:** If the solution requires a specific named theorem/law mentioned in the list, select that.
            3. **Standard Cases:** If the question refers to a standard textbook setup, choose the sub-topic that matches that setup.
            
            ### TASK:
            Select the most precise L2 tag.
            """
            print(f"\n>>> [Q {q_num}] STAGE 3 PROMPT (L2):\n{f_prompt}")
            
            try:
                f_resp = client.chat(model='llama3.1', messages=[{'role': 'user', 'content': f_prompt}], 
                                     format={'type':'object', 'properties':{'topic_l2':{'enum': l2_options}, 'difficulty':{'enum':['Easy','Medium','Difficult']}, 'confidence':{'type':'integer','minimum':1}}, 'required':['topic_l2', 'difficulty', 'confidence']})
                f_data = json.loads(f_resp['message']['content'])
                final_l2 = f_data.get('topic_l2')
            except:
                final_l2 = "Miscellaneous"
                f_data = {"difficulty": "Unknown", "confidence": 0}

            print(f"   [AI SELECTION]: {final_l2}")

        # --- SAVE ---
        new_row = {
            'Q': q_num, 'Exam': row['Exam'], 'Subject': row['Subject'], 
            'Difficulty_tag': row['Difficulty_tag'], 'Chapter': row['Chapter'], 
            'Topic': row['Topic'], 'Topic_L2': row['Topic_L2'], 'unique_id': u_id, 
            'OCR_Text': ocr_text, 'Sug_Chapter': chapter, 'Sug_Topic': selected_topic, 
            'Sug_Topic_L2': final_l2, 'Sug_Difficulty': f_data.get('difficulty'), 
            'AI_Reasoning': t_data.get('reasoning'), 'AI_Confidence': f_data.get('confidence'),
            'AI_Tag_Accepted': 'No' 
        }
        
        # Enforce column order and save
        save_df = pd.DataFrame([new_row])
        save_df = save_df[output_cols] 
        save_df.to_csv(suggested_csv, mode='a', header=False, index=False)

        q_time = time.perf_counter() - q_start
        
        # --- FINAL BLOCK ---
        print("\n" + "#" * 100)
        print(f"   RESULTS FOR QUESTION {q_num} (ID: {u_id})")
        print("#" * 100)
        print(f" >> TIME TAKEN:    {q_time:.2f}s")
        print(f" >> FILEPATH:      {os.path.join(img_base_path, folder, f'Q_{q_num}.png')}")
        print(f" >> FINAL TAGS:    [{chapter}] -> [{selected_topic}] -> [{final_l2}]")
        print(f" >> ASSESSMENT:    {f_data.get('difficulty')} (Confidence: {f_data.get('confidence')}%)")
        print("-" * 100)
        print(f" >> REASONING:     {t_data.get('reasoning')}")
        print("#" * 100 + "\n\n")

if __name__ == "__main__":
    run_universal_auditor()

================================================================================
FILE: AIsuggestedTagsGroq - Copy.py
================================================================================
import pandas as pd
import os
import json
import time
from groq import Groq
from tqdm import tqdm

# --- 1. CONFIGURATION ---
BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
TARGET_FILENAME = "questionToTagUsingAI.csv"
METADATA_FILENAME = "DB Metadata.xlsx"

# API KEY
GROQ_API_KEY = "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"

# BATCH SETTINGS
BATCH_SIZE = 5  # Small batch size for reliability

MODEL_QUEUE = [
    "llama-3.3-70b-versatile",
    "llama-3.1-70b-versatile",
    "deepseek-r1-distill-llama-70b",
    "mixtral-8x7b-32768"
]

client = Groq(api_key=GROQ_API_KEY)
current_model_index = 0

# --- 2. INDEX-BASED MAPPER ---
def load_indexed_syllabus(metadata_path):
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        meta_df = meta_df.map(lambda x: str(x).strip() if pd.notna(x) else "nan")
        
        # 1. Build Chapter Index
        chapters = sorted([c for c in meta_df['Chapter'].unique() if c.lower() not in ["nan", "unknown"]])
        chapter_map = {i+1: name for i, name in enumerate(chapters)}
        
        # 2. Build Topic Index per Chapter
        topic_map = {}
        for ch in chapters:
            group = meta_df[meta_df['Chapter'] == ch]
            topics = sorted([t for t in group['Topic'].unique() if t.lower() not in ["nan", "miscellaneous"]])
            topic_map[ch] = {i+1: name for i, name in enumerate(topics)}
            
        return chapter_map, topic_map
    except Exception as e:
        print(f"‚ùå Error loading syllabus: {e}")
        return {}, {}

def generate_chapter_menu_with_topics(chapter_map, topic_map):
    lines = []
    for cid, cname in chapter_map.items():
        topics_dict = topic_map.get(cname, {})
        t_list = list(topics_dict.values())
        # Context: Show first 20 topics to help AI decide
        t_str = ", ".join(t_list[:20]) 
        lines.append(f"{cid}. {cname}\n   (Includes: {t_str})")
    return "\n".join(lines)

def generate_topic_menu(options_dict):
    return "\n".join([f"{k}. {v}" for k, v in options_dict.items()])

# --- 3. AI INTERACTION ---
def call_ai_with_retry(system_prompt, user_prompt):
    global current_model_index
    
    while current_model_index < len(MODEL_QUEUE):
        model = MODEL_QUEUE[current_model_index]
        try:
            # --- DEBUG: FULL PROMPT LOGGING ---
            print(f"\n\n{'='*20} SENDING TO {model} {'='*20}")
            print(f"--- SYSTEM PROMPT ---\n{system_prompt}")
            print(f"--- USER PROMPT ---\n{user_prompt}")
            print("="*60)
            
            completion = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0, 
                response_format={"type": "json_object"}
            )
            
            raw_resp = completion.choices[0].message.content
            
            # --- DEBUG: FULL RESPONSE LOGGING ---
            print(f"--- RESPONSE RECEIVED ---\n{raw_resp}\n{'='*60}\n")
            
            return json.loads(raw_resp), model

        except Exception as e:
            err = str(e).lower()
            if "429" in err or "rate limit" in err:
                print(f"‚ö†Ô∏è Rate Limit on {model}. Switching...")
                current_model_index += 1
            else:
                print(f"‚ùå Error on {model}: {e}. Switching...")
                current_model_index += 1
    return None, None

# --- 4. MAIN LOGIC ---
def run_index_tagger():
    target_csv = os.path.join(BASE_PATH, TARGET_FILENAME)
    
    print("1Ô∏è‚É£ Loading Syllabus Indices...")
    chapter_idx_map, topic_master_map = load_indexed_syllabus(os.path.join(BASE_PATH, METADATA_FILENAME))
    chapter_menu_str = generate_chapter_menu_with_topics(chapter_idx_map, topic_master_map)
    
    print("2Ô∏è‚É£ Loading Data...")
    if not os.path.exists(target_csv):
        print(f"‚ùå Error: {target_csv} does not exist.")
        return
        
    df = pd.read_csv(target_csv)
    
    for c in ['Chapter', 'Topic', 'AI_Reasoning', 'Model_Used']:
        if c not in df.columns: df[c] = None

    if 'OCR_Text' not in df.columns:
        print("‚ùå CRITICAL ERROR: 'OCR_Text' column missing.")
        return

    # Filter Valid Rows
    def is_bad(val): return str(val).lower().strip() in ["nan", "none", "", "unknown", "ai_missed_id"]
    
    pending_indices = []
    for idx, row in df.iterrows():
        if is_bad(row['Chapter']):
            # Verify OCR text is present
            if pd.notna(row['OCR_Text']) and str(row['OCR_Text']).lower() != 'nan' and str(row['OCR_Text']).strip() != "":
                pending_indices.append(idx)
    
    print(f"üëâ Found {len(pending_indices)} valid questions to tag.")
    
    # --- BATCH LOOP ---
    for i in range(0, len(pending_indices), BATCH_SIZE):
        batch_idx = pending_indices[i : i + BATCH_SIZE]
        batch_df = df.loc[batch_idx]
        
        # --- STEP 1: IDENTIFY CHAPTERS ---
        q_block = ""
        for idx, row in batch_df.iterrows():
            text = str(row['OCR_Text']).replace("\n", " ") 
            q_block += f"Q_ID_{idx}: {text}\n"

        chap_system = f"""
You are a Physics Subject Matter Expert for JEE/NEET.
You are provided with a Reference Syllabus List below.

[CONSTRAINT: MUTUALLY EXCLUSIVE, COLLECTIVELY EXHAUSTIVE]
1. The list below is EXHAUSTIVE. Every single physics question belongs to exactly ONE Chapter ID from this list.
2. You CANNOT create new chapters. You CANNOT say "None" or "Other".
3. Force a selection based on the strongest conceptual overlap.

[CHAPTER LIST]
{chapter_menu_str}

[TASK]
1. Identify the Chapter ID for each question.
2. Provide short reasoning explaining the link to the topics in that chapter.
3. OUTPUT JSON ONLY: {{ "Q_ID_12": {{ "chapter_id": 5, "reasoning": "..." }} }}
"""
        chap_user = f"Questions:\n{q_block}"
        
        chap_resp, model = call_ai_with_retry(chap_system, chap_user)
        if not chap_resp: break

        # --- STEP 2: PROCESS & GROUP ---
        questions_by_chapter = {} 

        for idx in batch_idx:
            key = f"Q_ID_{idx}"
            if key in chap_resp:
                try:
                    cid = int(chap_resp[key].get('chapter_id'))
                    reason = chap_resp[key].get('reasoning', '')
                    
                    if cid in chapter_idx_map:
                        chapter_name = chapter_idx_map[cid]
                        df.at[idx, 'Chapter'] = chapter_name
                        df.at[idx, 'AI_Reasoning'] = reason
                        df.at[idx, 'Model_Used'] = model
                        
                        if chapter_name not in questions_by_chapter:
                            questions_by_chapter[chapter_name] = []
                        questions_by_chapter[chapter_name].append(idx)
                    else:
                        df.at[idx, 'AI_Reasoning'] = f"Invalid_ID: {cid}"
                except:
                    df.at[idx, 'AI_Reasoning'] = "Format_Error"

        # --- STEP 3: TOPIC DRILL-DOWN ---
        for ch_name, q_indices in questions_by_chapter.items():
            if ch_name not in topic_master_map or not topic_master_map[ch_name]:
                continue
                
            topic_menu = topic_master_map[ch_name]
            topic_menu_str = generate_topic_menu(topic_menu)
            
            sub_q_block = ""
            for idx in q_indices:
                text = str(df.at[idx, 'OCR_Text']).replace("\n", " ")
                sub_q_block += f"Q_ID_{idx}: {text}\n"
            
            topic_system = f"""
You are a Physics Expert.
The following questions have been mapped to Chapter: '{ch_name}'.

[CONSTRAINT: MUTUALLY EXCLUSIVE, COLLECTIVELY EXHAUSTIVE]
1. Below is the COMPLETE list of valid topics for this chapter.
2. You MUST select exactly ONE Topic ID from this list.
3. Do NOT invent topics. Do NOT use "General" or "Misc" unless it appears in the numbered list below.

[TOPIC LIST FOR '{ch_name}']
{topic_menu_str}

[TASK]
Select the Topic ID that best fits the question.
OUTPUT JSON ONLY: {{ "Q_ID_12": {{ "topic_id": 3 }} }}
"""
            topic_user = f"Questions:\n{sub_q_block}"
            
            topic_resp, _ = call_ai_with_retry(topic_system, topic_user)
            
            if topic_resp:
                for idx in q_indices:
                    key = f"Q_ID_{idx}"
                    if key in topic_resp:
                        try:
                            tid = int(topic_resp[key].get('topic_id'))
                            if tid in topic_menu:
                                df.at[idx, 'Topic'] = topic_menu[tid]
                            else:
                                df.at[idx, 'Topic'] = "Unknown_ID_Returned"
                        except:
                            pass

        df.to_csv(target_csv, index=False)
        print(f"‚úÖ Batch Saved.")
        time.sleep(1)

if __name__ == "__main__":
    run_index_tagger()

================================================================================
FILE: AIsuggestedTagsGroq.py
================================================================================
import pandas as pd
import os
import json
import time
from groq import Groq
from tqdm import tqdm

# --- 1. CONFIGURATION ---
BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
TARGET_FILENAME = "questionToTagUsingAI.csv"
METADATA_FILENAME = "DB Metadata.xlsx"

# API KEY
GROQ_API_KEY = "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"

# BATCH SETTINGS
BATCH_SIZE = 10  # Small batch size for reliability
VERBOSE = False  # <--- TOGGLE THIS TO FALSE TO HIDE PRINT STATEMENTS

MODEL_QUEUE = [

    "canopylabs/orpheus-arabic-saudi",
    
    "openai/gpt-oss-120b", # OK

    "openai/gpt-oss-safeguard-20b",

    "openai/gpt-oss-20b", # OK


    "meta-llama/llama-4-maverick-17b-128e-instruct", # OK


    "moonshotai/kimi-k2-instruct-0905",

    "llama-3.3-70b-versatile", # OK

    "meta-llama/llama-guard-4-12b", # Rate limit too low


    "meta-llama/llama-prompt-guard-2-22m",

    "qwen/qwen3-32b"
    "meta-llama/llama-4-scout-17b-16e-instruct", # Rubbish    
    "meta-llama/llama-prompt-guard-2-86m",
]

client = Groq(api_key=GROQ_API_KEY)
current_model_index = 0

# --- HELPER: CONDITIONAL PRINT ---
def log(msg):
    if VERBOSE:
        print(msg)

# --- 2. INDEX-BASED MAPPER ---
def load_indexed_syllabus(metadata_path):
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        meta_df = meta_df.map(lambda x: str(x).strip() if pd.notna(x) else "nan")
        
        # 1. Build Chapter Index
        chapters = sorted([c for c in meta_df['Chapter'].unique() if c.lower() not in ["nan", "unknown"]])
        chapter_map = {i+1: name for i, name in enumerate(chapters)}
        
        # 2. Build Topic Index per Chapter
        topic_map = {}
        for ch in chapters:
            group = meta_df[meta_df['Chapter'] == ch]
            topics = sorted([t for t in group['Topic'].unique() if t.lower() not in ["nan", "miscellaneous"]])
            topic_map[ch] = {i+1: name for i, name in enumerate(topics)}
            
        return chapter_map, topic_map
    except Exception as e:
        print(f"‚ùå Error loading syllabus: {e}")
        return {}, {}

def generate_chapter_menu_with_topics(chapter_map, topic_map):
    lines = []
    for cid, cname in chapter_map.items():
        topics_dict = topic_map.get(cname, {})
        t_list = list(topics_dict.values())
        # Context: Show first 20 topics to help AI decide
        t_str = ", ".join(t_list[:20]) 
        lines.append(f"{cid}. {cname}\n   (Includes: {t_str})")
    return "\n".join(lines)

def generate_topic_menu(options_dict):
    return "\n".join([f"{k}. {v}" for k, v in options_dict.items()])

# --- 3. AI INTERACTION ---
def call_ai_with_retry(system_prompt, user_prompt):
    global current_model_index
    
    # Loop through models starting from the current index
    # We try as many times as there are models in the queue
    for _ in range(len(MODEL_QUEUE)):
        # If index goes out of bounds, reset (optional, or stop) - here we clamp to bounds
        if current_model_index >= len(MODEL_QUEUE):
            current_model_index = 0 

        model = MODEL_QUEUE[current_model_index]
        
        try:
            # --- DEBUG: FULL PROMPT LOGGING ---
            if VERBOSE:
                print(f"\n\n{'='*20} SENDING TO {model} {'='*20}")
                print(f"--- SYSTEM PROMPT ---\n{system_prompt}")
                print(f"--- USER PROMPT ---\n{user_prompt}")
                print("="*60)
            
            completion = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0, 
                response_format={"type": "json_object"}
            )
            
            raw_resp = completion.choices[0].message.content
            
            # --- DEBUG: FULL RESPONSE LOGGING ---
            if VERBOSE:
                print(f"--- RESPONSE RECEIVED ---\n{raw_resp}\n{'='*60}\n")
            
            return json.loads(raw_resp), model

        except Exception as e:
            err = str(e).lower()
            if "429" in err or "rate limit" in err:
                print(f"‚ö†Ô∏è Rate Limit on {model}. Switching to next model...")
            else:
                print(f"‚ùå Error on {model}: {e}. Switching to next model...")
            
            # Move to next model for the next iteration of the loop
            current_model_index += 1
            time.sleep(1) # Brief pause before retry

    print("‚ùå All models failed or exhausted.")
    return None, None

# --- 4. MAIN LOGIC ---
def run_index_tagger():
    target_csv = os.path.join(BASE_PATH, TARGET_FILENAME)
    
    print("1Ô∏è‚É£ Loading Syllabus Indices...")
    chapter_idx_map, topic_master_map = load_indexed_syllabus(os.path.join(BASE_PATH, METADATA_FILENAME))
    chapter_menu_str = generate_chapter_menu_with_topics(chapter_idx_map, topic_master_map)
    
    print("2Ô∏è‚É£ Loading Data...")
    if not os.path.exists(target_csv):
        print(f"‚ùå Error: {target_csv} does not exist.")
        return
        
    df = pd.read_csv(target_csv)
    
    for c in ['Chapter', 'Topic', 'AI_Reasoning', 'Model_Used']:
        if c not in df.columns: df[c] = None

    if 'OCR_Text' not in df.columns:
        print("‚ùå CRITICAL ERROR: 'OCR_Text' column missing.")
        return

    # Filter Valid Rows
    def is_bad(val): return str(val).lower().strip() in ["nan", "none", "", "unknown", "ai_missed_id"]
    
    pending_indices = []
    for idx, row in df.iterrows():
        if is_bad(row['Chapter']):
            # Verify OCR text is present
            if pd.notna(row['OCR_Text']) and str(row['OCR_Text']).lower() != 'nan' and str(row['OCR_Text']).strip() != "":
                pending_indices.append(idx)
    
    print(f"üëâ Found {len(pending_indices)} valid questions to tag.")
    
    # --- BATCH LOOP ---
    for i in range(0, len(pending_indices), BATCH_SIZE):
        batch_idx = pending_indices[i : i + BATCH_SIZE]
        batch_df = df.loc[batch_idx]
        
        # --- STEP 1: IDENTIFY CHAPTERS ---
        q_block = ""
        for idx, row in batch_df.iterrows():
            text = str(row['OCR_Text']).replace("\n", " ") 
            q_block += f"Q_ID_{idx}: {text}\n"

        chap_system = f"""
You are a Physics Subject Matter Expert for JEE/NEET.
You are provided with a Reference Syllabus List below.

[CONSTRAINT: MUTUALLY EXCLUSIVE, COLLECTIVELY EXHAUSTIVE]
1. The list below is EXHAUSTIVE. Every single physics question belongs to exactly ONE Chapter ID from this list.
2. You CANNOT create new chapters. You CANNOT say "None" or "Other".
3. Force a selection based on the strongest conceptual overlap.

[CHAPTER LIST]
{chapter_menu_str}

[TASK]
1. Identify the Chapter ID for each question.
2. Provide short reasoning explaining the link to the topics in that chapter.
3. OUTPUT JSON ONLY:Example {{ "Q_ID_12": {{ "chapter_id": 5, "reasoning": "..." }} }}
"""
        chap_user = f"Questions:\n{q_block}"
        
        chap_resp, model = call_ai_with_retry(chap_system, chap_user)
        if not chap_resp: break

        # --- STEP 2: PROCESS & GROUP ---
        questions_by_chapter = {} 

        for idx in batch_idx:
            key = f"Q_ID_{idx}"
            if key in chap_resp:
                try:
                    cid = int(chap_resp[key].get('chapter_id'))
                    reason = chap_resp[key].get('reasoning', '')
                    
                    if cid in chapter_idx_map:
                        chapter_name = chapter_idx_map[cid]
                        df.at[idx, 'Chapter'] = chapter_name
                        df.at[idx, 'AI_Reasoning'] = reason
                        df.at[idx, 'Model_Used'] = model
                        
                        if chapter_name not in questions_by_chapter:
                            questions_by_chapter[chapter_name] = []
                        questions_by_chapter[chapter_name].append(idx)
                    else:
                        df.at[idx, 'AI_Reasoning'] = f"Invalid_ID: {cid}"
                except:
                    df.at[idx, 'AI_Reasoning'] = "Format_Error"

        # --- STEP 3: TOPIC DRILL-DOWN ---
        for ch_name, q_indices in questions_by_chapter.items():
            if ch_name not in topic_master_map or not topic_master_map[ch_name]:
                continue
                
            topic_menu = topic_master_map[ch_name]
            topic_menu_str = generate_topic_menu(topic_menu)
            
            sub_q_block = ""
            for idx in q_indices:
                text = str(df.at[idx, 'OCR_Text']).replace("\n", " ")
                sub_q_block += f"Q_ID_{idx}: {text}\n"
            
            topic_system = f"""
You are a Physics Expert.
The following questions have been mapped to Chapter: '{ch_name}'.

[CONSTRAINT: MUTUALLY EXCLUSIVE, COLLECTIVELY EXHAUSTIVE]
1. Below is the COMPLETE list of valid topics for this chapter.
2. You MUST select exactly ONE Topic ID from this list.
3. Do NOT invent topics. Do NOT use "General" or "Misc" unless it appears in the numbered list below.

[TOPIC LIST FOR '{ch_name}']
{topic_menu_str}

[TASK]
Select the Topic ID that best fits the question.
OUTPUT JSON ONLY:Example {{ "Q_ID_12": {{ "topic_id": 3 }} }}
"""
            topic_user = f"Questions:\n{sub_q_block}"
            
            topic_resp, _ = call_ai_with_retry(topic_system, topic_user)
            
            if topic_resp:
                for idx in q_indices:
                    key = f"Q_ID_{idx}"
                    if key in topic_resp:
                        try:
                            tid = int(topic_resp[key].get('topic_id'))
                            if tid in topic_menu:
                                df.at[idx, 'Topic'] = topic_menu[tid]
                            else:
                                df.at[idx, 'Topic'] = "Unknown_ID_Returned"
                        except:
                            pass

        df.to_csv(target_csv, index=False)
        print(f"‚úÖ Batch Saved.")
        time.sleep(1)

if __name__ == "__main__":
    run_index_tagger()

================================================================================
FILE: AIsuggestedTagsLight.py
================================================================================
import pandas as pd
import ollama
import os
import json
import time
import sys

def run_universal_auditor():
    # --- 1. CONFIGURATION ---
    BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
    TARGET_FILENAME = "questionToTagUsingAI.csv"
    METADATA_FILENAME = "DB Metadata.xlsx"

    target_csv = os.path.join(BASE_PATH, TARGET_FILENAME)
    metadata_path = os.path.join(BASE_PATH, METADATA_FILENAME)
    
    # Using Llama 3.1
    MODEL_NAME = 'llama3.1' 
    client = ollama.Client(host='http://localhost:11434', timeout=180.0)

    # --- 2. BUILD HIERARCHY & SYLLABUS CONTEXT ---
    print("\n[SYSTEM] Initializing Syllabus Hierarchy...")
    syllabus_context_str = ""
    
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        meta_df = meta_df.map(lambda x: str(x).strip() if pd.notna(x) else "nan")

        taxonomy = {}
        syllabus_lines = []

        for ch, group in meta_df.groupby('Chapter'):
            if ch.lower() in ["unknown", "nan", "none", "", "miscellaneous"]:
                continue

            taxonomy[ch] = {}
            topics_in_chapter = []
            
            for top, sub_group in group.groupby('Topic'):
                if top.lower() not in ["nan", "unknown", "none", "", "miscellaneous", ch.lower()]:
                    topics_in_chapter.append(top)
                    
                    l2s = [v for v in sub_group['Topic_L2'].unique() if v.lower() != "nan"]
                    cleaned_l2s = []
                    for item in l2s:
                        cleaned_l2s.extend([i.strip() for i in item.replace('[','').replace(']','').replace("'",'').split(',')])
                    
                    valid_l2s = [x for x in cleaned_l2s if x and x.lower() != "miscellaneous"]
                    taxonomy[ch][top] = sorted(list(set(valid_l2s)))
            
            # Create the mapping string: "Chapter Name: Topic 1, Topic 2, Topic 3"
            if topics_in_chapter:
                syllabus_lines.append(f"- {ch}: {', '.join(topics_in_chapter)}")
        
        syllabus_context_str = "\n".join(syllabus_lines)

    except Exception as e:
        print(f"‚ùå Critical Error loading Metadata: {e}")
        return

    # --- 3. LOAD DATA ---
    if not os.path.exists(target_csv):
        print(f"‚ùå Target CSV not found: {target_csv}")
        return

    print(f"[SYSTEM] Loading {TARGET_FILENAME}...")
    df_main = pd.read_csv(target_csv)

    ai_meta_cols = ['AI_Reasoning', 'AI_Confidence', 'AI_Tag_Accepted']
    for col in ai_meta_cols:
        if col not in df_main.columns:
            df_main[col] = None 
            if col == 'AI_Tag_Accepted':
                df_main[col] = 'No'

    pending_indices = df_main[df_main['AI_Reasoning'].isna() | (df_main['AI_Reasoning'] == "")].index.tolist()
    
    print(f"\n[SYSTEM] Starting Strict Audit using {MODEL_NAME}.")
    print(f"        Total Rows: {len(df_main)}")
    print(f"        Pending:    {len(pending_indices)}")

    stats = {
        "total_processed": 0,
        "chapter_was_missing": 0,
        "topic_was_missing": 0
    }

    # --- 4. PROCESSING LOOP ---
    try:
        for idx in pending_indices:
            row = df_main.loc[idx]
            q_start = time.perf_counter()
            
            q_num = str(row['Q']).strip()
            ocr_text = str(row['OCR_Text'])[:1200]
            orig_chapter = str(row['Chapter']).strip()
            
            # --- STAGE 1: CHAPTER ---
            # Check if original chapter is valid
            existing_chapter = orig_chapter
            is_chapter_known = existing_chapter.lower() not in ["nan", "unknown", "", "none"] and existing_chapter in taxonomy
            
            if not is_chapter_known:
                stats["chapter_was_missing"] += 1
            
            if is_chapter_known:
                print(f"\n>>> [Q {q_num}] STAGE 1: [SKIPPED] - CHAPTER KNOWN: '{existing_chapter}'")
                chapter = existing_chapter
            else:
                # LIST FOR ENUM (OUTPUT CONSTRAINTS)
                ch_list = sorted(list(taxonomy.keys()))
                
                # *** CORRECTION HERE: Sending syllabus_context_str (Chapter + Topics) instead of just list ***
                ch_prompt = f"""
You are an expert Physics Faculty for JEE/NEET.

[SYLLABUS MAP (Chapter: Topics Included)]
{syllabus_context_str}

[STRICT RULES]
1. You MUST select exactly one Chapter Name from the map above.
2. Read the topics in the map. If the question mentions concepts (e.g., "centripetal acceleration"), find which Chapter contains that Topic.
3. You are FORBIDDEN from creating new categories.
4. Output JSON only: {{"chapter": "Exact Chapter Name"}}

[QUESTION TEXT]
{ocr_text}
"""
                
                print(f"\n{'='*20} FULL PROMPT (CHAPTER) {'='*20}")
                print(ch_prompt)
                print(f"{'='*60}")

                try:
                    ch_resp = client.chat(
                        model=MODEL_NAME, 
                        messages=[{'role': 'user', 'content': ch_prompt}], 
                        format={'type':'object', 'properties':{'chapter':{'enum': ch_list}}, 'required':['chapter']}
                    )
                    chapter = json.loads(ch_resp['message']['content']).get('chapter')
                except Exception as e:
                    print(f"API Error: {e}")
                    chapter = "MANUAL_REVIEW_REQUIRED"
                
                print(f"   [AI SELECTION]: {chapter}")

            # --- STAGE 2: TOPIC ---
            if chapter not in taxonomy:
                selected_topic = "N/A"
                final_l2 = "N/A"
                f_data = {"difficulty": "Unknown", "confidence": 0}
                t_data = {"reasoning": "Chapter invalid or skipped."}
            else:
                topics_available = taxonomy.get(chapter, {})
                topic_list = sorted(list(topics_available.keys()))
                
                existing_topic = str(row['Topic']).strip()
                is_topic_known = existing_topic.lower() not in ["nan", "unknown", "", "none"] and existing_topic in topics_available

                if is_chapter_known and not is_topic_known:
                    stats["topic_was_missing"] += 1
                
                if is_topic_known:
                    print(f">>> [Q {q_num}] STAGE 2: [SKIPPED] - TOPIC KNOWN: '{existing_topic}'")
                    selected_topic = existing_topic
                    t_data = {"reasoning": "Pre-classified in source."}
                
                elif not topic_list:
                    print(f">>> [Q {q_num}] STAGE 2: [SKIPPED] - NO TOPICS DEFINED FOR CHAPTER '{chapter}'")
                    selected_topic = "General"
                    t_data = {"reasoning": "No topics in syllabus."}

                elif len(topic_list) == 1:
                    print(f">>> [Q {q_num}] STAGE 2: [SKIPPED] - SINGLE TOPIC: '{topic_list[0]}'")
                    selected_topic = topic_list[0]
                    t_data = {"reasoning": "Only one topic exists."}
                    
                else:
                    t_prompt = f"""
You are an expert Physics Faculty.

[CONTEXT]
Selected Chapter: {chapter}

[ALLOWED TOPICS]
{', '.join(topic_list)}

[STRICT RULES]
1. Select the BEST fitting topic from the list.
2. NO "Miscellaneous" or "Other".
3. Use keyword matching if the text is unclear.

[QUESTION TEXT]
{ocr_text}
"""
                    print(f"\n{'-'*20} FULL PROMPT (TOPIC) {'-'*20}")
                    print(t_prompt)
                    print(f"{'-'*60}")
                    
                    try:
                        t_resp = client.chat(
                            model=MODEL_NAME, 
                            messages=[{'role': 'user', 'content': t_prompt}], 
                            format={'type':'object', 'properties':{'topic':{'enum': topic_list}, 'reasoning':{'type':'string'}}, 'required':['topic', 'reasoning']}
                        )
                        t_data = json.loads(t_resp['message']['content'])
                        selected_topic = t_data.get('topic')
                    except Exception as e:
                        print(f"API Error: {e}")
                        selected_topic = "MANUAL_REVIEW_REQUIRED"
                        t_data = {"reasoning": "API Error"}

                    print(f"   [AI SELECTION]: {selected_topic}")

                # --- STAGE 3: L2 (SUB-TOPIC) ---
                l2_list = topics_available.get(selected_topic, [])
                
                if not l2_list or selected_topic == "MANUAL_REVIEW_REQUIRED":
                    final_l2 = "N/A"
                    d_prompt = f"Assess difficulty of this physics question: {ocr_text}"
                    try:
                        d_resp = client.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': d_prompt}],
                                             format={'type':'object', 'properties':{'difficulty':{'enum':['Easy','Medium','Difficult']}, 'confidence':{'type':'integer'}}, 'required':['difficulty']})
                        f_data = json.loads(d_resp['message']['content'])
                    except:
                        f_data = {"difficulty": "Medium", "confidence": 0}

                else:
                    l2_options = sorted(l2_list)
                    
                    f_prompt = f"""
[CONTEXT]
Category: {chapter} > {selected_topic}

[ALLOWED SUB-TOPICS]
{', '.join(l2_options)}

[RULES]
1. Pick exactly one Sub-Topic.
2. NO "Miscellaneous". Force a best fit.
3. Also assess difficulty (Easy/Medium/Difficult).

[QUESTION TEXT]
{ocr_text}
"""
                    print(f"\n{'-'*20} FULL PROMPT (L2) {'-'*20}")
                    print(f_prompt)
                    print(f"{'-'*60}")

                    try:
                        f_resp = client.chat(
                            model=MODEL_NAME, 
                            messages=[{'role': 'user', 'content': f_prompt}], 
                            format={'type':'object', 'properties':{'topic_l2':{'enum': l2_options}, 'difficulty':{'enum':['Easy','Medium','Difficult']}, 'confidence':{'type':'integer','minimum':1}}, 'required':['topic_l2', 'difficulty', 'confidence']}
                        )
                        f_data = json.loads(f_resp['message']['content'])
                        final_l2 = f_data.get('topic_l2')
                    except Exception as e:
                        print(f"API Error: {e}")
                        final_l2 = "MANUAL_REVIEW_REQUIRED"
                        f_data = {"difficulty": "Unknown", "confidence": 0}

                    print(f"   [AI SELECTION]: {final_l2}")

            # --- SAVE ---
            df_main.at[idx, 'Chapter'] = chapter
            df_main.at[idx, 'Topic'] = selected_topic
            df_main.at[idx, 'Topic_L2'] = final_l2
            df_main.at[idx, 'Difficulty_tag'] = f_data.get('difficulty')
            
            df_main.at[idx, 'AI_Reasoning'] = t_data.get('reasoning')
            df_main.at[idx, 'AI_Confidence'] = f_data.get('confidence')
            
            df_main.to_csv(target_csv, index=False)
            
            stats["total_processed"] += 1
            print(f"   [DONE] Time: {(time.perf_counter() - q_start):.2f}s | Confidence: {f_data.get('confidence')}%")
            print("=" * 60)

    except KeyboardInterrupt:
        print("\n\nüõë STOPPING SCRIPT...")

    finally:
        print(f"\nTotal Processed: {stats['total_processed']}")
        print(f"File saved: {target_csv}")

if __name__ == "__main__":
    run_universal_auditor()

================================================================================
FILE: AIsuggestedTagsWorkload.py
================================================================================
import pandas as pd
import os

def calculate_processing_load():
    # --- CONFIGURATION ---
    BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
    # Change this to "DB Master_OCR.csv" if you want to check the WHOLE database
    INPUT_FILENAME = "DB Master.csv" 
    METADATA_FILENAME = "DB Metadata.xlsx"

    input_csv = os.path.join(BASE_PATH, INPUT_FILENAME)
    metadata_path = os.path.join(BASE_PATH, METADATA_FILENAME)

    # --- 1. LOAD TAXONOMY ---
    print("Loading Taxonomy...")
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        meta_df = meta_df.map(lambda x: str(x).strip() if pd.notna(x) else "nan")

        taxonomy = {}
        for ch, group in meta_df.groupby('Chapter'):
            taxonomy[ch] = {}
            for top, sub_group in group.groupby('Topic'):
                if top.lower() != "nan" and top.lower() != ch.lower():
                    l2s = [v for v in sub_group['Topic_L2'].unique() if v.lower() != "nan"]
                    cleaned_l2s = []
                    for item in l2s:
                        cleaned_l2s.extend([i.strip() for i in item.replace('[','').replace(']','').replace("'",'').split(',')])
                    taxonomy[ch][top] = sorted(list(set([x for x in cleaned_l2s if x])))
    except Exception as e:
        print(f"‚ùå Error loading Metadata: {e}")
        return

    # --- 2. LOAD DATA ---
    if not os.path.exists(input_csv):
        print(f"‚ùå Input CSV not found: {input_csv}")
        return

    df_main = pd.read_csv(input_csv)
    
    # Exclude already processed ones if you have a results file, otherwise assumes fresh run
    print(f"Analyzing {len(df_main)} questions with new skipping logic...\n")

    # --- 3. COUNTERS ---
    stats = {
        "chapter_ai_calls": 0,
        "topic_ai_calls": 0,
        "l2_ai_calls": 0,
        "total_ai_calls": 0,
        "skipped_stage_2_single_topic": 0,
        "skipped_stage_3_no_l2": 0
    }

    # --- 4. SIMULATION LOOP ---
    for index, row in df_main.iterrows():
        # Variables
        chapter = None
        selected_topic = None
        
        # --- STAGE 1 CHECK (CHAPTER) ---
        existing_chapter = str(row['Chapter']).strip()
        is_chapter_known = existing_chapter.lower() not in ["nan", "unknown", "", "none"] and existing_chapter in taxonomy
        
        if is_chapter_known:
            chapter = existing_chapter
        else:
            stats["chapter_ai_calls"] += 1
            # We can't simulate Stage 2/3 accurately if we don't know the chapter, 
            # so we assume worst case: AI finds a chapter with multiple topics.
            stats["topic_ai_calls"] += 1
            stats["l2_ai_calls"] += 1
            continue # Move to next question as we can't predict the specific taxonomy path

        # --- STAGE 2 CHECK (TOPIC) ---
        topics_available = taxonomy.get(chapter, {})
        existing_topic = str(row['Topic']).strip()
        is_topic_known = existing_topic.lower() not in ["nan", "unknown", "", "none"] and existing_topic in topics_available
        
        if is_topic_known:
            selected_topic = existing_topic
        elif len(topics_available) == 1:
            # SKIP: Only one topic exists
            stats["skipped_stage_2_single_topic"] += 1
            selected_topic = list(topics_available.keys())[0]
        else:
            stats["topic_ai_calls"] += 1
            # Again, assume worst case that AI picks a topic that needs L2
            stats["l2_ai_calls"] += 1
            continue

        # --- STAGE 3 CHECK (L2) ---
        l2_list = topics_available.get(selected_topic, [])
        
        if not l2_list:
            # SKIP: No L2 tags exist for this topic
            stats["skipped_stage_3_no_l2"] += 1
        else:
            stats["l2_ai_calls"] += 1

    # --- 5. REPORT ---
    stats["total_ai_calls"] = stats["chapter_ai_calls"] + stats["topic_ai_calls"] + stats["l2_ai_calls"]
    # Estimate: Llama 3.2 is fast (~1.5s per call on GPU, maybe 3s on CPU)
    est_time_min = (stats["total_ai_calls"] * 1.5) / 60
    est_time_max = (stats["total_ai_calls"] * 4.0) / 60

    print("="*60)
    print("             PRE-RUN SIMULATION REPORT")
    print("="*60)
    print(f"Total Questions:         {len(df_main)}")
    print("-" * 60)
    print(f"AI Calls - Stage 1 (Chapter):   {stats['chapter_ai_calls']}")
    print(f"AI Calls - Stage 2 (Topic):     {stats['topic_ai_calls']}")
    print(f"AI Calls - Stage 3 (L2 Tags):   {stats['l2_ai_calls']}")
    print("-" * 60)
    print(f"TOTAL AI CALLS REQUIRED:        {stats['total_ai_calls']}")
    print(f"ESTIMATED TIME:                 {est_time_min:.1f} to {est_time_max:.1f} minutes")
    print("="*60)
    print("Savings from Logic Upgrades:")
    print(f"‚Ä¢ Skipped Topic (Single Choice): {stats['skipped_stage_2_single_topic']}")
    print(f"‚Ä¢ Skipped L2 (None Defined):     {stats['skipped_stage_3_no_l2']}")
    print("="*60)

if __name__ == "__main__":
    calculate_processing_load()

================================================================================
FILE: autoQC.py
================================================================================
import pandas as pd
import numpy as np
import shutil
from pathlib import Path
from datetime import datetime
from PIL import Image

# --- CONFIGURATION ---
# Use raw strings (r'...') or forward slashes for paths
BASE_PATH = Path(r'D:/Main/3. Work - Teaching/Projects/Question extractor/')
DB_PATH = BASE_PATH / 'DB Master.csv'
IMG_DIR = BASE_PATH / 'Processed_Database'
BACKUP_DIR = BASE_PATH / 'Backups'

# --- GEOMETRY RULES ---
LIMITS = {
    'MIN_HEIGHT_NUMERICAL': 50,  # Single line questions
    'MIN_HEIGHT_MCQ': 100,       # Needs space for options
    'MIN_HEIGHT_DEFAULT': 75,    # Fallback
    'MAX_HEIGHT': 3000,          # Uncropped full page
    'MIN_ASPECT': 0.25,          # Too thin (Vertical sliver)
    'MAX_ASPECT': 20.0           # Too wide (Horizontal bar)
}

def create_backup(file_path):
    """Creates a timestamped backup of the database."""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = BACKUP_DIR / f"DB_Master_AutoQC_{timestamp}.csv"
    
    try:
        shutil.copy2(file_path, backup_path)
        print(f"‚úÖ Backup created: {backup_path.name}")
    except Exception as e:
        print(f"‚ö†Ô∏è Backup failed: {e}")

def get_image_metadata(folder, q_num):
    """
    Attempts to find the image and return dimensions.
    Handles '1' vs '1.0' discrepancy automatically.
    Returns: (width, height, status_message)
    """
    if pd.isna(folder) or pd.isna(q_num):
        return None, None, "Invalid Metadata"

    folder_path = IMG_DIR / str(folder).strip()
    
    # Normalize q_num: "1.0" -> "1"
    try:
        q_clean = str(int(float(q_num)))
    except ValueError:
        q_clean = str(q_num)

    file_name = f"Q_{q_clean}.png"
    img_path = folder_path / file_name

    if not img_path.exists():
        return None, None, "Missing File"
    
    if img_path.stat().st_size == 0:
        return None, None, "Corrupt (0 KB)"

    try:
        with Image.open(img_path) as img:
            return img.width, img.height, "Found"
    except Exception:
        return None, None, "Read Error"

def evaluate_row(row):
    """
    Applies geometry rules to a single row.
    Returns: (New_Status, Fail_Reason)
    """
    # 1. Check File Integrity
    if row['img_status'] != "Found":
        return "Auto-Fail", row['img_status']

    w, h = row['q_width'], row['q_height']
    q_type = str(row.get('Question type', '')).lower()
    reasons = []

    # 2. Determine Height Limit based on Context
    if "numerical" in q_type:
        limit = LIMITS['MIN_HEIGHT_NUMERICAL']
        lbl = "Numerical"
    elif "single" in q_type or "multiple" in q_type:
        limit = LIMITS['MIN_HEIGHT_MCQ']
        lbl = "MCQ"
    else:
        limit = LIMITS['MIN_HEIGHT_DEFAULT']
        lbl = "General"

    # 3. Check Height Rules
    if h < limit:
        reasons.append(f"Too Short for {lbl} ({h}px < {limit}px)")
    if h > LIMITS['MAX_HEIGHT']:
        reasons.append(f"Too Tall ({h}px)")

    # 4. Check Aspect Ratio Rules
    aspect = w / h
    if aspect < LIMITS['MIN_ASPECT']:
        reasons.append(f"Too Thin (Ratio {aspect:.2f})")
    elif aspect > LIMITS['MAX_ASPECT']:
        reasons.append(f"Too Wide (Ratio {aspect:.2f})")

    # 5. Final Verdict
    if reasons:
        return "Auto-Fail", "; ".join(reasons)
    else:
        return "Auto-Pass", ""

def run_geometry_check():
    print("="*60)
    print("      üìê GEOMETRIC AUTO-QC AUDITOR (v2.0)")
    print("="*60)

    if not DB_PATH.exists():
        print(f"‚ùå Database not found at: {DB_PATH}")
        return

    # --- LOAD ---
    print("üìÇ Loading Database...")
    try:
        if DB_PATH.suffix == '.csv':
            df = pd.read_csv(DB_PATH)
        else:
            df = pd.read_excel(DB_PATH)
    except Exception as e:
        print(f"‚ùå Error reading DB: {e}")
        return

    # --- PREPARE ---
    # Ensure columns exist
    cols_to_ensure = ['QC_Status', 'QC_Fail_Reason', 'q_width', 'q_height', 'QC_Locked']
    for col in cols_to_ensure:
        if col not in df.columns:
            df[col] = None

    # Clean QC_Locked (Force to 0 or 1)
    df['QC_Locked'] = pd.to_numeric(df['QC_Locked'], errors='coerce').fillna(0).astype(int)

    # Filter Active Rows (Not Locked)
    # We use the index to update the main DF later
    active_mask = (df['QC_Locked'] == 0)
    active_idx = df[active_mask].index

    print(f"   -> Analyzing {len(active_idx)} active questions...")

    if len(active_idx) == 0:
        print("‚úÖ No active questions to process.")
        return

    # --- EXECUTE: IO PHASE (Get Sizes) ---
    print("üîç Phase 1: verifying images dimensions...")
    
    # We create a temporary DataFrame for calculation to avoid fragmentation
    temp_df = df.loc[active_idx].copy()
    
    # Apply is cleaner than iterrows. 
    # It returns a Series of tuples which we break into columns.
    meta_results = temp_df.apply(
        lambda x: get_image_metadata(x['Folder'], x['Question No.']), axis=1
    )
    
    # Unpack results into the temp dataframe
    temp_df[['q_width', 'q_height', 'img_status']] = pd.DataFrame(meta_results.tolist(), index=temp_df.index)

    # --- EXECUTE: LOGIC PHASE (Check Rules) ---
    print("üß† Phase 2: Applying geometry rules...")
    
    qc_results = temp_df.apply(evaluate_row, axis=1)
    
    # Unpack results
    temp_df[['QC_Status', 'QC_Fail_Reason']] = pd.DataFrame(qc_results.tolist(), index=temp_df.index)

    # --- UPDATE MAIN DATAFRAME ---
    # Update only the specific columns for the active rows
    cols_to_update = ['q_width', 'q_height', 'QC_Status', 'QC_Fail_Reason']
    df.loc[active_idx, cols_to_update] = temp_df[cols_to_update]

    # --- REPORT ---
    print("\nüìä AUDIT RESULTS:")
    print(df.loc[active_idx, 'QC_Status'].value_counts())

    fails = df[(df['QC_Status'] == 'Auto-Fail') & (df.index.isin(active_idx))]
    if not fails.empty:
        print("\n‚ùå Top Fail Reasons:")
        print(fails['QC_Fail_Reason'].value_counts().head(5))

    # --- SAVE ---
    print("\nüíæ Saving updates...")
    create_backup(DB_PATH)
    
    try:
        df.to_csv(DB_PATH, index=False)
        print("‚úÖ Database Updated Successfully.")
    except PermissionError:
        print("‚ùå ERROR: Could not save. Is the CSV file open in Excel?")

if __name__ == "__main__":
    run_geometry_check()

================================================================================
FILE: batchExtractMainsPYQs.py
================================================================================
import os
import pandas as pd
import pdfplumber
from pdf2image import convert_from_path
from PIL import Image, ImageChops
import re
import logging
import glob
from tqdm import tqdm
import json

# --- SUPPRESS WARNINGS ---
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# --- CONFIGURATION (DYNAMIC) ---
# We expect config.json to be in the same folder as this script
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(CURRENT_DIR, "config.json")

def load_config():
    if not os.path.exists(CONFIG_PATH):
        print(f"‚ùå Error: config.json not found at {CONFIG_PATH}")
        # Fallback to hardcoded defaults if config fails, just to be safe
        return {
            "BASE_PATH": r'D:\Main\3. Work - Teaching\Projects\Question extractor',
            "SOURCE_DIR": r'D:\Main\3. Work - Teaching\Books\0. Favs\JEE Mains PYQs',
            "DB_FILENAME": "DB Master.xlsx",
            "last_unique_id": 0
        }
    with open(CONFIG_PATH, 'r') as f:
        return json.load(f)

def get_new_unique_id():
    """
    Reads the last ID from config.json, increments it, saves it back,
    and returns the NEW ID.
    """
    if not os.path.exists(CONFIG_PATH):
        return 0 # Should not happen if load_config works

    try:
        with open(CONFIG_PATH, 'r+') as f:
            data = json.load(f)
            current_id = data.get("last_unique_id", 0)
            new_id = int(current_id) + 1
            
            # Update and save immediately
            data["last_unique_id"] = new_id
            f.seek(0)
            json.dump(data, f, indent=4)
            f.truncate()
            return new_id
    except Exception as e:
        print(f"‚ùå Error updating unique ID: {e}")
        return 0

# Load paths immediately
config = load_config()
BASE_PATH = config.get("BASE_PATH")
SOURCE_DIR = config.get("SOURCE_DIR")
DB_FILENAME = config.get("DB_FILENAME", "DB Master.xlsx")

OUTPUT_BASE = os.path.join(BASE_PATH, 'Processed_Database')
MASTER_DB_PATH = os.path.join(BASE_PATH, DB_FILENAME)


# --- HELPER FUNCTIONS ---

def trim_whitespace(im):
    try:
        bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
        diff = ImageChops.difference(im, bg)
        diff = ImageChops.add(diff, diff, 2.0, -100)
        bbox = diff.getbbox()
        if bbox: return im.crop(bbox)
        return im
    except Exception: return im

def parse_answer_key(pdf_path, total_expected=None):
    """
    Scans the last 5 pages using the proven regex from the debug script.
    Pattern: 1. (2)
    """
    ans_map = {}
    with pdfplumber.open(pdf_path) as pdf:
        total_pages = len(pdf.pages)
        # Scan last 5 pages
        start_scan = max(0, total_pages - 5)
        
        for page_idx in range(start_scan, total_pages):
            page = pdf.pages[page_idx]
            text = page.extract_text()
            if not text: continue
            
            # --- THE PROVEN REGEX ---
            # 1. (\d+)   : Question Number
            # 2. \.      : Literal Dot
            # 3. \s* : Optional Space
            # 4. \(      : Literal Open Parenthesis
            # 5. ([^)]+) : Answer Content (Capture everything until closing paren)
            # 6. \)      : Literal Closing Parenthesis
            pattern = r'(\d+)\.\s*\(([^)]+)\)'
            
            matches = re.findall(pattern, text)
            
            for q_num, ans_val in matches:
                try:
                    q = int(q_num)
                    # Filter: Ignore if Q number is suspiciously large (likely a year "2024")
                    if q < 2000: 
                        ans_map[q] = ans_val
                except: pass
                
    return ans_map

def find_questions_and_tags(pdf_path):
    anchors = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_idx, page in enumerate(pdf.pages):
            height = page.height
            MARGIN_TOP, MARGIN_BOTTOM = height * 0.05, height * 0.92
            words = page.extract_words(x_tolerance=2, y_tolerance=3, keep_blank_chars=False)
            words.sort(key=lambda w: (round(w['top'], 1), w['x0']))
            
            for i, word in enumerate(words):
                if word['top'] < MARGIN_TOP or word['top'] > MARGIN_BOTTOM: continue
                text = word['text'].strip()
                
                # Regex for Question Start (e.g. Q1., Q7*, Q15)
                # Matches "Q" -> digits -> optional symbol (*,^) -> optional separator (.:)
                match = re.match(r'^Q\s*(\d+)[^a-zA-Z0-9]?[\.:]?', text, re.IGNORECASE)
                
                if match:
                    q_num = int(match.group(1))
                    exam_year = "Mixed"
                    
                    # Look ahead for Year Tags
                    search_range = words[i:i+50] 
                    for w in search_range:
                        if w != word and re.match(r'^Q\s*\d+', w['text']): break
                        if re.match(r'20[0-2]\d', w['text']):
                            exam_year = re.sub(r'[^\d]', '', w['text'])
                            break
                    
                    anchors.append({'q_num': q_num, 'page_idx': page_idx, 'top': word['top'], 'year': exam_year})
    
    unique_anchors = {a['q_num']: a for a in anchors}
    return sorted(unique_anchors.values(), key=lambda x: x['q_num'])

def crop_mains_questions(pdf_path, anchors, ans_map, output_folder, file_prefix):
    if not anchors: return []
    try:
        pdf_images = convert_from_path(pdf_path, dpi=300, use_pdftocairo=True, strict=False)
    except: return []
    
    scale = 300 / 72 
    page_height = pdf_images[0].height
    TOP_MARGIN_PX, BOTTOM_MARGIN_PX = (page_height * 0.05), (page_height * 0.92)
    VERTICAL_PADDING = 10 
    processed_data = []

    pbar = tqdm(total=len(anchors), desc="   üì∏ Cropping", leave=False)

    for i, start in enumerate(anchors):
        q_num = start['q_num']
        next_q_page = anchors[i+1]['page_idx'] if i + 1 < len(anchors) else start['page_idx']
        end_loop_page = min(next_q_page + 1, start['page_idx'] + 3)
        images_to_stitch = []
        
        for p_idx in range(start['page_idx'], end_loop_page):
            if p_idx >= len(pdf_images): break
            img = pdf_images[p_idx]
            w, h = img.size
            if p_idx == start['page_idx']:
                curr_top = max(TOP_MARGIN_PX, (start['top'] * scale) - VERTICAL_PADDING)
                if p_idx == next_q_page and i + 1 < len(anchors):
                    curr_bottom = min((anchors[i+1]['top'] * scale) - VERTICAL_PADDING, BOTTOM_MARGIN_PX)
                else: curr_bottom = BOTTOM_MARGIN_PX
                if curr_bottom > curr_top + 20: images_to_stitch.append(img.crop((0, curr_top, w, curr_bottom)))
            else:
                curr_top = TOP_MARGIN_PX
                if p_idx == next_q_page and i + 1 < len(anchors):
                    curr_bottom = min((anchors[i+1]['top'] * scale) - VERTICAL_PADDING, BOTTOM_MARGIN_PX)
                else: curr_bottom = BOTTOM_MARGIN_PX
                images_to_stitch.append(img.crop((0, curr_top, w, curr_bottom)))
                if p_idx == next_q_page: break

        if images_to_stitch:
            total_h = sum(im.height for im in images_to_stitch)
            max_w = max(im.width for im in images_to_stitch)
            final_img = Image.new('RGB', (max_w, total_h), (255, 255, 255))
            y = 0
            for im in images_to_stitch:
                final_img.paste(im, (0, y)); y += im.height
            final_img = trim_whitespace(final_img)
            
            # Save logic
            if final_img.height > 40:
                final_img.save(os.path.join(output_folder, f"{file_prefix}_{q_num}.png"))
                processed_data.append({
                    'q_num': q_num, 
                    'answer': ans_map.get(q_num, ""), 
                    'year': start.get('year', 'Mixed')
                })
        
        pbar.update(1)
    
    pbar.close()
    return processed_data

def run_batch_extraction():
    print("="*60)
    print("      üß™ JEE MAINS BATCH EXTRACTOR v9.2 (Fixed & Configured)")
    print("="*60)
    print(f"üìÇ Base Path: {BASE_PATH}")
    print(f"üìÇ DB Path:   {MASTER_DB_PATH}")

    if not os.path.exists(SOURCE_DIR):
        print(f"‚ùå Source directory not found: {SOURCE_DIR}")
        return

    pdf_files = glob.glob(os.path.join(SOURCE_DIR, "*.pdf"))
    if not pdf_files:
        print("‚ÑπÔ∏è No PDFs found.")
        return

    processed_folders = set()
    if os.path.exists(MASTER_DB_PATH):
        try:
            df_existing = pd.read_excel(MASTER_DB_PATH)
            if 'Folder' in df_existing.columns:
                processed_folders = set(df_existing['Folder'].unique())
        except: pass

    # --- MAIN LOOP ---
    for pdf_path in tqdm(pdf_files, desc="üìÇ Total Files Progress", unit="file"):
        filename = os.path.basename(pdf_path)
        parts = [p.strip() for p in filename.replace('.pdf', '').split('-')]
        
        if len(parts) < 2: continue
        
        subject = parts[0]
        chapter = parts[-1]
        safe_chapter = "".join([c for c in chapter if c.isalnum() or c in (' ', '_')]).strip()
        folder_name = f"JEE_Mains_{safe_chapter}"
        
        if folder_name in processed_folders:
            continue

        print(f"\nüöÄ Processing: {filename}")
        output_dir = os.path.join(OUTPUT_BASE, folder_name)
        os.makedirs(output_dir, exist_ok=True)

        ans_map = parse_answer_key(pdf_path)
        anchors = find_questions_and_tags(pdf_path)
        
        # Simple stats report
        matched_answers = len([q for q in anchors if q['q_num'] in ans_map])
        print(f"   üîç Found {len(anchors)} questions. (Matched Answers: {matched_answers}/{len(anchors)})")

        extracted_data = crop_mains_questions(pdf_path, anchors, ans_map, output_dir, "Q")
        
        if extracted_data:
            new_rows = []
            for item in extracted_data:
                
                # --- UPDATE 1: Generate Unique ID & Increment Config ---
                unique_id = get_new_unique_id()
                
                new_rows.append({
                    'Q': item['q_num'],    # --- UPDATE 2: Changed from 'Question No.' to 'Q'
                    'Folder': folder_name,
                    'unique_id': unique_id, # --- UPDATE 3: Added Unique ID
                    'Subject': subject, 
                    'Chapter': chapter, 
                    'Exam': 'JEE Main',
                    'Question type': 'Single Correct', 
                    'Difficulty_tag': 'Medium',
                    'Correct Answer': item['answer'], 
                    'PYQ': 'Yes',
                    'PYQ_Year': item['year'] if len(item['year']) == 4 else "Mixed",
                    'QC_Status': 'Pass', 
                    'QC_Locked': 0, 
                    'manually updated': 0
                })
            
            # Atomic Save
            try:
                if os.path.exists(MASTER_DB_PATH):
                    df_master = pd.read_excel(MASTER_DB_PATH)
                    df_final = pd.concat([df_master, pd.DataFrame(new_rows)], ignore_index=True)
                else:
                    df_final = pd.DataFrame(new_rows)
                
                df_final.to_excel(MASTER_DB_PATH, index=False)
                # Show ID range in logs
                start_id = new_rows[0]['unique_id']
                end_id = new_rows[-1]['unique_id']
                print(f"   ‚úÖ Saved {len(new_rows)} entries to DB. (IDs {start_id} - {end_id})")
            except PermissionError:
                print("   ‚ùå Error: DB Master.xlsx is open. Could not save.")
                continue

if __name__ == "__main__":
    run_batch_extraction()

================================================================================
FILE: batchExtract_CollegeDoors.py
================================================================================
import os
import glob
import pandas as pd
import pdfplumber
import json
import re
import time
from pdf2image import convert_from_path
from PIL import Image, ImageChops
try:
    from tqdm import tqdm
except ImportError:
    from tqdm.notebook import tqdm

# --- 1. CONFIGURATION ---
CONFIG_PATH = 'config.json'
DEFAULT_BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor'

config = {}
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, 'r') as f:
        config = json.load(f)
    print(f"‚úÖ Loaded config file.")
else:
    print(f"‚ö†Ô∏è Config not found. Using defaults.")

BASE_PATH = config.get('BASE_PATH', DEFAULT_BASE_PATH)
RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw data') 
OUTPUT_BASE = os.path.join(BASE_PATH, 'Processed_Database')
MASTER_DB_PATH = os.path.join(BASE_PATH, 'DB Master.xlsx')

start_id = config.get('last_unique_id', config.get('latest_question_id', 0))

print(f"üîß CONFIGURATION CHECK:")
print(f"   - Source Folder : {RAW_DATA_PATH}")
print(f"   - ID Counter    : Starts at {start_id}")

os.makedirs(OUTPUT_BASE, exist_ok=True)


# --- 2. HELPER FUNCTIONS ---

def trim_whitespace(im):
    try:
        bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
        diff = ImageChops.difference(im, bg)
        diff = ImageChops.add(diff, diff, 2.0, -100)
        bbox = diff.getbbox()
        if bbox: return im.crop(bbox)
        return im
    except Exception: return im

def find_anchors_robust(pdf_path, max_val=None, is_solution=False):
    """
    UPGRADED ANCHOR FINDER V3:
    - Relaxed 'Strong' anchor tolerance to 20% width (Fixes skipped Q12).
    - Added logic to catch 'Solution' keyword as a backup anchor.
    """
    anchors = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_idx, page in enumerate(pdf.pages):
            width = page.width
            midpoint = width / 2
            
            words = page.extract_words(keep_blank_chars=False)
            i = 0
            while i < len(words):
                curr_word = words[i]
                text = curr_word['text'].strip()
                x0 = curr_word['x0']
                
                # Determine Column
                col = 0 if x0 < midpoint else 1
                relative_x = x0 if col == 0 else (x0 - midpoint)
                
                found_q_num = None
                is_strong_anchor = False 

                # STRATEGY 1: Combined Regex (Q.12, Sol.12)
                match = re.match(r'^(?:Q|Sol|Solution|S)?[\.\s]*(\d+)[\.\s:)]*$', text, re.IGNORECASE)
                if match:
                    found_q_num = int(match.group(1))
                    if text[0].isalpha(): is_strong_anchor = True
                
                # STRATEGY 2: Split Token ("Q" then "12")
                elif text.lower() in ["q", "q.", "sol", "sol.", "solution", "solution:"] and i + 1 < len(words):
                    next_word = words[i+1]
                    match_next = re.match(r'^(\d+)[\.\s:)]*$', next_word['text'])
                    if match_next:
                        found_q_num = int(match_next.group(1))
                        is_strong_anchor = True 
                        i += 1 

                # --- VALIDATION LOGIC V3 ---
                if found_q_num:
                    if max_val and found_q_num > max_val: 
                        found_q_num = None
                    else:
                        # RELAXED TOLERANCE for Strong Anchors (Fixes Q12 issue)
                        if is_strong_anchor:
                            if relative_x > (width * 0.20): # Increased from 0.15 to 0.20
                                found_q_num = None 
                        
                        # STRICT TOLERANCE for Weak Anchors (Fixes 1000/3 issue)
                        else:
                            if relative_x > (width * 0.05):
                                found_q_num = None

                    if found_q_num:
                        anchors.append({'q_num': found_q_num, 'page_idx': page_idx, 'top': curr_word['top'], 'col': col})
                i += 1

    unique_anchors = {}
    for a in anchors:
        if a['q_num'] not in unique_anchors: unique_anchors[a['q_num']] = a
    return sorted(unique_anchors.values(), key=lambda x: x['q_num'])

def extract_text_content(pdf_path, anchors, is_two_column=True):
    extracted_text = {}
    with pdfplumber.open(pdf_path) as pdf:
        pages = pdf.pages
        if not pages: return {}
        
        width = pages[0].width
        height = pages[0].height
        midpoint = width / 2
        BOTTOM_LIMIT = height * 0.92

        for i, start in enumerate(anchors):
            q_num = start['q_num']
            text_segments = []
            
            if i + 1 < len(anchors):
                end = anchors[i+1]
            else:
                end = {'page_idx': start['page_idx'], 'top': BOTTOM_LIMIT, 'col': start['col']}

            curr_pidx = start['page_idx']
            curr_col = start['col']
            curr_top = start['top']
            
            while True:
                page = pages[curr_pidx]
                if is_two_column:
                    x0 = 0 if curr_col == 0 else midpoint
                    x1 = midpoint if curr_col == 0 else width
                else:
                    x0 = 0; x1 = width
                
                if curr_pidx == end['page_idx'] and curr_col == end['col']:
                    bottom = end['top']
                    done = True
                else:
                    bottom = BOTTOM_LIMIT
                    done = False
                
                if bottom > curr_top:
                    try:
                        cropped_page = page.crop((x0, curr_top, x1, bottom))
                        text = cropped_page.extract_text()
                        if text: text_segments.append(text)
                    except Exception: pass
                
                if done: break
                
                if is_two_column:
                    if curr_col == 0:
                        curr_col = 1; curr_top = 50 
                    else:
                        curr_col = 0; curr_pidx += 1; curr_top = 50
                else:
                    curr_pidx += 1; curr_top = 50
                
                if curr_pidx >= len(pages): break

            full_text = "\n".join(text_segments).strip()
            extracted_text[q_num] = full_text
            
    return extracted_text

def crop_and_save_standard(pdf_path, anchors, output_folder, suffix_type, is_two_column=True):
    try: pdf_images = convert_from_path(pdf_path, dpi=300)
    except: return

    if not pdf_images: return
    page_width, page_height = pdf_images[0].size
    scale = 300 / 72 
    midpoint_px = (page_width / 2) 
    FOOTER_CUTOFF_PX = page_height * 0.92
    VERTICAL_PADDING = 15 
    TOP_MARGIN = 50 * scale 
    
    # Progress Bar for Cropping (Fixes Request #3)
    pbar = tqdm(total=len(anchors), desc=f"   üì∑ Cropping {suffix_type}", leave=True)
    print 
    for i, start in enumerate(anchors):
        q_num = start['q_num']
        
        if i + 1 < len(anchors): 
            end = anchors[i+1]
        else: 
            end = {'page_idx': start['page_idx'], 'top': FOOTER_CUTOFF_PX / scale, 'col': start['col']} 

        start_top_px = max(0, (start['top'] * scale) - VERTICAL_PADDING)
        end_top_px = (end['top'] * scale) - VERTICAL_PADDING

        try:
            images_to_stitch = []
            if is_two_column:
                start_left = 0 if start['col'] == 0 else midpoint_px
                start_right = midpoint_px if start['col'] == 0 else page_width
                end_left = 0 if end['col'] == 0 else midpoint_px
                end_right = midpoint_px if end['col'] == 0 else page_width
            else:
                start_left = 0; start_right = page_width
                end_left = 0; end_right = page_width

            if start['page_idx'] == end['page_idx']:
                if start['col'] == end['col']:
                    bottom = min(end_top_px, FOOTER_CUTOFF_PX)
                    if bottom <= start_top_px: bottom = start_top_px + 100 
                    images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, bottom)))
                else:
                    images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, FOOTER_CUTOFF_PX)))
                    images_to_stitch.append(pdf_images[start['page_idx']].crop((end_left, TOP_MARGIN, end_right, end_top_px)))
            else:
                images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, FOOTER_CUTOFF_PX)))
                for mid_idx in range(start['page_idx'] + 1, end['page_idx']):
                    images_to_stitch.append(pdf_images[mid_idx].crop((start_left, TOP_MARGIN, start_right, FOOTER_CUTOFF_PX)))
                if end_top_px > TOP_MARGIN:
                    images_to_stitch.append(pdf_images[end['page_idx']].crop((end_left, TOP_MARGIN, end_right, end_top_px)))

            if images_to_stitch:
                total_height = sum(img.height for img in images_to_stitch)
                max_width = max(img.width for img in images_to_stitch)
                final_img = Image.new('RGB', (max_width, total_height), (255, 255, 255))
                y_offset = 0
                for img in images_to_stitch:
                    final_img.paste(img, (0, y_offset))
                    y_offset += img.height
                
                final_img = trim_whitespace(final_img)
                filename = f"{suffix_type}_{q_num}.png"
                final_img.save(os.path.join(output_folder, filename))
                
        except Exception: pass
        pbar.update(1) # Update progress bar
    pbar.close()

# --- 3. TARGETING ---
print(f"\n--- üîç SCANNING ---")
processed_folders = set()
final_master_df = pd.DataFrame()

if os.path.exists(MASTER_DB_PATH):
    try:
        final_master_df = pd.read_excel(MASTER_DB_PATH)
        if 'Folder' in final_master_df.columns:
            processed_folders = set(final_master_df['Folder'].dropna().astype(str).unique())
        print(f"‚úÖ DB Loaded. Found {len(processed_folders)} processed folders.")
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading DB: {e}. Starting fresh.")

if not os.path.exists(RAW_DATA_PATH):
    print(f"‚ùå CRITICAL: Folder not found: {RAW_DATA_PATH}")
    exit()

all_subfolders = [f.path for f in os.scandir(RAW_DATA_PATH) if f.is_dir()]
target_folders = []

print(f"üìÇ Looking in 'raw data'...")

for folder_path in all_subfolders:
    folder_name = os.path.basename(folder_path)
    clean_name = folder_name.strip()
    
    if not clean_name.startswith("CollegeDoors"): continue
    if clean_name in processed_folders: continue

    print(f"   ‚úÖ FOUND NEW: {clean_name}")
    target_folders.append(folder_path)

if not target_folders:
    print("\nüéâ All 'CollegeDoors' folders are already processed! Exiting.")
    exit()

# --- 4. PROCESSING LOOP ---
start_time = time.time()
current_global_id = start_id
new_data_list = []

for folder in tqdm(target_folders, desc="Processing Batches"):
    test_name = os.path.basename(folder).strip()
    print(f"\nüîπ Processing: {test_name}")
    
    q_papers = glob.glob(os.path.join(folder, "*question_paper.pdf"))
    sol_pdfs = glob.glob(os.path.join(folder, "*solution_pdf.pdf"))
    excel_keys = glob.glob(os.path.join(folder, "*excel_answer_key.xlsx")) + glob.glob(os.path.join(folder, "*excel_answer_key.csv"))
    raw_meta_path = os.path.join(folder, 'metadata.csv')

    if not (q_papers and sol_pdfs and excel_keys):
        print(f"   ‚ö†Ô∏è Missing required files. Skipping.")
        continue

    try:
        key_path = excel_keys[0]
        key_df = pd.read_csv(key_path) if key_path.endswith('.csv') else pd.read_excel(key_path)
        total_questions = key_df['Question No.'].max() if 'Question No.' in key_df.columns else len(key_df)
        
        if os.path.exists(raw_meta_path):
            meta_df = pd.read_csv(raw_meta_path)
        else:
            meta_df = pd.DataFrame({'Question No.': key_df['Question No.']})
            meta_df['Topic'] = "Unknown"
            meta_df['Sub-Topic'] = "Unknown"
            meta_df['Subject'] = "Unknown"
    except Exception as e:
        print(f"   ‚ùå Data Load Error: {e}")
        continue
    
    test_output_dir = os.path.join(OUTPUT_BASE, test_name)
    os.makedirs(test_output_dir, exist_ok=True)
    
    # 1. Calculate Anchors
    q_anchors = find_anchors_robust(q_papers[0], max_val=total_questions)
    sol_anchors = find_anchors_robust(sol_pdfs[0], max_val=total_questions, is_solution=True)

    # 2. Extract Images (Now with Progress Bars!)
    crop_and_save_standard(q_papers[0], q_anchors, test_output_dir, "Q", is_two_column=True)
    crop_and_save_standard(sol_pdfs[0], sol_anchors, test_output_dir, "Sol", is_two_column=True)

    # 3. Extract Text
    print("   üìù Extracting & Cleaning Text...")
    extracted_text_map = extract_text_content(q_papers[0], q_anchors, is_two_column=True)

    # DataFrame Logic
    meta_df['Folder'] = test_name
    
    if 'Question No.' in meta_df.columns and 'Question No.' in key_df.columns:
        combined_df = pd.merge(meta_df, key_df, on='Question No.', how='left', suffixes=('', '_key'))
    else:
        combined_df = pd.concat([meta_df.reset_index(drop=True), key_df.reset_index(drop=True)], axis=1)

    unique_ids_col = []
    pdf_text_col = []
    text_avail_col = []
    
    for idx, row in combined_df.iterrows():
        q_num = row.get('Question No.')
        if pd.isna(q_num): 
            unique_ids_col.append(None)
            pdf_text_col.append(None)
            text_avail_col.append("No")
            continue
            
        q_num = int(q_num)
        
        current_global_id += 1
        unique_ids_col.append(current_global_id)
        
        raw_text = extracted_text_map.get(q_num, "")
        cleaned_text = re.sub(r'\b\S*_\S*\b', '', raw_text) # Remove ATPH_...
        cleaned_text = " ".join(cleaned_text.split())
        
        if len(cleaned_text) < 30:
            pdf_text_col.append("")
            text_avail_col.append("No")
        else:
            pdf_text_col.append(cleaned_text)
            text_avail_col.append("Yes")

    combined_df['unique_id'] = unique_ids_col
    combined_df['pdf_Text'] = pdf_text_col
    combined_df['PDF_Text_Available'] = text_avail_col
    combined_df['QC_Status'] = "Pass"
    
    combined_df.fillna("Unknown", inplace=True)
    combined_df = combined_df[combined_df['unique_id'].notna()]

    new_data_list.append(combined_df)
    print(f"   ‚úÖ Processed. Current ID reached: {current_global_id}")


# --- 5. FINAL SAVE ---
print("\n--- Finalizing ---")
if new_data_list:
    new_entries_df = pd.concat(new_data_list, ignore_index=True)
    try:
        updated_master = pd.concat([final_master_df, new_entries_df], ignore_index=True)
        updated_master.to_excel(MASTER_DB_PATH, index=False)
        print(f"‚úÖ DB Updated. Added {len(new_entries_df)} questions.")
        
        config['SOURCE_DIR'] = RAW_DATA_PATH
        config['last_unique_id'] = current_global_id
        
        with open(CONFIG_PATH, 'w') as f:
            json.dump(config, f, indent=4)
        print(f"üíæ Config saved.")
        
    except Exception as e:
        print(f"‚ùå Save Error: {e}")
else:
    print("‚ö†Ô∏è No valid data processed.")

================================================================================
FILE: batchExtract_CollegeDoorsBulk.py
================================================================================
import os
import glob
import pandas as pd
import pdfplumber
import json
import re
import time
from pdf2image import convert_from_path
from PIL import Image, ImageChops
try:
    from tqdm import tqdm
except ImportError:
    from tqdm.notebook import tqdm

# --- 1. CONFIGURATION ---
CONFIG_PATH = 'config.json'
DEFAULT_BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor'

config = {}
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, 'r') as f:
        config = json.load(f)
    print(f"‚úÖ Loaded config file.")
else:
    print(f"‚ö†Ô∏è Config not found. Using defaults.")

BASE_PATH = config.get('BASE_PATH', DEFAULT_BASE_PATH)
RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw data') 
OUTPUT_BASE = os.path.join(BASE_PATH, 'Processed_Database')

# PATHS FOR DB
MASTER_XLSX_PATH = os.path.join(BASE_PATH, 'DB Master.xlsx') # Read-Only Source
MASTER_CSV_PATH = os.path.join(BASE_PATH, 'DB Master.csv')   # Active Write Target

METADATA_FILE_PATH = os.path.join(BASE_PATH, 'DB Metadata.xlsx')

start_id = config.get('last_unique_id', config.get('latest_question_id', 0))

print(f"üîß CONFIGURATION CHECK:")
print(f"   - Source Folder : {RAW_DATA_PATH}")
print(f"   - Metadata File : {METADATA_FILE_PATH}")
print(f"   - Master CSV    : {MASTER_CSV_PATH}")
print(f"   - ID Counter    : Starts at {start_id}")

os.makedirs(OUTPUT_BASE, exist_ok=True)


# --- 2. HELPER FUNCTIONS ---

def trim_whitespace(im):
    try:
        bg = Image.new(im.mode, im.size, im.getpixel((0,0)))
        diff = ImageChops.difference(im, bg)
        diff = ImageChops.add(diff, diff, 2.0, -100)
        bbox = diff.getbbox()
        if bbox: return im.crop(bbox)
        return im
    except Exception: return im

def find_anchors_robust(pdf_path, max_val=None, is_solution=False):
    anchors = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_idx, page in enumerate(pdf.pages):
            width = page.width
            midpoint = width / 2
            
            words = page.extract_words(keep_blank_chars=False)
            i = 0
            while i < len(words):
                curr_word = words[i]
                text = curr_word['text'].strip()
                x0 = curr_word['x0']
                
                col = 0 if x0 < midpoint else 1
                relative_x = x0 if col == 0 else (x0 - midpoint)
                
                found_q_num = None
                is_strong_anchor = False 

                match = re.match(r'^(?:Q|Sol|Solution|S)?[\.\s]*(\d+)[\.\s:)]*$', text, re.IGNORECASE)
                if match:
                    found_q_num = int(match.group(1))
                    if text[0].isalpha(): is_strong_anchor = True
                
                elif text.lower() in ["q", "q.", "sol", "sol.", "solution", "solution:"] and i + 1 < len(words):
                    next_word = words[i+1]
                    match_next = re.match(r'^(\d+)[\.\s:)]*$', next_word['text'])
                    if match_next:
                        found_q_num = int(match_next.group(1))
                        is_strong_anchor = True 
                        i += 1 

                if found_q_num:
                    if max_val and found_q_num > max_val: 
                        found_q_num = None
                    else:
                        if is_strong_anchor:
                            if relative_x > (width * 0.20): found_q_num = None 
                        else:
                            if relative_x > (width * 0.05): found_q_num = None

                    if found_q_num:
                        anchors.append({'q_num': found_q_num, 'page_idx': page_idx, 'top': curr_word['top'], 'col': col})
                i += 1

    unique_anchors = {}
    for a in anchors:
        if a['q_num'] not in unique_anchors: unique_anchors[a['q_num']] = a
    return sorted(unique_anchors.values(), key=lambda x: x['q_num'])

def extract_text_content(pdf_path, anchors, is_two_column=True):
    extracted_text = {}
    with pdfplumber.open(pdf_path) as pdf:
        pages = pdf.pages
        if not pages: return {}
        
        width = pages[0].width
        height = pages[0].height
        midpoint = width / 2
        BOTTOM_LIMIT = height * 0.92

        for i, start in enumerate(anchors):
            q_num = start['q_num']
            text_segments = []
            
            if i + 1 < len(anchors):
                end = anchors[i+1]
            else:
                end = {'page_idx': start['page_idx'], 'top': BOTTOM_LIMIT, 'col': start['col']}

            curr_pidx = start['page_idx']
            curr_col = start['col']
            curr_top = start['top']
            
            while True:
                page = pages[curr_pidx]
                if is_two_column:
                    x0 = 0 if curr_col == 0 else midpoint
                    x1 = midpoint if curr_col == 0 else width
                else:
                    x0 = 0; x1 = width
                
                if curr_pidx == end['page_idx'] and curr_col == end['col']:
                    bottom = end['top']
                    done = True
                else:
                    bottom = BOTTOM_LIMIT
                    done = False
                
                if bottom > curr_top:
                    try:
                        cropped_page = page.crop((x0, curr_top, x1, bottom))
                        text = cropped_page.extract_text()
                        if text: text_segments.append(text)
                    except Exception: pass
                
                if done: break
                
                if is_two_column:
                    if curr_col == 0:
                        curr_col = 1; curr_top = 50 
                    else:
                        curr_col = 0; curr_pidx += 1; curr_top = 50
                else:
                    curr_pidx += 1; curr_top = 50
                
                if curr_pidx >= len(pages): break

            full_text = "\n".join(text_segments).strip()
            extracted_text[q_num] = full_text
            
    return extracted_text

def crop_and_save_standard(pdf_path, anchors, output_folder, suffix_type, is_two_column=True):
    try: 
        pdf_images = convert_from_path(pdf_path, dpi=300)
    except: 
        print(f"‚ùå Error converting PDF: {os.path.basename(pdf_path)}")
        return

    if not pdf_images: return
    page_width, page_height = pdf_images[0].size
    scale = 300 / 72 
    midpoint_px = (page_width / 2) 
    FOOTER_CUTOFF_PX = page_height * 0.92
    VERTICAL_PADDING = 15 
    TOP_MARGIN = 50 * scale 
    
    pbar = tqdm(total=len(anchors), desc=f"   üì∑ Cropping {suffix_type}", leave=True)
    
    for i, start in enumerate(anchors):
        q_num = start['q_num']
        
        if i + 1 < len(anchors): 
            end = anchors[i+1]
        else: 
            end = {'page_idx': start['page_idx'], 'top': FOOTER_CUTOFF_PX / scale, 'col': start['col']} 

        start_top_px = max(0, (start['top'] * scale) - VERTICAL_PADDING)
        end_top_px = (end['top'] * scale) - VERTICAL_PADDING

        try:
            images_to_stitch = []
            if is_two_column:
                start_left = 0 if start['col'] == 0 else midpoint_px
                start_right = midpoint_px if start['col'] == 0 else page_width
                end_left = 0 if end['col'] == 0 else midpoint_px
                end_right = midpoint_px if end['col'] == 0 else page_width
            else:
                start_left = 0; start_right = page_width
                end_left = 0; end_right = page_width

            if start['page_idx'] == end['page_idx']:
                if start['col'] == end['col']:
                    bottom = min(end_top_px, FOOTER_CUTOFF_PX)
                    if bottom <= start_top_px: bottom = start_top_px + 10 
                    images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, bottom)))
                else:
                    if FOOTER_CUTOFF_PX > start_top_px:
                        images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, FOOTER_CUTOFF_PX)))
                    if end_top_px > TOP_MARGIN:
                        images_to_stitch.append(pdf_images[start['page_idx']].crop((end_left, TOP_MARGIN, end_right, end_top_px)))
            else:
                if FOOTER_CUTOFF_PX > start_top_px:
                    images_to_stitch.append(pdf_images[start['page_idx']].crop((start_left, start_top_px, start_right, FOOTER_CUTOFF_PX)))
                for mid_idx in range(start['page_idx'] + 1, end['page_idx']):
                    if FOOTER_CUTOFF_PX > TOP_MARGIN:
                        images_to_stitch.append(pdf_images[mid_idx].crop((start_left, TOP_MARGIN, start_right, FOOTER_CUTOFF_PX)))
                if end_top_px > TOP_MARGIN:
                    images_to_stitch.append(pdf_images[end['page_idx']].crop((end_left, TOP_MARGIN, end_right, end_top_px)))

            if images_to_stitch:
                total_height = sum(img.height for img in images_to_stitch)
                if total_height > 0:
                    max_width = max(img.width for img in images_to_stitch)
                    final_img = Image.new('RGB', (max_width, total_height), (255, 255, 255))
                    y_offset = 0
                    for img in images_to_stitch:
                        final_img.paste(img, (0, y_offset))
                        y_offset += img.height
                    
                    final_img = trim_whitespace(final_img)
                    filename = f"{suffix_type}_{q_num}.png"
                    final_img.save(os.path.join(output_folder, filename))
                
        except Exception as e:
            print(f"‚ùå Error Saving {suffix_type}_{q_num}: {e}")
            
        pbar.update(1)
    pbar.close()

# --- 3. TARGETING ---
print(f"\n--- üîç SCANNING ---")
processed_folders = set()
final_master_df = pd.DataFrame()

# 1. Load Processed List (Switching to CSV logic)
if os.path.exists(MASTER_CSV_PATH):
    try:
        final_master_df = pd.read_csv(MASTER_CSV_PATH)
        print(f"‚úÖ Active CSV Loaded ({len(final_master_df)} rows).")
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading Master CSV: {e}")

elif os.path.exists(MASTER_XLSX_PATH):
    # Fallback: Initialize CSV from Read-Only Excel if CSV doesn't exist
    try:
        print(f"‚ÑπÔ∏è CSV not found. Initializing from DB Master.xlsx...")
        final_master_df = pd.read_excel(MASTER_XLSX_PATH)
        # Save immediately to establish the CSV
        final_master_df.to_csv(MASTER_CSV_PATH, index=False)
        print(f"‚úÖ Created DB Master.csv from Excel source.")
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading Master XLSX: {e}")

if 'Folder' in final_master_df.columns:
    processed_folders = set(final_master_df['Folder'].dropna().astype(str).unique())
    print(f"   Found {len(processed_folders)} processed folders in DB.")

# 2. Load Central Metadata
central_meta_df = pd.DataFrame()
valid_folders_set = set()

if os.path.exists(METADATA_FILE_PATH):
    try:
        central_meta_df = pd.read_excel(METADATA_FILE_PATH, sheet_name='CD_Metadata')
        central_meta_df.columns = central_meta_df.columns.str.strip()
        
        if 'Folder' in central_meta_df.columns:
            central_meta_df['Folder'] = central_meta_df['Folder'].astype(str).str.strip()
            valid_folders_set = set(central_meta_df['Folder'].unique())
            
        print(f"‚úÖ Metadata Loaded. Found {len(central_meta_df)} rows. {len(valid_folders_set)} valid folders.")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not load 'CD_Metadata' from Excel. {e}")
else:
    print("‚ö†Ô∏è Warning: DB Metadata.xlsx not found.")

if not os.path.exists(RAW_DATA_PATH):
    print(f"‚ùå CRITICAL: Folder not found: {RAW_DATA_PATH}")
    exit()

all_subfolders = [f.path for f in os.scandir(RAW_DATA_PATH) if f.is_dir()]
target_folders = []

print(f"üìÇ Looking in 'raw data'...")

for folder_path in all_subfolders:
    folder_name = os.path.basename(folder_path)
    clean_name = folder_name.strip()
    
    if not clean_name.startswith("CollegeDoors"): 
        continue
    
    if clean_name not in valid_folders_set:
        continue

    if clean_name in processed_folders: 
        continue

    print(f"   ‚úÖ FOUND NEW: {clean_name}")
    target_folders.append(folder_path)

if not target_folders:
    print("\nüéâ All valid folders from Metadata are processed! Exiting.")
    exit()

# --- 4. PROCESSING LOOP ---
start_time = time.time()
current_global_id = start_id
new_data_list = []

for folder in tqdm(target_folders, desc="Processing Batches"):
    test_name = os.path.basename(folder).strip()
    print(f"\nüîπ Processing: {test_name}")
    
    q_papers = glob.glob(os.path.join(folder, "*question_paper.pdf"))
    sol_pdfs = glob.glob(os.path.join(folder, "*solution_pdf.pdf"))
    excel_keys = glob.glob(os.path.join(folder, "*excel_answer_key.xlsx")) + glob.glob(os.path.join(folder, "*excel_answer_key.csv"))
    
    if not (q_papers and sol_pdfs and excel_keys):
        print(f"   ‚ö†Ô∏è Missing required files. Skipping.")
        continue

    try:
        key_path = excel_keys[0]
        key_df = pd.read_csv(key_path) if key_path.endswith('.csv') else pd.read_excel(key_path)
        total_questions = key_df['Question No.'].max() if 'Question No.' in key_df.columns else len(key_df)
        
        # --- METADATA LOGIC ---
        folder_specific_meta = pd.DataFrame()
        if not central_meta_df.empty:
            folder_specific_meta = central_meta_df[central_meta_df['Folder'] == test_name].copy()
        
        if not folder_specific_meta.empty:
            meta_df = folder_specific_meta
            if 'Q' in meta_df.columns:
                meta_df = meta_df.rename(columns={'Q': 'Question No.'})
            print(f"   ‚ÑπÔ∏è Loaded metadata from Central Sheet ({len(meta_df)} rows).")
        else:
            print(f"   ‚ö†Ô∏è Metadata missing in Excel. Creating fallback.")
            meta_df = pd.DataFrame({'Question No.': key_df['Question No.']})
            meta_df['Topic'] = "Unknown"
            meta_df['Sub-Topic'] = "Unknown"
            meta_df['Subject'] = "Unknown"

    except Exception as e:
        print(f"   ‚ùå Data Load Error: {e}")
        continue
    
    test_output_dir = os.path.join(OUTPUT_BASE, test_name)
    os.makedirs(test_output_dir, exist_ok=True)
    
    # 1. Calculate Anchors
    q_anchors = find_anchors_robust(q_papers[0], max_val=total_questions)
    sol_anchors = find_anchors_robust(sol_pdfs[0], max_val=total_questions, is_solution=True)

    # 2. Extract Images
    crop_and_save_standard(q_papers[0], q_anchors, test_output_dir, "Q", is_two_column=True)
    crop_and_save_standard(sol_pdfs[0], sol_anchors, test_output_dir, "Sol", is_two_column=True)

    # 3. Extract Text
    print("   üìù Extracting & Cleaning Text...")
    extracted_text_map = extract_text_content(q_papers[0], q_anchors, is_two_column=True)

    # DataFrame Logic
    meta_df['Folder'] = test_name
    
    # Merge Metadata with Answer Key
    if 'Question No.' in meta_df.columns and 'Question No.' in key_df.columns:
        combined_df = pd.merge(meta_df, key_df, on='Question No.', how='left', suffixes=('', '_key'))
    else:
        combined_df = pd.concat([meta_df.reset_index(drop=True), key_df.reset_index(drop=True)], axis=1)

    # --- FIX: COLUMN CLEANUP (DUPLICATE ANSWERS) ---
    # Merge 'Correct Answer_key' into 'Correct Answer' if it exists
    if 'Correct Answer_key' in combined_df.columns:
        if 'Correct Answer' in combined_df.columns:
            # Fill blanks in main col with values from key col
            combined_df['Correct Answer'] = combined_df['Correct Answer'].fillna(combined_df['Correct Answer_key'])
        else:
            # Rename key col to main col
            combined_df['Correct Answer'] = combined_df['Correct Answer_key']
        
        # Drop the extra key column
        combined_df.drop(columns=['Correct Answer_key'], inplace=True)

    unique_ids_col = []
    pdf_text_col = []
    text_avail_col = []
    
    for idx, row in combined_df.iterrows():
        q_num = row.get('Question No.')
        if pd.isna(q_num): 
            unique_ids_col.append(None)
            pdf_text_col.append(None)
            text_avail_col.append("No")
            continue
            
        q_num = int(q_num)
        
        current_global_id += 1
        unique_ids_col.append(current_global_id)
        
        raw_text = extracted_text_map.get(q_num, "")
        cleaned_text = re.sub(r'\b\S*_\S*\b', '', raw_text) # Remove ATPH_...
        cleaned_text = " ".join(cleaned_text.split())
        
        if len(cleaned_text) < 30:
            pdf_text_col.append("")
            text_avail_col.append("No")
        else:
            pdf_text_col.append(cleaned_text)
            text_avail_col.append("Yes")

    combined_df['unique_id'] = unique_ids_col
    combined_df['pdf_Text'] = pdf_text_col
    combined_df['PDF_Text_Available'] = text_avail_col
    
    # --- FIX: QC STATUS DEFAULT ---
    # Set to 'Pending QC' so we don't assume Pass
    combined_df['QC_Status'] = "Pending QC"
    
    combined_df.fillna("Unknown", inplace=True)
    combined_df = combined_df[combined_df['unique_id'].notna()]

    # --- INCREMENTAL SAVE (DB MASTER CSV & CONFIG) ---
    if not combined_df.empty:
        try:
            # 1. Update Master DF in memory
            final_master_df = pd.concat([final_master_df, combined_df], ignore_index=True)
            
            # 2. Write to CSV (Instead of Excel)
            final_master_df.to_csv(MASTER_CSV_PATH, index=False)
            
            # 3. Update Config Counter
            config['last_unique_id'] = current_global_id
            with open(CONFIG_PATH, 'w') as f:
                json.dump(config, f, indent=4)
                
            print(f"   üíæ SAVED: Added {len(combined_df)} questions to DB Master.csv. (Current ID: {current_global_id})")
            
        except Exception as e:
            print(f"   ‚ùå FATAL SAVE ERROR: {e}")

print("\n--- Finalizing ---")
print("‚úÖ All folders processed and saved.")

================================================================================
FILE: batchUnzipCD.py
================================================================================
import pandas as pd
import zipfile
import os
import json
import warnings
from tqdm import tqdm

# Suppress openpyxl warnings
warnings.filterwarnings("ignore", category=UserWarning) 

def batch_unzip():
    # --- 1. CONFIGURATION ---
    CONFIG_PATH = 'config.json'
    
    config = {
        "BASE_PATH": r"D:\Main\3. Work - Teaching\Projects\Question extractor",
        "METADATA_FILENAME": "DB Metadata.xlsx"
    }
    
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r') as f:
            config.update(json.load(f))
            
    BASE_PATH = os.path.normpath(config['BASE_PATH'])
    RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw data')
    METADATA_PATH = os.path.join(BASE_PATH, config.get("METADATA_FILENAME", "DB Metadata.xlsx"))

    print(f"üîß CONFIGURATION:")
    print(f"   - Raw Data Dir : {RAW_DATA_PATH}")
    print(f"   - Metadata     : {METADATA_PATH}")

    # --- 2. LOAD METADATA ---
    if not os.path.exists(METADATA_PATH):
        print(f"‚ùå Metadata file not found: {METADATA_PATH}")
        return

    try:
        df = pd.read_excel(METADATA_PATH, sheet_name='CD_Metadata')
        df.columns = df.columns.str.strip()
        
        col_map = {
            'Zip_File': 'Zip_Prefix',
            'Zip_Filepath': 'Zip_Prefix', 
            'Unzip Folder name': 'Target_Name'
        }
        df = df.rename(columns=col_map)
        
        if 'Zip_Prefix' not in df.columns or 'Target_Name' not in df.columns:
            print(f"‚ùå Error: Sheet must contain 'Zip_File' and 'Unzip Folder name'")
            return
            
        df = df.dropna(subset=['Zip_Prefix', 'Target_Name'])
        
    except Exception as e:
        print(f"‚ùå Error reading Excel: {e}")
        return

    # --- 3. SCAN FILES ---
    try:
        all_files = [f for f in os.listdir(RAW_DATA_PATH) if f.lower().endswith('.zip')]
    except Exception as e:
        print(f"‚ùå Error accessing raw data folder: {e}")
        return

    print(f"\nüìÇ Metadata has {len(df)} entries. Scanning {len(all_files)} zip files on disk...")

    # --- 4. PROCESSING LOOP ---
    success_count = 0
    skipped_count = 0
    
    for index, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
        zip_prefix = str(row['Zip_Prefix']).strip()
        clean_name = str(row['Target_Name']).strip()
        
        # Define Target Path
        final_folder_name = f"CollegeDoors - {clean_name}"
        target_full_path = os.path.join(RAW_DATA_PATH, final_folder_name)

        # --- SKIP LOGIC ---
        # If folder exists AND has files in it, skip.
        if os.path.exists(target_full_path) and len(os.listdir(target_full_path)) > 0:
            skipped_count += 1
            continue

        # Find matching zips
        matching_zips = [f for f in all_files if f.startswith(zip_prefix)]
        
        if not matching_zips:
            continue

        # Create Folder
        os.makedirs(target_full_path, exist_ok=True)

        # Extract
        for zip_filename in matching_zips:
            zip_full_path = os.path.join(RAW_DATA_PATH, zip_filename)
            try:
                with zipfile.ZipFile(zip_full_path, 'r') as zip_ref:
                    members = [m for m in zip_ref.namelist() if not m.startswith('__MACOSX')]
                    zip_ref.extractall(target_full_path, members=members)
            except Exception as e:
                print(f"\n   ‚ùå Error extracting {zip_filename}: {e}")
        
        success_count += 1

    print(f"\n‚ú® Task Complete.")
    print(f"   ‚úÖ Extracted: {success_count}")
    print(f"   ‚è© Skipped:   {skipped_count} (Already done)")

if __name__ == "__main__":
    batch_unzip()

================================================================================
FILE: CD_MergeAllAnswerKeys.py
================================================================================
import pandas as pd
import os
import json
from tqdm import tqdm

def merge_answer_keys():
    # --- 1. CONFIGURATION ---
    CONFIG_PATH = 'config.json'
    DEFAULT_BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor'
    
    # Load Config
    config = {}
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r') as f:
            config = json.load(f)
            
    BASE_PATH = config.get('BASE_PATH', DEFAULT_BASE_PATH)
    RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw data')
    SUMMARY_CSV = os.path.join(BASE_PATH, 'CD_QuestionCount.csv')
    OUTPUT_CSV = os.path.join(BASE_PATH, 'CQ_Questions_Master.csv')

    print(f"üîß CONFIGURATION:")
    print(f"   - Reading Map  : {SUMMARY_CSV}")
    print(f"   - Raw Data Dir : {RAW_DATA_PATH}")
    print(f"   - Output File  : {OUTPUT_CSV}")

    # --- 2. LOAD SUMMARY ---
    if not os.path.exists(SUMMARY_CSV):
        print(f"‚ùå Error: {SUMMARY_CSV} not found. Please run count_questions.py first.")
        return

    summary_df = pd.read_csv(SUMMARY_CSV)
    
    # Check for 'Chapter' column
    if 'Chapter' not in summary_df.columns:
        print("‚ö†Ô∏è Warning: 'Chapter' column not found in CD_QuestionCount.csv. Columns will be empty.")
        summary_df['Chapter'] = "Unknown"

    print(f"\nüìÇ Found {len(summary_df)} folders to process.")

    # --- 3. MERGE LOOP ---
    all_data_frames = []
    success_count = 0
    missing_count = 0

    for index, row in tqdm(summary_df.iterrows(), total=len(summary_df), desc="Merging"):
        folder_name = str(row['Folder']).strip()
        filename = str(row['File_Used']).strip()
        chapter_name = str(row['Chapter']).strip()
        
        # Skip if no valid file was found previously
        if filename.lower() in ['none', 'nan', '']:
            continue

        file_path = os.path.join(RAW_DATA_PATH, folder_name, filename)

        if os.path.exists(file_path):
            try:
                # Read the individual answer key
                if filename.lower().endswith('.csv'):
                    df = pd.read_csv(file_path)
                else:
                    df = pd.read_excel(file_path)
                
                # --- ENRICHMENT ---
                # Add the Chapter info from your summary CSV
                df['Chapter_From_Summary'] = chapter_name 
                # Add Folder name for traceability
                df['Source_Folder'] = folder_name
                
                all_data_frames.append(df)
                success_count += 1
                
            except Exception as e:
                print(f"\n‚ùå Error reading {filename}: {e}")
        else:
            # print(f"\n‚ö†Ô∏è File not found: {file_path}")
            missing_count += 1

    # --- 4. CONCATENATE & SAVE ---
    if all_data_frames:
        print(f"\nüîÑ Concatenating {len(all_data_frames)} files...")
        master_df = pd.concat(all_data_frames, ignore_index=True)
        
        # Move key columns to the front for better visibility
        cols = list(master_df.columns)
        priorities = ['Source_Folder', 'Chapter_From_Summary', 'Question No.', 'Q']
        for p in reversed(priorities):
            if p in cols:
                cols.insert(0, cols.pop(cols.index(p)))
        master_df = master_df[cols]

        master_df.to_csv(OUTPUT_CSV, index=False)
        print(f"\n‚úÖ Success! Master CSV created at: {OUTPUT_CSV}")
        print(f"   Total Rows: {len(master_df)}")
        print(f"   Files Merged: {success_count}")
        if missing_count > 0:
            print(f"   Missing Files: {missing_count}")
    else:
        print("\n‚ö†Ô∏è No data collected. Check your paths and summary CSV.")

if __name__ == "__main__":
    merge_answer_keys()

================================================================================
FILE: CD_QuestionCount.py
================================================================================
import os
import glob
import pandas as pd
import json

def count_questions():
    # --- 1. CONFIGURATION ---
    CONFIG_PATH = 'config.json'
    DEFAULT_BASE_PATH = 'D:/Main/3. Work - Teaching/Projects/Question extractor'
    
    # Load Config or use default
    config = {}
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r') as f:
            config = json.load(f)
            
    BASE_PATH = config.get('BASE_PATH', DEFAULT_BASE_PATH)
    RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw data')
    OUTPUT_CSV = os.path.join(BASE_PATH, 'CD_QuestionCount.csv')

    print(f"üìÇ Scanning: {RAW_DATA_PATH}")

    if not os.path.exists(RAW_DATA_PATH):
        print("‚ùå Error: Raw data folder not found.")
        return

    # --- 2. SCANNING LOOP ---
    results = []
    
    # Get all subfolders
    subfolders = [f.path for f in os.scandir(RAW_DATA_PATH) if f.is_dir()]
    
    print(f"üîç Found {len(subfolders)} total folders. Filtering for 'CollegeDoors'...")

    for folder_path in subfolders:
        folder_name = os.path.basename(folder_path).strip()
        
        # Filter logic
        if not folder_name.startswith("CollegeDoors"):
            continue

        # Look for the answer key
        excel_files = glob.glob(os.path.join(folder_path, "*answer_key.xlsx"))
        
        # Fallback for CSV if XLSX isn't found (just in case)
        if not excel_files:
            excel_files = glob.glob(os.path.join(folder_path, "*answer_key.csv"))

        count = 0
        file_used = "None"
        status = "Missing Key"

        if excel_files:
            file_used = os.path.basename(excel_files[0])
            try:
                # Read the file
                if file_used.endswith('.csv'):
                    df = pd.read_csv(excel_files[0])
                else:
                    df = pd.read_excel(excel_files[0])
                
                # Count logic: prefer 'Question No.' column, otherwise just row count
                if 'Question No.' in df.columns:
                    count = df['Question No.'].nunique()
                else:
                    count = len(df)
                
                status = "Success"
            except Exception as e:
                status = f"Error: {str(e)}"
        
        results.append({
            "Folder": folder_name,
            "Question_Count": count,
            "File_Used": file_used,
            "Status": status
        })
        
        print(f"   üëâ {folder_name}: {count}")

    # --- 3. SAVE RESULTS ---
    if results:
        df_results = pd.DataFrame(results)
        df_results.to_csv(OUTPUT_CSV, index=False)
        print(f"\n‚úÖ Done. Summary saved to: {OUTPUT_CSV}")
        print(f"   Total Questions Found: {df_results['Question_Count'].sum()}")
    else:
        print("\n‚ö†Ô∏è No matching folders found.")

if __name__ == "__main__":
    count_questions()

================================================================================
FILE: DBHealthCheck.py
================================================================================
import pandas as pd
import os
from pathlib import Path
from tqdm import tqdm

# --- CONFIGURATION ---
# BASE_PATH = Path(r"D:/Main/3. Work - Teaching/Projects/Question extractor")
BASE_PATH = Path(".") # Use this if running inside the folder

CSV_PATH = BASE_PATH / "DB Master.csv"
METADATA_PATH = BASE_PATH / "DB Metadata.xlsx"
IMG_BASE_DIR = BASE_PATH / "Processed_Database"
COMPRESSED_BASE_DIR = IMG_BASE_DIR / "Compressed"

OUTPUT_REPORT = BASE_PATH / "DB Health Check.csv"

def load_chapter_topic_map():
    """
    Loads metadata into a simple dictionary: { 'Chapter': {'Topic1', 'Topic2'} }
    """
    if not METADATA_PATH.exists():
        print(f"‚ùå Error: Metadata file not found at {METADATA_PATH}")
        return {}

    try:
        df_meta = pd.read_excel(METADATA_PATH, sheet_name="Syllabus tree")
        df_meta = df_meta.map(lambda x: str(x).strip() if pd.notna(x) else "nan")
        
        chapter_map = {}
        for _, row in df_meta.iterrows():
            chap = row.get('Chapter', 'nan')
            top = row.get('Topic', 'nan')

            if chap == 'nan': continue

            if chap not in chapter_map: 
                chapter_map[chap] = set()
            
            if top != 'nan':
                chapter_map[chap].add(top)
                
        print(f"‚úÖ Loaded {len(chapter_map)} Chapters from Metadata.")
        return chapter_map
    except Exception as e:
        print(f"‚ùå Error reading metadata: {e}")
        return {}

def check_db_health():
    # 1. Load Data
    if not CSV_PATH.exists():
        print(f"‚ùå Master DB not found at {CSV_PATH}")
        return

    print("‚è≥ Loading DB Master...")
    df = pd.read_csv(CSV_PATH)
    
    # Normalize Inputs
    df.columns = df.columns.str.strip()
    df['unique_id'] = df['unique_id'].astype(str).str.strip()
    
    valid_chapters = load_chapter_topic_map()
    
    report_data = []
    
    print(f"üöÄ Validating {len(df)} records...")

    for idx, row in tqdm(df.iterrows(), total=len(df), unit="q"):
        uid = row.get('unique_id', f"Unknown_{idx}")
        folder = str(row.get('Folder', '')).strip()
        q_num = str(row.get('Question No.', '')).strip()
        
        # --- A. HIERARCHY VALIDATION ---
        chap = str(row.get('Chapter', '')).strip()
        top = str(row.get('Topic', '')).strip()
        
        chap_issue = "None"
        topic_issue = "None"
        is_tag_valid = True
        
        # 1. Check Chapter
        if chap == '' or chap.lower() == 'nan':
             chap_issue = "Missing"
             is_tag_valid = False
        elif chap not in valid_chapters:
            chap_issue = "Unknown" # Typo or not in Tree
            is_tag_valid = False
            
        # 2. Check Topic (Only checks if Chapter is valid)
        if chap_issue == "None":
            # If topic exists, it MUST be in the allowed list for this chapter
            if top and top.lower() != 'nan':
                allowed_topics = valid_chapters[chap]
                if allowed_topics and top not in allowed_topics:
                    topic_issue = "Invalid"
                    is_tag_valid = False
        else:
            # If chapter is invalid, we can't validate topic strictly
            if top and top.lower() != 'nan':
                 topic_issue = "Cannot Validate (Bad Chapter)"

        # --- B. ASSET VALIDATION (Compressed Only) ---
        fname_q = f"Q_{q_num}.png"
        fname_sol = f"Sol_{q_num}.png"
        
        path_comp_q = COMPRESSED_BASE_DIR / folder / fname_q
        path_comp_sol = COMPRESSED_BASE_DIR / folder / fname_sol
        
        has_comp_q = path_comp_q.exists()
        has_comp_sol = path_comp_sol.exists()
        
        # --- C. STATUS ---
        # Ready if: Tags Valid AND Compressed Question Exists
        is_ready = is_tag_valid and has_comp_q

        report_data.append({
            'unique_id': uid,
            'Folder': folder,
            'Question No.': q_num,
            'Chapter': chap,
            'Topic': top,
            'Chapter_Tag_Issue': chap_issue,
            'Topic_Tag_Issue': topic_issue,
            'Comp_Q_Found': has_comp_q,
            'Comp_Sol_Found': has_comp_sol,
            'OVERALL_STATUS': "READY" if is_ready else "FAIL"
        })

    # --- SAVE REPORT ---
    df_report = pd.DataFrame(report_data)
    df_report.to_csv(OUTPUT_REPORT, index=False)
    
    # --- SUMMARY ---
    print("\n" + "="*40)
    print("      HEALTH CHECK SUMMARY")
    print("="*40)
    
    total = len(df_report)
    passed = df_report[df_report['OVERALL_STATUS'] == "READY"].shape[0]
    failed = total - passed
    
    print(f"Total Records:      {total}")
    print(f"‚úÖ READY to Migrate: {passed} ({(passed/total)*100:.1f}%)")
    print(f"‚ùå FAILED Check:     {failed} ({(failed/total)*100:.1f}%)")
    print("-" * 40)
    
    if failed > 0:
        chap_fails = df_report[df_report['Chapter_Tag_Issue'] != "None"].shape[0]
        topic_fails = df_report[df_report['Topic_Tag_Issue'] == "Invalid"].shape[0]
        missing_imgs = df_report[~df_report['Comp_Q_Found']].shape[0]
        
        print("Failure Breakdown:")
        print(f" ‚Ä¢ Chapter Issues: {chap_fails}")
        print(f" ‚Ä¢ Topic Issues:   {topic_fails}")
        print(f" ‚Ä¢ Missing Images: {missing_imgs}")
        
    print(f"\nDetailed report saved to: {OUTPUT_REPORT}")
    print("="*40)

if __name__ == "__main__":
    check_db_health()

================================================================================
FILE: deleteFilesUploadedToDrive.py
================================================================================
import pandas as pd
import os
from tqdm import tqdm

# ================= CONFIGURATION =================
CSV_FILE_PATH = 'FilesToUpload.csv'  # Path to your log file
DRY_RUN = False  # Set to False to actually delete files
# =================================================

def clean_up_files():
    if not os.path.exists(CSV_FILE_PATH):
        print(f"Error: Log file '{CSV_FILE_PATH}' not found.")
        return

    print("Reading log file...")
    try:
        df = pd.read_csv(CSV_FILE_PATH)
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return

    # Filter only files that were successfully uploaded
    files_to_delete = df[df['Status'] == 'Uploaded']
    
    total_files = len(files_to_delete)
    deleted_count = 0
    errors_count = 0
    bytes_cleared = 0
    
    print(f"Found {total_files} files marked as 'Uploaded'.")
    
    if total_files == 0:
        print("No files to delete.")
        return

    if DRY_RUN:
        print("\n--- DRY RUN MODE: No files will be deleted ---")
    else:
        print("\n--- DELETING FILES ---")

    # Iterate with progress bar
    for index, row in tqdm(files_to_delete.iterrows(), total=total_files, unit="file"):
        file_path = row['FullPath']
        
        # Check if file exists locally
        if os.path.exists(file_path):
            try:
                file_size = os.path.getsize(file_path)
                
                if not DRY_RUN:
                    os.remove(file_path)
                
                bytes_cleared += file_size
                deleted_count += 1
                
            except Exception as e:
                errors_count += 1
                # tqdm.write allows printing without breaking the progress bar layout
                tqdm.write(f"Error deleting {file_path}: {e}")
        else:
            # File might already be deleted or moved
            if DRY_RUN:
                 tqdm.write(f"Skipping (Not Found): {file_path}")
            errors_count += 1

    # Final Report
    mb_cleared = bytes_cleared / (1024 * 1024)
    gb_cleared = mb_cleared / 1024
    
    print("\n" + "="*30)
    print("       SUMMARY       ")
    print("="*30)
    if DRY_RUN:
        print(f"Mode:          DRY RUN (Simulation)")
        print(f"Would Delete:  {deleted_count} files")
        print(f"Would Reclaim: {mb_cleared:.2f} MB ({gb_cleared:.2f} GB)")
    else:
        print(f"Mode:          LIVE DELETION")
        print(f"Deleted:       {deleted_count} files")
        print(f"Errors/Missing:{errors_count} files")
        print(f"Space Cleared: {mb_cleared:.2f} MB ({gb_cleared:.2f} GB)")
    print("="*30)

if __name__ == "__main__":
    clean_up_files()

================================================================================
FILE: fetchManualTagsFromMetadata.py
================================================================================
import pandas as pd
import os

def update_database_from_metadata():
    # --- 1. CONFIGURATION ---
    base_path = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
    main_db_path = os.path.join(base_path, "DB Master.csv") # Assuming csv based on your prompt
    metadata_path = os.path.join(base_path, "DB Metadata.xlsx")
    output_path = os.path.join(base_path, "DB Master_Updated.csv")

    print("Loading databases...")
    
    # Load Main DB
    if not os.path.exists(main_db_path):
        # Fallback to xlsx if csv doesn't exist, just in case
        main_db_path = os.path.join(base_path, "DB Master.xlsx")
        if not os.path.exists(main_db_path):
            print(f"‚ùå Error: Could not find DB Master at {base_path}")
            return
        main_df = pd.read_excel(main_db_path)
    else:
        main_df = pd.read_csv(main_db_path)

    # Load Tagged Metadata
    try:
        tagged_df = pd.read_excel(metadata_path, sheet_name="CD_Metadata")
    except Exception as e:
        print(f"‚ùå Error loading metadata sheet: {e}")
        return

    # --- 2. CREATE TEMP KEYS ---
    print("Creating temporary keys for matching...")
    
    # Ensure columns are strings to avoid type mismatch errors
    main_df['Folder'] = main_df['Folder'].fillna('').astype(str).str.strip()
    main_df['Q'] = main_df['Q'].fillna('').astype(str).str.strip()
    
    tagged_df['Folder'] = tagged_df['Folder'].fillna('').astype(str).str.strip()
    tagged_df['Q'] = tagged_df['Q'].fillna('').astype(str).str.strip()

    # Create Key: "Folder_Q"
    main_df['Temp_Key'] = main_df['Folder'] + "_" + main_df['Q']
    tagged_df['Temp_Key'] = tagged_df['Folder'] + "_" + tagged_df['Q']

    # Index tagged_df for faster lookup
    tagged_df.set_index('Temp_Key', inplace=True)

    # --- 3. FILL MISSING DATA ---
    print("Merging data...")
    
    # Counter for stats
    updates_count = 0
    missing_before = 0
    
    # Columns to check and fill
    target_columns = ['Chapter', 'Topic']

    for index, row in main_df.iterrows():
        key = row['Temp_Key']
        
        # Check if we have manual tags for this question
        if key in tagged_df.index:
            source_row = tagged_df.loc[key]
            
            # Handle duplicates in tagged_df (just take the first one if multiple exist)
            if isinstance(source_row, pd.DataFrame):
                source_row = source_row.iloc[0]

            row_updated = False
            
            for col in target_columns:
                current_val = str(row[col]).strip().lower()
                
                # Condition: If current value is missing, nan, or unknown
                if current_val in ['nan', 'unknown', '', 'none']:
                    
                    # Check if new value exists
                    new_val = str(source_row.get(col, '')).strip()
                    
                    if new_val and new_val.lower() not in ['nan', 'unknown', '', 'none']:
                        main_df.at[index, col] = new_val
                        row_updated = True
            
            if row_updated:
                updates_count += 1

    # --- 4. CLEANUP & SAVE ---
    # Remove the temporary key before saving
    main_df.drop(columns=['Temp_Key'], inplace=True)
    
    print(f"Saving updated database to: {output_path}")
    main_df.to_csv(output_path, index=False)

    # --- 5. INSIGHTS ---
    total_questions = len(main_df)
    
    # Calculate how many still need tags
    missing_chapter = main_df['Chapter'].map(lambda x: str(x).lower() in ['nan', 'unknown', '', 'none']).sum()
    missing_topic = main_df['Topic'].map(lambda x: str(x).lower() in ['nan', 'unknown', '', 'none']).sum()

    print("\n" + "="*50)
    print("            MIGRATION SUMMARY")
    print("="*50)
    print(f"‚Ä¢ Total Questions processed:   {total_questions}")
    print(f"‚Ä¢ Rows updated with new tags:  {updates_count}")
    print("-" * 50)
    print(f"‚Ä¢ Questions still missing Chapter: {missing_chapter}")
    print(f"‚Ä¢ Questions still missing Topic:   {missing_topic}")
    print("="*50)

if __name__ == "__main__":
    update_database_from_metadata()

================================================================================
FILE: GoogleDriveUploader.py
================================================================================
import os
import time
import json
import pickle
import pandas as pd
from tqdm import tqdm
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from googleapiclient.errors import HttpError

# --- CONFIGURATION ---
try:
    with open('config.json', 'r') as f:
        config = json.load(f)
        BASE_PATH = config.get('BASE_PATH', os.getcwd())
except FileNotFoundError:
    print("‚ö†Ô∏è config.json not found. Using current directory.")
    BASE_PATH = os.getcwd()

# File Paths
FOLDERS_CSV = os.path.join(BASE_PATH, 'FoldersToUploadToDrive.csv')  
MASTER_FILE_CSV = os.path.join(BASE_PATH, 'FilesToUpload.csv')     
# UPDATED: Pointing to your new OAuth JSON file
CLIENT_SECRETS_FILE = os.path.join(BASE_PATH, 'DriveUploaderOAuth.json') 
TOKEN_FILE = os.path.join(BASE_PATH, 'token.pickle')                 

# Target Shared Folder
SHARED_DRIVE_FOLDER_ID = "1pCo45FQsYAFdwXNBhFCQoZz0p0O3DBLL" 
SCOPES = ['https://www.googleapis.com/auth/drive']

class SmartMigrator:
    def __init__(self):
        self.service = self._authenticate()
        self.folder_cache = {} 
        print(f"üìÇ Target Drive Folder ID: {SHARED_DRIVE_FOLDER_ID}")

    def _authenticate(self):
        """Authenticates using OAuth (User Account) so quota works."""
        creds = None
        # Load existing login token if available
        if os.path.exists(TOKEN_FILE):
            with open(TOKEN_FILE, 'rb') as token:
                creds = pickle.load(token)
        
        # If no valid token, let user log in via browser
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                if not os.path.exists(CLIENT_SECRETS_FILE):
                    print(f"‚ùå CRITICAL: '{CLIENT_SECRETS_FILE}' missing.")
                    print("   Please check the filename in your folder.")
                    exit()
                    
                flow = InstalledAppFlow.from_client_secrets_file(
                    CLIENT_SECRETS_FILE, SCOPES)
                creds = flow.run_local_server(port=0)
            
            # Save the token for next time
            with open(TOKEN_FILE, 'wb') as token:
                pickle.dump(creds, token)
        
        return build('drive', 'v3', credentials=creds)

    def generate_file_list(self):
        if os.path.exists(MASTER_FILE_CSV):
            print(f"‚ÑπÔ∏è  '{MASTER_FILE_CSV}' found. Skipping Scan Phase.")
            return

        print("üîç Phase 1: Scanning folders...")
        file_list = []
        
        if not os.path.exists(FOLDERS_CSV):
            print(f"‚ùå Error: {FOLDERS_CSV} not found.")
            return

        try:
            df_folders = pd.read_csv(FOLDERS_CSV)
        except Exception as e:
            print(f"‚ùå Error reading folders csv: {e}")
            return

        for _, row in df_folders.iterrows():
            raw_path = str(row['FolderPath']).replace('"', '').replace("'", '').strip()
            if os.path.isabs(raw_path):
                base_folder = raw_path
            else:
                base_folder = os.path.join(BASE_PATH, raw_path)

            if not os.path.exists(base_folder):
                print(f"‚ö†Ô∏è  Skipping missing: {base_folder}")
                continue

            print(f"    Scanning: {base_folder}")
            
            for root, dirs, files in os.walk(base_folder):
                for file_name in files:
                    full_path = os.path.join(root, file_name)
                    try:
                        f_size = os.path.getsize(full_path)
                    except:
                        f_size = 0
                    
                    file_list.append({
                        'FullPath': full_path,
                        'RootFolder': base_folder,
                        'FileSize': f_size,
                        'Status': 'Pending',
                        'DriveFileID': ''
                    })

        if file_list:
            df = pd.DataFrame(file_list)
            df.to_csv(MASTER_FILE_CSV, index=False)
            total_gb = df['FileSize'].sum() / (1024**3)
            print(f"‚úÖ Scan Complete. Found {len(df)} files ({total_gb:.2f} GB).")
        else:
            print("‚ö†Ô∏è  No files found to upload.")

    def _get_or_create_folder(self, folder_name, parent_id):
        cache_key = f"{parent_id}_{folder_name}"
        if cache_key in self.folder_cache:
            return self.folder_cache[cache_key]

        query = f"name = '{folder_name}' and '{parent_id}' in parents and mimeType = 'application/vnd.google-apps.folder' and trashed = false"
        try:
            response = self.service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
            files = response.get('files', [])
            if files:
                folder_id = files[0]['id']
            else:
                metadata = {'name': folder_name, 'mimeType': 'application/vnd.google-apps.folder', 'parents': [parent_id]}
                folder = self.service.files().create(body=metadata, fields='id').execute()
                folder_id = folder.get('id')
            
            self.folder_cache[cache_key] = folder_id
            return folder_id
        except HttpError:
            return parent_id 

    def upload_file_chunked(self, local_path, parent_id, pbar_bytes):
        file_name = os.path.basename(local_path)
        metadata = {'name': file_name, 'parents': [parent_id]}
        media = MediaFileUpload(local_path, resumable=True)

        try:
            request = self.service.files().create(body=metadata, media_body=media, fields='id')
            response = None
            previous_progress = 0
            
            while response is None:
                status, response = request.next_chunk()
                if status:
                    current_progress = int(status.resumable_progress)
                    chunk_size = current_progress - previous_progress
                    pbar_bytes.update(chunk_size)
                    previous_progress = current_progress
            
            # Update remaining bytes
            total_size = os.path.getsize(local_path)
            if previous_progress < total_size:
                pbar_bytes.update(total_size - previous_progress)
                
            return response.get('id'), None
        except Exception as e:
            return None, str(e)

    def start_upload(self):
        if not os.path.exists(MASTER_FILE_CSV):
            print("‚ùå Master CSV not found.")
            return

        # Load Data
        df = pd.read_csv(MASTER_FILE_CSV)
        
        # --- RESUME STATE ---
        total_files = len(df)
        total_bytes = df['FileSize'].sum()
        
        # Count what is ALREADY done
        completed_mask = df['Status'] == 'Uploaded'
        completed_files = completed_mask.sum()
        completed_bytes = df.loc[completed_mask, 'FileSize'].sum()
        
        if completed_files == total_files:
            print("üéâ All files are already uploaded!")
            return

        print(f"üöÄ Phase 2: Resuming Upload (User Auth)...")
        print(f"   Already Done: {completed_files} files ({completed_bytes / (1024**2):.1f} MB)")
        
        # Initialize Bars
        pbar_files = tqdm(total=total_files, initial=completed_files, unit='file', desc="Files", position=0, leave=True)
        pbar_bytes = tqdm(total=total_bytes, initial=completed_bytes, unit='B', unit_scale=True, desc="Data ", position=1, leave=True)

        try:
            for index, row in df.iterrows():
                
                # SKIP if already done
                if row['Status'] == 'Uploaded':
                    continue

                local_path = row['FullPath']
                root_folder = row['RootFolder']
                
                if not os.path.exists(local_path):
                    df.at[index, 'Status'] = 'Missing Local File'
                    pbar_files.update(1)
                    continue

                try:
                    # 1. Folder Logic
                    rel_path = os.path.relpath(local_path, root_folder)
                    folder_path = os.path.dirname(rel_path)
                    root_name = os.path.basename(root_folder)
                    
                    current_parent_id = self._get_or_create_folder(root_name, SHARED_DRIVE_FOLDER_ID)
                    
                    if folder_path and folder_path != ".":
                        for part in folder_path.split(os.sep):
                            current_parent_id = self._get_or_create_folder(part, current_parent_id)
                    
                    # 2. Upload
                    file_id, error_msg = self.upload_file_chunked(local_path, current_parent_id, pbar_bytes)

                    if file_id:
                        df.at[index, 'Status'] = 'Uploaded'
                        df.at[index, 'DriveFileID'] = file_id
                    else:
                        df.at[index, 'Status'] = f'Failed: {error_msg}'

                except Exception as e:
                    df.at[index, 'Status'] = f'Error: {str(e)}'

                # Update Counter
                pbar_files.update(1)

                # SAVE
                df.to_csv(MASTER_FILE_CSV, index=False)

        except KeyboardInterrupt:
            print("\nüõë Paused by User. Saving progress...")
        
        finally:
            df.to_csv(MASTER_FILE_CSV, index=False)
            pbar_files.close()
            pbar_bytes.close()
            print("\n‚úÖ Progress Saved.")

if __name__ == '__main__':
    migrator = SmartMigrator()
    migrator.generate_file_list()
    migrator.start_upload()

================================================================================
FILE: Home - Copy.py
================================================================================
import streamlit as st
import pandas as pd
import os
import plotly.express as px

# --- CONFIGURATION ---
# Get the folder where THIS script (Home.py) is located
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_PATH, 'DB Master.csv')

st.set_page_config(
    page_title="Question Bank HQ",
    page_icon="üè´",
    layout="wide"
)

st.title("üè´ Question Bank Command Center")

# --- AUTO-HEAL: Create DB if missing ---
if not os.path.exists(DB_PATH):
    try:
        # Create an empty dataframe with standard schema
        cols = [
            'Question No.', 'Folder', 'Subject', 'Chapter', 'Exam', 
            'Question type', 'Difficulty_tag', 'Correct Answer', 
            'Marks', 'PYQ', 'PYQ_Year', 'QC_Status', 'QC_Locked', 
            'manually updated'
        ]
        df_empty = pd.DataFrame(columns=cols)
        df_empty.to_excel(DB_PATH, index=False)
        st.toast("üÜï Database not found, so I created a fresh 'DB Master.xlsx' for you!", icon="‚ú®")
    except Exception as e:
        st.error(f"‚ùå Could not create database. Check permissions.\nError: {e}")

# --- QUICK ACTIONS ---
st.subheader("üöÄ Quick Actions")
col1, col2, col3 = st.columns(3)

# Note: These filenames must match exactly what is in your 'pages' folder!
with col1:
    if st.button("‚ûï Add Question Manually", use_container_width=True):
        st.switch_page("pages/1_Add_Question.py")

with col2:
    if st.button("‚úÖ Review & QC", use_container_width=True):
        st.switch_page("pages/2_Review_QC.py")

with col3:
    if st.button("üìù Create Question Paper", use_container_width=True):
        st.switch_page("pages/3_Create_Paper.py")

st.divider()

# --- METRICS DASHBOARD ---
if os.path.exists(DB_PATH):
    try:
        df = pd.read_excel(DB_PATH)
        
        # --- TOP LEVEL METRICS ---
        total_qs = len(df)
        
        if 'QC_Status' not in df.columns: df['QC_Status'] = 'Pending'
        
        # Normalize status
        pass_count = len(df[df['QC_Status'].astype(str).str.lower() == 'pass'])
        fail_count = len(df[df['QC_Status'].astype(str).str.lower() == 'fail'])
        pending_count = total_qs - pass_count - fail_count
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("üìö Total Questions", total_qs)
        c2.metric("‚úÖ Ready (Pass)", pass_count)
        c3.metric("‚ö†Ô∏è Needs Review", pending_count)
        c4.metric("‚ùå Failed", fail_count)
        
        st.divider()
        
        # --- CHARTS ---
        if total_qs > 0:
            col_charts1, col_charts2 = st.columns(2)
            
            with col_charts1:
                st.subheader("Subject Distribution")
                if 'Subject' in df.columns:
                    sub_counts = df['Subject'].value_counts().reset_index()
                    sub_counts.columns = ['Subject', 'Count']
                    fig_sub = px.pie(sub_counts, values='Count', names='Subject', hole=0.4)
                    st.plotly_chart(fig_sub, use_container_width=True)
                else:
                    st.info("Subject column missing.")

            with col_charts2:
                st.subheader("Exam Source")
                if 'Exam' in df.columns:
                    exam_counts = df['Exam'].value_counts().reset_index()
                    exam_counts.columns = ['Exam', 'Count']
                    exam_counts = exam_counts.sort_values(by='Count', ascending=True)
                    fig_exam = px.bar(exam_counts, x='Count', y='Exam', orientation='h')
                    st.plotly_chart(fig_exam, use_container_width=True)
                else:
                    st.info("Exam column missing.")
        else:
            st.info("Database is empty. Add some questions to see charts!")

    except Exception as e:
        st.error(f"Error reading database: {e}")
else:
    # Fallback debug info if creation failed
    st.error("‚ö†Ô∏è Critical Error: Database file is still missing.")
    st.code(f"Looking for: {DB_PATH}")

================================================================================
FILE: Home.py
================================================================================
import streamlit as st
import pandas as pd
import os
import plotly.express as px

# --- CONFIGURATION ---
# Get the folder where THIS script (Home.py) is located
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
# UPDATED: Pointing to CSV now
DB_PATH = os.path.join(BASE_PATH, 'DB Master.csv')

st.set_page_config(
    page_title="Question Bank HQ",
    page_icon="üè´",
    layout="wide"
)

st.title("üè´ Question Bank Command Center")

# --- AUTO-HEAL: Create DB if missing ---
if not os.path.exists(DB_PATH):
    try:
        # Create an empty dataframe with standard schema
        cols = [
            'Question No.', 'Folder', 'Subject', 'Chapter', 'Exam', 
            'Question type', 'Difficulty_tag', 'Correct Answer', 
            'Marks', 'PYQ', 'PYQ_Year', 'QC_Status', 'QC_Locked', 
            'manually updated'
        ]
        df_empty = pd.DataFrame(columns=cols)
        
        # UPDATED: Save as CSV instead of Excel
        df_empty.to_csv(DB_PATH, index=False)
        st.toast("üÜï Database not found, so I created a fresh 'DB Master.csv' for you!", icon="‚ú®")
    except Exception as e:
        st.error(f"‚ùå Could not create database. Check permissions.\nError: {e}")

# --- QUICK ACTIONS ---
st.subheader("üöÄ Quick Actions")
col1, col2, col3 = st.columns(3)

# Note: These filenames must match exactly what is in your 'pages' folder!
with col1:
    if st.button("‚ûï Add Question Manually", use_container_width=True):
        st.switch_page("pages/1_Add_Question.py")

with col2:
    if st.button("‚úÖ Review & QC", use_container_width=True):
        st.switch_page("pages/2_Review_QC.py")

with col3:
    if st.button("üìù Create Question Paper", use_container_width=True):
        st.switch_page("pages/3_Create_Paper.py")

st.divider()

# --- METRICS DASHBOARD ---
if os.path.exists(DB_PATH):
    try:
        # UPDATED: Read from CSV
        # low_memory=False prevents mixed-type warnings on large files
        df = pd.read_csv(DB_PATH, low_memory=False) 
        
        # --- TOP LEVEL METRICS ---
        total_qs = len(df)
        
        if 'QC_Status' not in df.columns: df['QC_Status'] = 'Pending'
        
        # Normalize status
        pass_count = len(df[df['QC_Status'].astype(str).str.lower() == 'pass'])
        fail_count = len(df[df['QC_Status'].astype(str).str.lower() == 'fail'])
        pending_count = total_qs - pass_count - fail_count
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("üìö Total Questions", total_qs)
        c2.metric("‚úÖ Ready (Pass)", pass_count)
        c3.metric("‚ö†Ô∏è Needs Review", pending_count)
        c4.metric("‚ùå Failed", fail_count)
        
        st.divider()
        
        # --- CHARTS ---
        if total_qs > 0:
            col_charts1, col_charts2 = st.columns(2)
            
            with col_charts1:
                st.subheader("Subject Distribution")
                if 'Subject' in df.columns:
                    # Clean up data for chart
                    sub_counts = df['Subject'].fillna('Unknown').value_counts().reset_index()
                    sub_counts.columns = ['Subject', 'Count']
                    fig_sub = px.pie(sub_counts, values='Count', names='Subject', hole=0.4)
                    st.plotly_chart(fig_sub, use_container_width=True)
                else:
                    st.info("Subject column missing.")

            with col_charts2:
                st.subheader("Exam Source")
                if 'Exam' in df.columns:
                    exam_counts = df['Exam'].fillna('Unknown').value_counts().reset_index()
                    exam_counts.columns = ['Exam', 'Count']
                    exam_counts = exam_counts.sort_values(by='Count', ascending=True)
                    fig_exam = px.bar(exam_counts, x='Count', y='Exam', orientation='h')
                    st.plotly_chart(fig_exam, use_container_width=True)
                else:
                    st.info("Exam column missing.")
        else:
            st.info("Database is empty. Add some questions to see charts!")

    except Exception as e:
        st.error(f"Error reading database: {e}")
else:
    # Fallback debug info if creation failed
    st.error("‚ö†Ô∏è Critical Error: Database file is still missing.")
    st.code(f"Looking for: {DB_PATH}")

================================================================================
FILE: imageCompression.py
================================================================================
import os
import csv
from PIL import Image
from tqdm import tqdm

def get_all_folders(base_dir, exclude_folder="Compressed"):
    """
    Scans the base directory and returns a list of all subfolders 
    excluding the 'Compressed' folder.
    """
    if not os.path.exists(base_dir):
        return []
    
    all_items = os.listdir(base_dir)
    folders = []
    
    for item in all_items:
        item_path = os.path.join(base_dir, item)
        
        # We only want directories, and we MUST skip the 'Compressed' folder
        if os.path.isdir(item_path) and item != exclude_folder:
            folders.append(item)
            
    return folders

def clean_and_compress_all(noise_threshold=170):
    # --- SETUP ---
    base_dir = "Processed_Database"
    compressed_base_dir = os.path.join(base_dir, "Compressed")
    csv_report_path = os.path.join(base_dir, "compression_stats.csv")

    # 1. Auto-discover folders
    folders_to_process = get_all_folders(base_dir, exclude_folder="Compressed")

    if not folders_to_process:
        print(f"No folders found in {base_dir} to process!")
        return

    # Data collection for stats
    stats_data = []
    grand_total_orig_bytes = 0
    grand_total_new_bytes = 0

    print(f"Found {len(folders_to_process)} folders to process.")
    print(f"Report will be saved to: {csv_report_path}\n")

    # --- OUTER LOOP: FOLDERS ---
    for folder_name in tqdm(folders_to_process, desc="Overall Progress", unit="folder", position=0, leave=True):
        input_dir = os.path.join(base_dir, folder_name)
        output_dir = os.path.join(compressed_base_dir, folder_name)

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        files = [f for f in os.listdir(input_dir) if f.lower().endswith('.png')]
        
        if not files:
            continue

        folder_orig_bytes = 0
        folder_new_bytes = 0

        # --- INNER LOOP: FILES ---
        for filename in tqdm(files, desc=f"  Processing {folder_name}", leave=True, unit="img", position=1):
            file_path = os.path.join(input_dir, filename)
            save_path = os.path.join(output_dir, filename)

            try:
                # 1. Capture Original Size
                orig_size = os.path.getsize(file_path)
                folder_orig_bytes += orig_size

                # 2. Compress (Smart Grayscale)
                with Image.open(file_path) as img:
                    gray = img.convert('L')
                    # Smart Thresholding
                    cleaned = gray.point(lambda p: 255 if p > noise_threshold else p)
                    cleaned.save(save_path, "PNG", optimize=True)

                # 3. Capture New Size
                new_size = os.path.getsize(save_path)
                folder_new_bytes += new_size

                # 4. Log Data
                savings_pct = ((orig_size - new_size) / orig_size) * 100 if orig_size > 0 else 0
                stats_data.append([
                    folder_name,
                    filename,
                    round(orig_size / 1024, 2),  # KB
                    round(new_size / 1024, 2),   # KB
                    round(savings_pct, 2)
                ])

            except Exception as e:
                tqdm.write(f"‚ùå Error on {filename}: {e}")

        # Update Grand Totals
        grand_total_orig_bytes += folder_orig_bytes
        grand_total_new_bytes += folder_new_bytes

        # Print Folder Summary
        if folder_orig_bytes > 0:
            folder_savings_pct = ((folder_orig_bytes - folder_new_bytes) / folder_orig_bytes) * 100
            tqdm.write(
                f"‚úÖ {folder_name}: "
                f"Saved {folder_savings_pct:.1f}% "
                f"({folder_orig_bytes/1024/1024:.2f}MB ‚ûù {folder_new_bytes/1024/1024:.2f}MB)"
            )
        
        tqdm.write("\n") # Spacer

    # --- FINAL REPORT ---
    total_savings_bytes = grand_total_orig_bytes - grand_total_new_bytes
    total_savings_pct = (total_savings_bytes / grand_total_orig_bytes * 100) if grand_total_orig_bytes > 0 else 0

    print("\n" + "="*45)
    print(f"       FINAL COMPRESSION REPORT")
    print("="*45)
    print(f"‚Ä¢ Total Images:    {len(stats_data)}")
    print(f"‚Ä¢ Original Size:   {grand_total_orig_bytes / 1024 / 1024:.2f} MB")
    print(f"‚Ä¢ New Size:        {grand_total_new_bytes / 1024 / 1024:.2f} MB")
    print(f"‚Ä¢ Space Saved:     {total_savings_bytes / 1024 / 1024:.2f} MB ({total_savings_pct:.2f}%)")
    print(f"‚Ä¢ CSV Report:      {csv_report_path}")
    print("="*45)


clean_and_compress_all()

================================================================================
FILE: MergeAISuggestedTagsToDB.py
================================================================================
import pandas as pd
import os
import shutil
from datetime import datetime

def update_single_source_of_truth():
    # --- 1. CONFIGURATION ---
    # BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
    BASE_PATH = "." 
    
    MASTER_FILENAME = "DB Master.csv" 
    SUGGESTIONS_FILENAME = "questionToTagUsingAITagged.csv" 

    master_path = os.path.join(BASE_PATH, MASTER_FILENAME)
    suggestions_path = os.path.join(BASE_PATH, SUGGESTIONS_FILENAME)
    
    # Define "Empty" markers (Values we are allowed to overwrite)
    empty_markers = ["", "nan", "NaN", "unknown", "Unknown", "None", "none", "0", 0, "Manual_Review_Required"]

    # --- 2. SAFETY: BACKUP ---
    if os.path.exists(master_path):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"{master_path.replace('.csv', '')}_BACKUP_{timestamp}.csv"
        shutil.copy(master_path, backup_path)
        print(f"[SAFETY] Backup created: {backup_path}")
    else:
        print(f"[ERROR] {master_path} not found!")
        return

    # --- 3. LOAD DATABASES ---
    print("Loading datasets...")
    df_master = pd.read_csv(master_path)
    
    if not os.path.exists(suggestions_path):
        print(f"[ERROR] {suggestions_path} not found.")
        return
    
    df_ai = pd.read_csv(suggestions_path)
    print(f"[INFO] Loaded {len(df_ai)} suggestions.")

    # Normalize IDs
    df_master['unique_id'] = df_master['unique_id'].astype(str).str.strip()
    df_ai['unique_id'] = df_ai['unique_id'].astype(str).str.strip()
    
    # --- ENSURE META COLUMNS EXIST ---
    meta_cols = ['Labelled by AI', 'AI_Reasoning', 'Model_Used']
    for col in meta_cols:
        if col not in df_master.columns:
            df_master[col] = ""

    # Initialize Sort Helper Column (0 = Old, 1 = Newly Updated)
    df_master['_temp_newly_updated'] = 0

    # Lookup Dict
    master_lookup = df_master.reset_index().set_index('unique_id')['index'].to_dict()

    # --- 4. MERGE LOGIC ---
    print(f"Merging AI tags (including Model Used)...")
    
    updated_count = 0
    skipped_verified = 0
    
    # Map AI columns to Master columns
    # Note: We handle Model_Used separately as it's metadata
    cols_to_map = {
        'Chapter': 'Chapter',
        'Topic': 'Topic',
        'Topic_L2': 'Topic_L2'
    }

    for index, ai_row in df_ai.iterrows():
        uid = ai_row['unique_id']
        
        if uid not in master_lookup: continue
        master_idx = master_lookup[uid]
        
        # Check Verification Lock
        if 'AI_Tag_Accepted' in df_master.columns:
            val = str(df_master.at[master_idx, 'AI_Tag_Accepted']).strip().lower()
            if val == 'yes':
                skipped_verified += 1
                continue 
            
        row_was_updated = False
        
        # Check if AI result is valid (skip errors)
        ai_reasoning = str(ai_row.get('AI_Reasoning', '')).lower()
        if "missed" in ai_reasoning or "fail" in ai_reasoning:
            continue 

        # Transfer Content Tags
        for ai_col, master_col in cols_to_map.items():
            current_master_val = str(df_master.at[master_idx, master_col]).strip()
            
            if ai_col in ai_row:
                ai_val = str(ai_row[ai_col]).strip()
                
                is_master_empty = current_master_val in [str(x) for x in empty_markers]
                is_ai_valid = ai_val not in [str(x) for x in empty_markers] and ai_val.lower() != "nan"
                
                if is_master_empty and is_ai_valid:
                    df_master.at[master_idx, master_col] = ai_val
                    row_was_updated = True
        
        # If we updated any data, update the Metadata fields
        if row_was_updated:
            df_master.at[master_idx, 'Labelled by AI'] = "Yes"
            df_master.at[master_idx, '_temp_newly_updated'] = 1 # Mark for sorting
            
            # Transfer Meta info if available
            if 'AI_Reasoning' in df_ai.columns:
                 df_master.at[master_idx, 'AI_Reasoning'] = ai_row['AI_Reasoning']
            
            if 'Model_Used' in df_ai.columns:
                 df_master.at[master_idx, 'Model_Used'] = ai_row['Model_Used']

            updated_count += 1

    # --- 5. SORT & SAVE ---
    print(f"Sorting {updated_count} newly updated rows to the top...")
    
    # Sort by: 1. Newly Updated (Desc), 2. Unique ID (Asc)
    df_master = df_master.sort_values(by=['_temp_newly_updated', 'unique_id'], ascending=[False, True])
    
    # Clean up helper column
    df_master.drop(columns=['_temp_newly_updated'], inplace=True)

    print("-" * 50)
    print(f"UPDATE COMPLETE")
    print(f"Rows Skipped (Verified):   {skipped_verified}")
    print(f"Rows Updated & Moved Top:  {updated_count}")
    print(f"Overwriting: {master_path}...")
    
    df_master.to_csv(master_path, index=False)
    print("SUCCESS: Database updated (Model info included).")
    print("-" * 50)

if __name__ == "__main__":
    update_single_source_of_truth()

================================================================================
FILE: migrateToFirebase.py
================================================================================
import pandas as pd
import firebase_admin
from firebase_admin import credentials, firestore, storage
import os
import mimetypes
import datetime
import time
import re
import math
import csv

# --- 1. CONFIGURATION ---
CREDENTIALS_FILE = 'studysmart-5da53-firebase-adminsdk-fbsvc-ca5974c5e9.json'
STORAGE_BUCKET = 'studysmart-5da53.firebasestorage.app'
CSV_FILE = 'DB Master Firebase.csv'
LOG_FILE = 'migrationLogs.csv'  # Persistent Log File

# COLLECTION NAME
COLLECTION_NAME = 'questions' 

# Local Base Directory
BASE_DIR = r"D:\Main\3. Work - Teaching\Projects\Question extractor\Processed_Database\Compressed"

# Cloud Storage Root Folder
STORAGE_ROOT = "Question Bank" 

# SAFETY FLAG: 
ERASE_EXISTING_DATA = False 

# --- COLUMNS TO FIX ---
TEXT_COLUMNS_TO_FIX = [
    "Section Name", "Model_Used", "pdf_Text", "PDF_Text_Available", 
    "AI_Reasoning", "Correct Answer_key", "Sub-Topic", 
    "Q_Image_Path", "Sol_Image_Path", "PYQ", "Chapter", "Question type", 
    "Subject", "OCR_Text", "Labelled by AI", "Topic_L2", "Topic", 
    "image_url", "question_id", "Exam", "Difficulty_tag", "Folder", 
    "Correct Answer", "QC_Status", "AI_Tag_Accepted", "PYQ_Year_Detailed"
]

INT_COLUMNS_TO_FIX = [
    "q_width", "q_height", "sol_width", "sol_height", "Question No.", 
    "QC_Locked", "manually updated"
]

# --- 2. INITIALIZE FIREBASE ---
if not firebase_admin._apps:
    cred = credentials.Certificate(CREDENTIALS_FILE)
    firebase_admin.initialize_app(cred, {
        'storageBucket': STORAGE_BUCKET
    })

db = firestore.client()
bucket = storage.bucket()

# --- 3. HELPER FUNCTIONS ---
def clean_text_field(value):
    """Converts NaNs/Floats to empty strings."""
    if value is None: return ""
    if isinstance(value, float):
        if math.isnan(value): return ""
        return str(value).replace(".0", "") # Remove .0 if it looks like an int
    return str(value).strip()

def clean_int_field(value):
    """Converts floats/strings to safe integers. Returns 0 if invalid."""
    if value is None: return 0
    try:
        if isinstance(value, float) and math.isnan(value): return 0
        return int(float(value))
    except (ValueError, TypeError):
        return 0

def extract_year(text):
    """Extracts year for the new Integer column."""
    if not text: return 0
    text_str = str(text)
    
    # --- FIX APPLIED HERE ---
    # Old: r'\b(19|20)\d{2}\b' (Captures only 19 or 20)
    # New: r'\b(19\d{2}|20\d{2})\b' (Captures the full 19xx or 20xx)
    matches = re.findall(r'\b(19\d{2}|20\d{2})\b', text_str)
    
    if matches:
        return max(map(int, matches))
    return 0

def load_processed_ids(log_file):
    """Reads the log file and returns a set of already processed question_ids."""
    processed = set()
    if not os.path.exists(log_file):
        with open(log_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'question_id', 'status'])
        return processed
    
    try:
        df_log = pd.read_csv(log_file)
        if 'question_id' in df_log.columns:
            success_rows = df_log[df_log['status'] == 'Success']
            processed = set(success_rows['question_id'].astype(str).str.strip())
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not read log file. Starting fresh. Error: {e}")
    
    return processed

def log_result(q_id, status):
    """Appends a single result to the log file."""
    with open(LOG_FILE, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([datetime.datetime.now(), q_id, status])

# --- 4. CLEANUP FUNCTION ---
def clean_slate():
    print("!!! WARNING: ERASE_EXISTING_DATA is True !!!")
    print(f"Cleaning up collection: {COLLECTION_NAME}...")
    docs = db.collection(COLLECTION_NAME).stream()
    for doc in docs: doc.reference.delete()
    
    blobs = bucket.list_blobs(prefix=STORAGE_ROOT)
    for blob in blobs: blob.delete()
    
    if os.path.exists(LOG_FILE):
        os.remove(LOG_FILE)
        print(" -> Deleted migrationLogs.csv")
    print("Cleanup Complete.\n")

# --- 5. UPLOAD FUNCTION ---
def upload_file_as_is(local_path, destination_path):
    if not os.path.exists(local_path): return None, 0
    try:
        file_size = os.path.getsize(local_path)
        blob = bucket.blob(destination_path)
        content_type, _ = mimetypes.guess_type(local_path)
        if content_type is None: content_type = 'application/octet-stream' 
        blob.upload_from_filename(local_path, content_type=content_type)
        blob.make_public()
        return blob.public_url, file_size
    except Exception as e:
        print(f"    Error uploading {os.path.basename(local_path)}: {e}")
        return None, 0

# --- 6. EXECUTE MIGRATION ---
start_time = time.time()
print(f"--- Migration Started at {datetime.datetime.now()} ---")

if ERASE_EXISTING_DATA:
    clean_slate()
    PROCESSED_IDS = set() 
else:
    PROCESSED_IDS = load_processed_ids(LOG_FILE)
    print(f"üîÑ Found {len(PROCESSED_IDS)} already processed questions in logs.")

# Load CSV
try:
    df = pd.read_csv(CSV_FILE)
    if 'unique_id' in df.columns:
        df.rename(columns={'unique_id': 'question_id'}, inplace=True)
    
    df = df[df['question_id'].notna()]
    print(f"Loaded Master CSV with {len(df)} rows.")
except Exception as e:
    print(f"CRITICAL ERROR: {e}")
    exit()

stats = {"skipped": 0, "success": 0, "error": 0, "new_images": 0}
cum_count=len(PROCESSED_IDS)
for index, row in df.iterrows():
    try:
        q_id = str(row['question_id']).strip()
        
        # --- SKIP LOGIC ---
        if q_id in PROCESSED_IDS:
            stats["skipped"] += 1
            continue
            
        original_folder = str(row['Folder']).strip()
        q_num = str(row['Q']).strip()
        
        if original_folder.startswith("CollegeDoors"):
            target_folder = original_folder.replace("CollegeDoors", "DC", 1)
        else:
            target_folder = original_folder
            
        print(f"[{cum_count+1}/{len(df)}] Processing: {q_id}")
        cum_count=cum_count+1

        # --- PATHS ---
        local_q_path = os.path.join(BASE_DIR, original_folder, f"Q_{q_num}.png")
        local_sol_path = os.path.join(BASE_DIR, original_folder, f"Sol_{q_num}.png")
        storage_q_path = f"{STORAGE_ROOT}/{target_folder}/Q_{q_num}.png"
        storage_sol_path = f"{STORAGE_ROOT}/{target_folder}/Sol_{q_num}.png"

        # --- PREPARE DATA ---
        raw_data = row.to_dict()
        doc_data = {}

        # 1. Type Cleaning
        for col, val in raw_data.items():
            if col in TEXT_COLUMNS_TO_FIX:
                doc_data[col] = clean_text_field(val)
            elif col in INT_COLUMNS_TO_FIX:
                doc_data[col] = clean_int_field(val)
            else:
                if isinstance(val, float) and math.isnan(val):
                    doc_data[col] = None
                else:
                    doc_data[col] = val

        # 2. PYQ Logic (FIXED REGEX & COLUMN PRIORITY)
        csv_pyq_year = clean_text_field(raw_data.get('PYQ_Year'))
        csv_pyq_tag = clean_text_field(raw_data.get('PYQ'))
        
        # Pick best source for the DETAILED string
        if len(csv_pyq_year) > 0:
            source_text = csv_pyq_year
        else:
            source_text = csv_pyq_tag

        doc_data['PYQ_Year'] = extract_year(source_text)      # Int (Now correctly returns 2024)
        doc_data['PYQ_Year_Detailed'] = source_text           # Str

        # 3. Timestamps
        doc_data['lastUpdated'] = firestore.SERVER_TIMESTAMP
        doc_data['createdAt'] = firestore.SERVER_TIMESTAMP

        # 4. Upload Images
        q_url, q_size = upload_file_as_is(local_q_path, storage_q_path)
        if q_url:
            doc_data['image_url'] = q_url
            stats["new_images"] += 1
        else:
            print(f"    -> ‚ùå Question Image Missing: {local_q_path}")
            log_result(q_id, "Error: Missing Q Image")
            stats["error"] += 1
            continue 

        sol_url, sol_size = upload_file_as_is(local_sol_path, storage_sol_path)
        if sol_url:
            doc_data['solution_url'] = sol_url
            stats["new_images"] += 1
        else:
            doc_data['solution_url'] = None

        # 5. Firestore Write
        doc_ref = db.collection(COLLECTION_NAME).document(q_id)
        
        doc_snap = doc_ref.get()
        if doc_snap.exists:
             if 'createdAt' in doc_data:
                 del doc_data['createdAt']
        
        doc_ref.set(doc_data, merge=True)
        
        print(f"    -> ‚úÖ Success! Logged to CSV.")
        log_result(q_id, "Success")
        stats["success"] += 1
            
    except Exception as e:
        print(f"    -> ERROR processing row {index}: {e}")
        log_result(q_id, f"Error: {e}")
        stats["error"] += 1

# --- REPORT ---
end_time = time.time()
print(f"\n--- Batch Complete ---")
print(f"Skipped (Already Done): {stats['skipped']}")
print(f"New Success: {stats['success']}")
print(f"Errors: {stats['error']}")
print(f"Time Taken: {end_time - start_time:.2f}s")

================================================================================
FILE: OCR_SmartBatch.py
================================================================================
import pandas as pd
import easyocr
import os
import json
import warnings
import numpy as np
from tqdm import tqdm
from datetime import datetime

# Suppress specific warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning)

def update_ocr_to_csv():
    # --- 1. CONFIGURATION ---
    CONFIG_PATH = 'config.json'
    config = {
        "BASE_PATH": "D:/Main/3. Work - Teaching/Projects/Question extractor",
        "MODEL_DIR": "models"
    }

    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'r') as f:
            loaded = json.load(f)
            config.update(loaded)

    # Normalize Paths
    BASE_PATH = os.path.normpath(config['BASE_PATH'])
    
    if os.path.isabs(config['MODEL_DIR']):
        MODEL_STORAGE = os.path.normpath(config['MODEL_DIR'])
    else:
        MODEL_STORAGE = os.path.normpath(os.path.join(BASE_PATH, config['MODEL_DIR']))

    IMG_BASE_DIR = os.path.join(BASE_PATH, 'Processed_Database')
    
    # SAFE IO SETUP
    MASTER_DB_PATH = os.path.join(BASE_PATH, 'DB Master.csv')       # READ ONLY
    OUTPUT_DB_PATH = os.path.join(BASE_PATH, 'DB Master_OCR.csv')   # READ/WRITE

    print(f"üîß CONFIGURATION:")
    print(f"   - Master DB (Read) : {MASTER_DB_PATH}")
    print(f"   - Output DB (Write): {OUTPUT_DB_PATH}")

    # --- 2. LOAD DATABASE SAFELY ---
    print(f"\nüìÇ Loading Master Database...")
    if not os.path.exists(MASTER_DB_PATH):
        print(f"‚ùå Error: Master Database not found at {MASTER_DB_PATH}")
        return

    try:
        df_master = pd.read_csv(MASTER_DB_PATH)
        print(f"   ‚úÖ Loaded {len(df_master)} rows from Master DB.")
    except Exception as e:
        print(f"‚ùå Error reading Master CSV: {e}")
        return

    # --- 3. MERGE EXISTING PROGRESS ---
    if os.path.exists(OUTPUT_DB_PATH):
        print(f"üîÑ Found existing Output DB. Merging progress...")
        try:
            df_progress = pd.read_csv(OUTPUT_DB_PATH)
            
            if 'unique_id' in df_master.columns and 'unique_id' in df_progress.columns:
                df_progress.drop_duplicates(subset=['unique_id'], keep='last', inplace=True)
                
                df_master.set_index('unique_id', inplace=True, drop=False)
                df_progress.set_index('unique_id', inplace=True, drop=False)
                
                df_master.update(df_progress)
                
                df_master.reset_index(drop=True, inplace=True)
                print(f"   ‚úÖ Merged progress. Current Row Count: {len(df_master)}")
            else:
                print("   ‚ö†Ô∏è 'unique_id' missing. Cannot safely merge. Starting from Master data.")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Could not read Output DB: {e}")
    
    df = df_master

    # --- 4. DATA PREP & INDEX FIX ---
    # Critical: Reset index to ensure 0..n sequence
    df.reset_index(drop=True, inplace=True)

    if 'OCR_Text' not in df.columns: 
        df['OCR_Text'] = ""
    if 'last_updated' not in df.columns:
        df['last_updated'] = ""
        
    # Convert to string, replace 'nan' text with empty string for cleaner logic
    df['OCR_Text'] = df['OCR_Text'].astype(str).replace('nan', '')

    # --- 5. DEFINE "BAD OCR" LOGIC ---
    # A row needs processing if:
    # 1. It is empty
    # 2. It is just "0"
    # 3. It is shorter than 30 characters
    
    # Calculate text length series
    text_len = df['OCR_Text'].str.strip().str.len()
    
    # Create the mask for "Needs Work"
    mask_needs_work = (
        (df['OCR_Text'].str.strip() == "") | 
        (df['OCR_Text'].str.strip() == "0") | 
        (text_len < 30)
    )
    
    print(f"   üìä Initial check: {mask_needs_work.sum()} rows identified as having missing/bad OCR.")

    # --- 6. SMART COPY: PDF_TEXT -> OCR_TEXT ---
    print("üîÑ Checking for PDF Text to fast-fill...")

    cols_map = {c.lower(): c for c in df.columns}
    col_pdf_text = cols_map.get('pdf_text')            
    col_pdf_flag = cols_map.get('pdf_text_available')  

    if col_pdf_text and col_pdf_flag:
        # Conditions for Smart Copy:
        # 1. Row needs work (as defined above)
        # 2. PDF_Text_Available == "Yes"
        # 3. PDF_Text actually has content
        
        pdf_available_mask = df[col_pdf_flag].astype(str).str.strip().str.lower() == "yes"
        pdf_content_exists = df[col_pdf_text].notna() & (df[col_pdf_text].astype(str).str.strip() != "")
        
        mask_smart_copy = mask_needs_work & pdf_available_mask & pdf_content_exists
        
        count_copied = mask_smart_copy.sum()

        if count_copied > 0:
            print(f"   ‚Ü≥ ‚ö° Fast-filled {count_copied} rows from '{col_pdf_text}'.")
            df.loc[mask_smart_copy, 'OCR_Text'] = df.loc[mask_smart_copy, col_pdf_text]
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            df.loc[mask_smart_copy, 'last_updated'] = current_time
        else:
            print("   ‚Ü≥ No rows qualified for Smart Copy.")
    else:
        print(f"   ‚ö†Ô∏è Could not find '{col_pdf_text}' or '{col_pdf_flag}' columns.")

    # --- 7. RE-CALCULATE PENDING LIST ---
    # Re-run the check to see what is STILL missing after the smart copy
    text_len = df['OCR_Text'].astype(str).str.strip().str.len()
    mask_still_needs_work = (
        (df['OCR_Text'].astype(str).str.strip() == "") | 
        (df['OCR_Text'].astype(str).str.strip() == "0") | 
        (text_len < 30)
    )
    
    pending_indices = df[mask_still_needs_work].index

    if len(pending_indices) == 0:
        print("\nüéâ No pending items! Database is up to date.")
        df.sort_values(by='last_updated', ascending=False, inplace=True)
        df.to_csv(OUTPUT_DB_PATH, index=False)
        print(f"‚úÖ Saved clean DB to: {OUTPUT_DB_PATH}")
        return

    print(f"\nüéØ Found {len(pending_indices)} questions needing actual OCR (GPU).")

    # --- 8. INITIALIZE ENGINE ---
    print("üöÄ Initializing EasyOCR...")
    if not os.path.exists(MODEL_STORAGE):
        os.makedirs(MODEL_STORAGE, exist_ok=True)
        
    reader = easyocr.Reader(['en'], model_storage_directory=MODEL_STORAGE, gpu=True) 

    # --- 9. PROCESSING LOOP ---
    print("\n‚ñ∂Ô∏è  Starting Batch Processing...")
    
    processed_count = 0
    save_frequency = 5
    
    try:
        with tqdm(total=len(pending_indices), unit="img") as pbar:
            
            for index in pending_indices:
                folder = str(df.at[index, 'Folder']).strip()
                
                # --- Get Question Number ---
                q_num = None
                if 'Question No.' in df.columns:
                    val = df.at[index, 'Question No.']
                    if pd.notna(val) and str(val).strip() != "":
                        q_num = str(val).split('.')[0]
                
                if not q_num and 'Q' in df.columns:
                    val = df.at[index, 'Q']
                    if pd.notna(val) and str(val).strip() != "":
                        q_num = str(val).split('.')[0]
                
                if not q_num:
                    pbar.write(f"‚ö†Ô∏è  Skipping Row {index}: No Question Number found.")
                    pbar.update(1)
                    continue

                img_filename = f"Q_{q_num}.png"
                img_path = os.path.join(IMG_BASE_DIR, folder, img_filename)
                
                if os.path.exists(img_path):
                    try:
                        result = reader.readtext(img_path, detail=0)
                        text_content = " ".join(result)
                        
                        df.at[index, 'OCR_Text'] = text_content
                        df.at[index, 'last_updated'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        
                        processed_count += 1
                        
                        if processed_count % save_frequency == 0:
                            df_save = df.sort_values(by='last_updated', ascending=False)
                            df_save.to_csv(OUTPUT_DB_PATH, index=False)
                            
                    except Exception as e:
                        pbar.write(f"‚ùå Error on {img_filename}: {e}")
                else:
                    pass

                pbar.update(1)

    except KeyboardInterrupt:
        print("\n\nüõë Script stopped by user.")
    
    # --- 10. FINAL SORT & SAVE ---
    print("üíæ Performing final sort and save...")
    try:
        df.sort_values(by='last_updated', ascending=False, inplace=True)
        df.to_csv(OUTPUT_DB_PATH, index=False)
        print(f"‚úÖ Saved successfully to: {OUTPUT_DB_PATH}")
        print(f"üìä Total processed: {processed_count}")
    except PermissionError:
        print(f"‚ùå ERROR: Could not save to {OUTPUT_DB_PATH}. Is the file open?")

if __name__ == "__main__":
    update_ocr_to_csv()

================================================================================
FILE: pushToGit.py
================================================================================
import subprocess
import sys

# ==========================================
# üëá UPDATE YOUR COMMIT MESSAGE HERE üëá
# ==========================================
COMMIT_MESSAGE = "MVP Release: Integrated Smart Time Analysis Engine, Coaching UI, and Dynamic Firestore Config"
# ==========================================

def run_command(command, step_name):
    print(f"\nüîÑ {step_name}...")
    try:
        result = subprocess.run(
            command,
            check=True,
            text=True,
            capture_output=True,
            shell=True 
        )
        print(f"‚úÖ Success.")
        if result.stdout:
            lines = result.stdout.strip().splitlines()
            if lines:
                print(f"   Output: {lines[0]}...")
            
    except subprocess.CalledProcessError as e:
        if "nothing to commit" in e.stdout.lower() or "nothing to commit" in e.stderr.lower():
            print("‚ö†Ô∏è  Nothing to commit (Working tree clean).")
            return
            
        print(f"‚ùå Error during: {step_name}")
        print(f"   Details: {e.stderr.strip()}")
        sys.exit(1)

if __name__ == "__main__":
    print(f"üöÄ Starting Git Push Sequence")
    print(f"üìù Message: '{COMMIT_MESSAGE}'")

    # 1. Check Status (Debugging step to see what Git sees)
    run_command("git status", "Checking repository status")

    # 2. Stage all changes
    # Forced add to ensure untracked files in subdirectories are caught
    run_command("git add --all", "Staging all files (including new folders)")

    # 3. Commit changes
    run_command(f'git commit -m "{COMMIT_MESSAGE}"', "Committing to local repo")

    # 4. Push to remote
    run_command("git push", "Pushing to remote origin")

    print("\nüéâ DONE! Code is live.")

================================================================================
FILE: sanitizeDB.py
================================================================================
import pandas as pd
import os

def run_sanity_update():
    # --- CONFIG ---
    BASE_PATH = "D:/Main/3. Work - Teaching/Projects/Question extractor"
    DB_PATH = os.path.join(BASE_PATH, "DB Master.csv")

    print(f"üìÇ Loading {DB_PATH}...")
    try:
        df = pd.read_csv(DB_PATH)
    except FileNotFoundError:
        print("‚ùå Error: DB Master.csv not found.")
        return

    # =================================================================
    # TASK 1: CONSOLIDATE 'Q' and 'Question No.'
    # =================================================================
    print("\nProcessing Column cleanup...")
    
    if 'Question No.' in df.columns:
        # Standardize empty values to NaN
        df['Q'] = df['Q'].replace(r'^\s*$', pd.NA, regex=True)
        
        # Identify rows where Q is missing AND Question No. exists
        missing_q_mask = df['Q'].isna() & df['Question No.'].notna()
        fill_count = missing_q_mask.sum()
        
        if fill_count > 0:
            print(f"   ‚Ü≥ Found {fill_count} rows with missing 'Q'. Filling from 'Question No.'...")
            df.loc[missing_q_mask, 'Q'] = df.loc[missing_q_mask, 'Question No.']
        else:
            print("   ‚Ü≥ 'Q' column is fully populated (or 'Question No.' provided no new data).")

        # Drop 'Question No.'
        print("   ‚Ü≥ Dropping 'Question No.' column...")
        df.drop(columns=['Question No.'], inplace=True)
        
    else:
        print("   ‚ö†Ô∏è Column 'Question No.' not found. Skipping consolidation.")

    # =================================================================
    # TASK 2: CHECK UNIQUE_ID DUPLICATES (READ ONLY)
    # =================================================================
    print("\nChecking 'unique_id' uniqueness...")
    
    if 'unique_id' in df.columns:
        duplicate_ids = df[df.duplicated('unique_id', keep=False)]
        num_duplicates = len(duplicate_ids)
        
        if num_duplicates > 0:
            unique_vals = duplicate_ids['unique_id'].nunique()
            print(f"   ‚ö†Ô∏è WARNING: Found {num_duplicates} rows involved in duplication.")
            print(f"      ({unique_vals} unique IDs repeated).")
            print("      NO CHANGES made to 'unique_id' (as requested).")
        else:
            print("   ‚úÖ PASSED: All 'unique_id' entries are unique.")
    else:
        print("   ‚ùå Error: 'unique_id' column missing!")

    # =================================================================
    # SAVE OVERWRITE
    # =================================================================
    print(f"\nüíæ Overwriting {DB_PATH}...")
    try:
        df.to_csv(DB_PATH, index=False)
        print("‚úÖ Database updated successfully.")
    except PermissionError:
        print("‚ùå ERROR: Could not save file. Is 'DB Master.csv' open in Excel?")

if __name__ == "__main__":
    run_sanity_update()

================================================================================
FILE: validateAITags.py
================================================================================
import pandas as pd
import os

# --- CONFIGURATION ---
BASE_PATH = r"D:\Main\3. Work - Teaching\Projects\Question extractor"
MASTER_FILENAME = "questionToTagUsingAITagged.csv"
METADATA_FILENAME = "DB Metadata.xlsx"
OUTPUT_FILENAME = "questionToTagUsingAITagged.csv"  # New file to avoid overwriting Master

# Helper to normalize strings for comparison (strip spaces)
def normalize(val):
    if pd.isna(val) or val is None:
        return ""
    return str(val).strip()

def run_validation():
    master_path = os.path.join(BASE_PATH, MASTER_FILENAME)
    metadata_path = os.path.join(BASE_PATH, METADATA_FILENAME)
    output_path = os.path.join(BASE_PATH, OUTPUT_FILENAME)

    # 1. LOAD SYLLABUS TREE (The Source of Truth)
    print("LOADING SYLLABUS...")
    try:
        meta_df = pd.read_excel(metadata_path, sheet_name="Syllabus tree", engine='openpyxl')
        
        # Build a Dictionary: { "ChapterName": {"Topic1", "Topic2", ...} }
        valid_syllabus = {}
        
        for ch, group in meta_df.groupby('Chapter'):
            ch_name = normalize(ch)
            if not ch_name or ch_name.lower() in ["nan", "unknown"]: continue
            
            # Get all valid topics for this chapter
            valid_topics = set()
            for t in group['Topic'].unique():
                t_norm = normalize(t)
                if t_norm and t_norm.lower() not in ["nan", "miscellaneous"]:
                    valid_topics.add(t_norm)
            
            valid_syllabus[ch_name] = valid_topics
            
        print(f"‚úÖ Loaded {len(valid_syllabus)} valid chapters from Metadata.")

    except Exception as e:
        print(f"‚ùå Error loading Metadata: {e}")
        return

    # 2. LOAD MASTER DB
    print("\nLOADING MASTER DATABASE...")
    if not os.path.exists(master_path):
        print("‚ùå Master DB not found.")
        return
    
    df = pd.read_csv(master_path)
    
    # Initialize Flag Column (Default = "Yes")
    df['TagFromValidList'] = "Yes"
    
    # Statistics counters
    invalid_chapter_count = 0
    invalid_topic_count = 0

    # 3. RUN VALIDATION LOGIC
    print("VALIDATING ROWS...")
    
    for idx, row in df.iterrows():
        # Get current tags from the row
        row_chap = normalize(row.get('Chapter'))
        row_topic = normalize(row.get('Topic'))
        
        is_valid = True
        
        # CHECK 1: Is Chapter in Syllabus?
        if row_chap not in valid_syllabus:
            is_valid = False
            invalid_chapter_count += 1
            # Optional: You can log the reason if you want
            # df.at[idx, 'Validation_Error'] = f"Invalid Chapter: '{row_chap}'"
        
        # CHECK 2: If Chapter is valid, is Topic in that Chapter's list?
        else:
            valid_topics_for_chapter = valid_syllabus[row_chap]
            
            if row_topic not in valid_topics_for_chapter:
                is_valid = False
                invalid_topic_count += 1
                # df.at[idx, 'Validation_Error'] = f"Invalid Topic: '{row_topic}' not in {row_chap}"

        # SET FLAG
        if not is_valid:
            df.at[idx, 'TagFromValidList'] = "No"

    # 4. SAVE TO NEW CSV
    print("-" * 40)
    print(f"VALIDATION COMPLETE")
    print(f"Rows with Invalid Chapters: {invalid_chapter_count}")
    print(f"Rows with Valid Chapter but Invalid Topic: {invalid_topic_count}")
    print(f"Total Invalid Rows Flagged: {invalid_chapter_count + invalid_topic_count}")
    print("-" * 40)
    
    df.to_csv(output_path, index=False)
    print(f"‚úÖ Saved validated data to: {OUTPUT_FILENAME}")

if __name__ == "__main__":
    run_validation()

================================================================================
FILE: validateFirebaseData.py
================================================================================
import firebase_admin
from firebase_admin import credentials, firestore
import pandas as pd
from collections import defaultdict

# --- CONFIGURATION ---
CREDENTIALS_FILE = 'studysmart-5da53-firebase-adminsdk-fbsvc-ca5974c5e9.json'
COLLECTION_NAME = 'questions'

# --- DEFINING EXPECTED SCHEMA ---
# Define the strict type you expect for each field.
# Use (int, float) if a field can be either.
# Use type(None) if a field is allowed to be null.

EXPECTED_SCHEMA = {
    'question_id': str,
    'Folder': str,
    'image_url': str,         # Must be a string (URL)
    'solution_url': (str, type(None)), # Can be string OR null
    
    # Fields from your CSV (Adjust these based on what you want)
    'Q': (int, float, str),   # Often mixed; best to enforce one, but we check for all here
    'Topic': str,
    'Subtopic': str
}

# Fields that MUST exist in every document
REQUIRED_FIELDS = ['question_id', 'image_url', 'Folder']

# --- INITIALIZE ---
if not firebase_admin._apps:
    cred = credentials.Certificate(CREDENTIALS_FILE)
    firebase_admin.initialize_app(cred)

db = firestore.client()

def get_type_name(value):
    return type(value).__name__

print(f"--- Starting Validation on '{COLLECTION_NAME}' ---")

# 1. Fetch All Documents
docs_stream = db.collection(COLLECTION_NAME).stream()

total_docs = 0
error_log = []
type_distribution = defaultdict(lambda: defaultdict(int))

print("Scanning documents...")

for doc in docs_stream:
    total_docs += 1
    data = doc.to_dict()
    doc_id = doc.id
    
    # A. Check Required Fields
    for field in REQUIRED_FIELDS:
        if field not in data:
            error_log.append({
                "doc_id": doc_id, 
                "error": "Missing Required Field", 
                "field": field, 
                "value": "MISSING"
            })
            continue

        # Check for Null in Required Fields
        if data[field] is None:
            error_log.append({
                "doc_id": doc_id,
                "error": "Required Field is Null",
                "field": field,
                "value": "None"
            })

    # B. Check Data Types & Schema
    for key, value in data.items():
        # Record the type distribution (for summary stats)
        val_type = type(value)
        type_distribution[key][val_type.__name__] += 1
        
        # If we have a rule for this field, validate it
        if key in EXPECTED_SCHEMA:
            expected = EXPECTED_SCHEMA[key]
            if not isinstance(value, expected):
                # If it's a float equivalent to an int (e.g., 10.0), it might be okay, but we flag it strict
                error_log.append({
                    "doc_id": doc_id,
                    "error": f"Type Mismatch (Expected {expected})",
                    "field": key,
                    "value": f"{value} ({type(value).__name__})"
                })

# --- REPORTING ---

print(f"\n--- Validation Complete: Scanned {total_docs} documents ---")

# 1. Show Type Distribution (To catch "Mixed" fields)
print("\n[Field Type Summary]")
print(f"{'Field':<20} | {'Types Found (Count)'}")
print("-" * 60)
for field, types in type_distribution.items():
    type_desc = ", ".join([f"{t} ({c})" for t, c in types.items()])
    print(f"{field:<20} | {type_desc}")
    
    # Warn if a field has multiple types (excluding None)
    non_none_types = [t for t in types.keys() if t != 'NoneType']
    if len(non_none_types) > 1:
        print(f"   >>> WARNING: Field '{field}' has mixed types! {non_none_types}")

# 2. Show Errors
if error_log:
    print(f"\n[Found {len(error_log)} Specific Errors]")
    df_errors = pd.DataFrame(error_log)
    
    # Display first 10 errors
    print(df_errors.head(10).to_string(index=False))
    
    # Save to CSV
    csv_filename = "validation_errors.csv"
    df_errors.to_csv(csv_filename, index=False)
    print(f"\nFull error log saved to: {csv_filename}")
else:
    print("\n[SUCCESS] No schema violations found.")

